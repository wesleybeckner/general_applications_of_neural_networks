
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.4">
    
    
      
        <title>The Multilayer Perceptron - General Applications of Neural Networks</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bb3983ee.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-114664473-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})})</script>
  <script async src="https://www.google-analytics.com/analytics.js"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#general-applications-of-neural-networks-session-1-the-multilayer-perceptron" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="General Applications of Neural Networks" class="md-header__button md-logo" aria-label="General Applications of Neural Networks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            General Applications of Neural Networks
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              The Multilayer Perceptron
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="General Applications of Neural Networks" class="md-nav__button md-logo" aria-label="General Applications of Neural Networks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    General Applications of Neural Networks
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Sessions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Sessions" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Sessions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          The Multilayer Perceptron
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        The Multilayer Perceptron
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-neural-network-building-blocks" class="md-nav__link">
    1.1 Neural Network Building Blocks
  </a>
  
    <nav class="md-nav" aria-label="1.1 Neural Network Building Blocks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-the-perceptron" class="md-nav__link">
    1.1.1 The Perceptron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-1-single-perceptron" class="md-nav__link">
    🏋️ Exercise 1: Single Perceptron
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-neural-network-architectures" class="md-nav__link">
    1.2 Neural Network Architectures
  </a>
  
    <nav class="md-nav" aria-label="1.2 Neural Network Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-neural-network-layers" class="md-nav__link">
    1.2.1 Neural Network Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-the-activation-function" class="md-nav__link">
    1.2.2 The Activation Function
  </a>
  
    <nav class="md-nav" aria-label="1.2.2 The Activation Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-2-the-rectifier-function" class="md-nav__link">
    🏋️ Exercise 2: The Rectifier Function
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123-stacking-layers" class="md-nav__link">
    1.2.3 Stacking Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#124-building-sequential-neural-networks" class="md-nav__link">
    1.2.4 Building Sequential Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-building-sequential-layers" class="md-nav__link">
    🏋️ Exercise 3: Building Sequential Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-4-other-activation-functions" class="md-nav__link">
    🏋️ Exercise 4: Other Activation Functions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-neural-network-training" class="md-nav__link">
    1.3 Neural Network Training
  </a>
  
    <nav class="md-nav" aria-label="1.3 Neural Network Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131-the-loss-function" class="md-nav__link">
    1.3.1 The Loss Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132-the-optimizer" class="md-nav__link">
    1.3.2 The Optimizer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#133-batches-and-epochs" class="md-nav__link">
    1.3.3 Batches and Epochs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#134-learning-rate" class="md-nav__link">
    1.3.4 Learning Rate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-51-train-your-first-neural-networks" class="md-nav__link">
    🏋️ Exercise 5.1: Train your first Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-52-improve-loss-by-varying-nodes-and-hidden-layers" class="md-nav__link">
    🏋️ Exercise 5.2: Improve loss by varying nodes and hidden layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-53-learning-curves" class="md-nav__link">
    🏋️ Exercise 5.3: Learning Curves
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S2_Feed_Forward_Neural_Networks/" class="md-nav__link">
        Feed Forward Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S3_Computer_Vision_I/" class="md-nav__link">
        Computer Vision I
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S4_Computer_Vision_II/" class="md-nav__link">
        Computer Vision II
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../S5_Time_Series_Analysis/" class="md-nav__link">
        Time Series Analysis
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Exercises
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Exercises" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Exercises
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E1_Neural_Network_Linearity/" class="md-nav__link">
        Neural Network Linearity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exercises/E4_Testing_and_Serving_Code/" class="md-nav__link">
        Testing and Serving Code
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#10-preparing-environment-and-importing-data" class="md-nav__link">
    1.0 Preparing Environment and Importing Data
  </a>
  
    <nav class="md-nav" aria-label="1.0 Preparing Environment and Importing Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-import-packages" class="md-nav__link">
    1.0.1 Import Packages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-load-dataset" class="md-nav__link">
    1.0.2 Load Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-neural-network-building-blocks" class="md-nav__link">
    1.1 Neural Network Building Blocks
  </a>
  
    <nav class="md-nav" aria-label="1.1 Neural Network Building Blocks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-the-perceptron" class="md-nav__link">
    1.1.1 The Perceptron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-1-single-perceptron" class="md-nav__link">
    🏋️ Exercise 1: Single Perceptron
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-neural-network-architectures" class="md-nav__link">
    1.2 Neural Network Architectures
  </a>
  
    <nav class="md-nav" aria-label="1.2 Neural Network Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-neural-network-layers" class="md-nav__link">
    1.2.1 Neural Network Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-the-activation-function" class="md-nav__link">
    1.2.2 The Activation Function
  </a>
  
    <nav class="md-nav" aria-label="1.2.2 The Activation Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercise-2-the-rectifier-function" class="md-nav__link">
    🏋️ Exercise 2: The Rectifier Function
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123-stacking-layers" class="md-nav__link">
    1.2.3 Stacking Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#124-building-sequential-neural-networks" class="md-nav__link">
    1.2.4 Building Sequential Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-building-sequential-layers" class="md-nav__link">
    🏋️ Exercise 3: Building Sequential Layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-4-other-activation-functions" class="md-nav__link">
    🏋️ Exercise 4: Other Activation Functions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-neural-network-training" class="md-nav__link">
    1.3 Neural Network Training
  </a>
  
    <nav class="md-nav" aria-label="1.3 Neural Network Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131-the-loss-function" class="md-nav__link">
    1.3.1 The Loss Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132-the-optimizer" class="md-nav__link">
    1.3.2 The Optimizer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#133-batches-and-epochs" class="md-nav__link">
    1.3.3 Batches and Epochs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#134-learning-rate" class="md-nav__link">
    1.3.4 Learning Rate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-51-train-your-first-neural-networks" class="md-nav__link">
    🏋️ Exercise 5.1: Train your first Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-52-improve-loss-by-varying-nodes-and-hidden-layers" class="md-nav__link">
    🏋️ Exercise 5.2: Improve loss by varying nodes and hidden layers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-53-learning-curves" class="md-nav__link">
    🏋️ Exercise 5.3: Learning Curves
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

<p><a href="https://colab.research.google.com/github/wesleybeckner/general_applications_of_neural_networks/blob/main/notebooks/S1_Multilayer_Perceptron.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="general-applications-of-neural-networks-session-1-the-multilayer-perceptron">General Applications of Neural Networks <br> Session 1: The Multilayer Perceptron<a class="headerlink" href="#general-applications-of-neural-networks-session-1-the-multilayer-perceptron" title="Permanent link">&para;</a></h1>
<p><strong>Instructor</strong>: Wesley Beckner</p>
<p><strong>Contact</strong>: <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#119;&#101;&#115;&#108;&#101;&#121;&#98;&#101;&#99;&#107;&#110;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a></p>
<hr />
<p><br></p>
<p>In this session we will introduce Neural Networks! We'll cover the over arching concepts used to talk about network architecture as well as their building blocks.</p>
<p><em>images in this notebook borrowed from <a href="https://mathformachines.com/">Ryan Holbrook</a></em></p>
<p><br></p>
<hr />
<p><br></p>
<p><a name='top'></a></p>
<p><a name='x.0'></a></p>
<h2 id="10-preparing-environment-and-importing-data">1.0 Preparing Environment and Importing Data<a class="headerlink" href="#10-preparing-environment-and-importing-data" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<div class="codehilite"><pre><span></span><code><span class="err">!</span><span class="n">pip</span> <span class="n">uninstall</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span> <span class="o">-</span><span class="n">y</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Found existing installation: scikit-learn 0.24.2
Uninstalling scikit-learn-0.24.2:
  Successfully uninstalled scikit-learn-0.24.2
Collecting scikit-learn
  Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0)
Requirement already satisfied: scipy&gt;=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)
Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)
Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)
Installing collected packages: scikit-learn
Successfully installed scikit-learn-0.24.2
</code></pre></div>

<p><a name='x.0.1'></a></p>
<h3 id="101-import-packages">1.0.1 Import Packages<a class="headerlink" href="#101-import-packages" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_transformer</span><span class="p">,</span> <span class="n">make_column_selector</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">set_config</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">set_config</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="s1">&#39;diagram&#39;</span><span class="p">)</span>
</code></pre></div>

<p><a name='x.0.1'></a></p>
<h3 id="102-load-dataset">1.0.2 Load Dataset<a class="headerlink" href="#102-load-dataset" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>Before diving in I want to compare two methods of reading and preprocessing our data:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># import wine data</span>
<span class="n">wine</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/wesleybeckner/&quot;</span>\
      <span class="s2">&quot;ds_for_engineers/main/data/wine_quality/winequalityN.csv&quot;</span><span class="p">)</span>

<span class="c1"># create X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;quality&#39;</span><span class="p">)</span>

<span class="c1"># split into train/test</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># the numerical values pipe</span>
<span class="n">num_proc</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">),</span> <span class="c1"># impute with median</span>
                         <span class="n">StandardScaler</span><span class="p">())</span> <span class="c1"># scale and center</span>

<span class="c1"># the categorical values pipe</span>
<span class="n">cat_proc</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> 
                  <span class="n">fill_value</span><span class="o">=</span><span class="s1">&#39;missing&#39;</span><span class="p">),</span> <span class="c1"># impute with placeholder</span>
    <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">))</span> <span class="c1"># one hot encode</span>

<span class="c1"># parallelize the two pipes</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">((</span><span class="n">num_proc</span><span class="p">,</span>
                                <span class="n">make_column_selector</span><span class="p">(</span><span class="n">dtype_include</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">)),</span>
                                       <span class="p">(</span><span class="n">cat_proc</span><span class="p">,</span>
                                <span class="n">make_column_selector</span><span class="p">(</span><span class="n">dtype_include</span><span class="o">=</span><span class="nb">object</span><span class="p">)))</span>

<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># fit_transform on train</span>
<span class="n">X_test_std</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># transform test and validation</span>
<span class="n">X_val_std</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">y_train_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="c1"># log output y</span>
<span class="n">y_val_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span> <span class="c1"># log output y</span>
<span class="n">y_test_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span> <span class="c1"># log output y</span>

<span class="n">preprocessor</span>
</code></pre></div>

<style>#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 {color: black;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 pre{padding: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable {background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator:hover {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-item {z-index: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:only-child::after {width: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-container {display: inline-block;position: relative;}</style>
<div id="sk-81b6bb9b-6f93-4195-900b-55d5f542e910" class"sk-top-container"><div class="sk-container"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="03c00fdc-8ba0-486f-b533-185820655dce" type="checkbox" ><label class="sk-toggleable__label" for="03c00fdc-8ba0-486f-b533-185820655dce">ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[('pipeline-1',
                                 Pipeline(steps=[('simpleimputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('standardscaler',
                                                  StandardScaler())]),
                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7f5a9bd26bd0>),
                                ('pipeline-2',
                                 Pipeline(steps=[('simpleimputer',
                                                  SimpleImputer(fill_value='missing',
                                                                strategy='constant')),
                                                 ('onehotencoder',
                                                  OneHotEncoder(handle_unknown='ignore'))]),
                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7f5a9bd26e90>)])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="3bf0a90e-c55c-46b7-8b60-3bd7dcc62d92" type="checkbox" ><label class="sk-toggleable__label" for="3bf0a90e-c55c-46b7-8b60-3bd7dcc62d92">pipeline-1</label><div class="sk-toggleable__content"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7f5a9bd26bd0></pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="5ffc1719-ade2-4237-a5ef-847bebefbf19" type="checkbox" ><label class="sk-toggleable__label" for="5ffc1719-ade2-4237-a5ef-847bebefbf19">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(strategy='median')</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="4cfdb6b5-ef34-4fdd-81e8-6a3a04495bcb" type="checkbox" ><label class="sk-toggleable__label" for="4cfdb6b5-ef34-4fdd-81e8-6a3a04495bcb">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="33f150b6-34e6-4f09-9374-b19c9e19aed9" type="checkbox" ><label class="sk-toggleable__label" for="33f150b6-34e6-4f09-9374-b19c9e19aed9">pipeline-2</label><div class="sk-toggleable__content"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7f5a9bd26e90></pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="4af628ce-f2a5-4948-a89c-4f69499c5b4e" type="checkbox" ><label class="sk-toggleable__label" for="4af628ce-f2a5-4948-a89c-4f69499c5b4e">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(fill_value='missing', strategy='constant')</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="489acc38-6db9-43de-877d-a41631d212fb" type="checkbox" ><label class="sk-toggleable__label" for="489acc38-6db9-43de-877d-a41631d212fb">OneHotEncoder</label><div class="sk-toggleable__content"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div></div></div>

<p><a name='x.1'></a></p>
<h2 id="11-neural-network-building-blocks">1.1 Neural Network Building Blocks<a class="headerlink" href="#11-neural-network-building-blocks" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p><a name='x.1.1'></a></p>
<h3 id="111-the-perceptron">1.1.1 The Perceptron<a class="headerlink" href="#111-the-perceptron" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>The simplest unit of a neural network is the perceptron. Given an input vector \(x\) and an output vector \(y\), we can illustrate this like so:</p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/mfOlDR6.png"></img>
</p>

<p>where \(w\) is a weight applied to \(x\) and \(b\) is an unweighted term that we call the <i>bias</i>. We include a bias so that the perceptron is not entirely dependent on the input data. A neural network <em>learns</em> by updating \(w\) and \(b\) so that it can accurately model \(x\) to \(y\). When we write out the perceptron mathematically we get the following:</p>
<p>
<script type="math/tex; mode=display"> y = xw+b </script>
</p>
<p>which should look familiar! This is our equation for a linear function. In fact, we will see that a neural network is essentially many instances of linear regression along side, and being fed into, one another. </p>
<p>Often, we will have not an input feature vector \(x\) but an input feature matrix, \(X\). We can update our schematic for a perceptron to account for this:</p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/vyXSnlZ.png"></img>
</p>

<p>We can write the mathematical formula for this neuron as follows:</p>
<p>
<script type="math/tex; mode=display"> y =  x_2 w_2 + x_1 w_1 + x_0 w_0 + b </script>
</p>
<p>In tensorflow/keras we can define this perceptron:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="c1"># Create a network with 1 linear unit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># number of units (the + filled circle above)</span>
                 <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span> <span class="c1"># number of x_ (the x filled circle above)</span>
<span class="p">])</span>
</code></pre></div>

<p>In order to build this single perceptron with keras, I had to use some additional language here: layers, dense, sequential. We'll explain what these are referring to in a moment. What I want to draw your attention to now, however, is that we tell <code>layers.Dense</code> that we want <em>1 unit</em>, the single perceptron, and <code>input_shape=[3]</code>, the number of features. Notice that <code>b</code> is automatically included without having it as a parameter. Just as we always have a y intercept in a linear model. </p>
<p>After we introduce the other aspects of the neural network architecture, we will train a single perceptron model and compare it with a linear model, we will see that they are functionally no different.</p>
<h3 id="exercise-1-single-perceptron">🏋️ Exercise 1: Single Perceptron<a class="headerlink" href="#exercise-1-single-perceptron" title="Permanent link">&para;</a></h3>
<p>define a single perceptron that could be used to predict wine density from acidity. </p>
<p>Inpsect the weights.</p>
<p>Use the untrained model to predict y and plot this against true y</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code cell for exercise 1</span>

<span class="c1"># DECLARE MODEL</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>

    <span class="c1">### YOUR CODE HERE ###    </span>

<span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">weights</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[1.4809548]], dtype=float32)&gt;,
 &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]
</code></pre></div>

<p>And now use the untrained model to predict <code>wine['density']</code> from <code>wine['fixed acidity']</code></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># in the line below, use model.predict() and provide wine[&#39;fixed acidity&#39;] as </span>
<span class="c1"># the input data to predict what wine[&#39;density&#39;] will be</span>
<span class="c1"># y_pred = </span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">wine</span><span class="p">[</span><span class="s1">&#39;density&#39;</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[&lt;matplotlib.lines.Line2D at 0x7f5a9269a890&gt;]
</code></pre></div>

<p><img alt="png" src="../S1_Multilayer_Perceptron_files/S1_Multilayer_Perceptron_16_1.png" /></p>
<p><a name='x.2'></a></p>
<h2 id="12-neural-network-architectures">1.2 Neural Network Architectures<a class="headerlink" href="#12-neural-network-architectures" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p><a name='x.2.1'></a></p>
<h3 id="121-neural-network-layers">1.2.1 Neural Network Layers<a class="headerlink" href="#121-neural-network-layers" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>Now that we have the idea of the most basic building block of a neural network, we will start to discuss the larger architecture. The reason we focused on the lowest building block, is that neural networks are <em>modular</em>. They are made up of instances of these perceptrons or neurons. neurons in parallel make up a <em>layer</em></p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/2MA4iMV.png"></img>
</p>

<p>These layers feed into one another. When each node of a preceding layer is connected to every node of a following layer, we say they are <em>fully connected</em> and the receiving layer is <em>a dense layer</em>. In a moment we will talk about input, output and hidden layers, for neural networks with three or more layers.</p>
<p><a name='x.2.2'></a></p>
<h3 id="122-the-activation-function">1.2.2 The Activation Function<a class="headerlink" href="#122-the-activation-function" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>It turns out that stringing together a bunch of linear functions, will still result in overall linear relationships. We need a way to break out of this. A neat trick is introduced at the output of each neuron. The output passes through an <em>activation function</em>. There are a handful of different activation functions used in practice, the most common is known as the <em>rectifier</em> function:</p>
<p>
<script type="math/tex; mode=display"> max(f(x), 0) </script>
</p>
<p>and the resulting node can be schematically drawn like this:</p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/eFry7Yu.png"></img>
</p>

<p>with the inset of the summation node indicating that at a minimum the resultant y value is 0.</p>
<h4 id="exercise-2-the-rectifier-function">🏋️ Exercise 2: The Rectifier Function<a class="headerlink" href="#exercise-2-the-rectifier-function" title="Permanent link">&para;</a></h4>
<p>Write a function called <code>my_perceptron</code> that takes x, a length 2 array, as an input. Have your function return the maximum of \((0, w*x + b)\) where w is a length 2 weight vector.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code cell for exercise 2</span>
<span class="k">def</span> <span class="nf">my_perceptron</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  a simple 2 input feature perceptron with predefined weights, intercept, and </span>
<span class="sd">  a rectifier activation function</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: array</span>
<span class="sd">    the input array of length 2</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  rect: int</span>
<span class="sd">    the rectified output of the perceptron</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># # define b, w, and y (y=mx+b)</span>
  <span class="c1"># w = </span>
  <span class="c1"># b = </span>
  <span class="c1"># y = </span>

  <span class="c1"># # return the max of 0 and y</span>
  <span class="c1"># rect = </span>
  <span class="k">return</span> <span class="n">rect</span>
</code></pre></div>

<p>After you write your function make sure it returns 0 when the output of the linear component is negative.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_zero_output</span><span class="p">():</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># 10 * 1 + (-10) * 2 + 1 = -11</span>
  <span class="k">assert</span> <span class="n">my_perceptron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;The output is not zero when it should be!&quot;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">test_zero_output</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test passing&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>test passing
</code></pre></div>

<p><a name='x.2.3'></a></p>
<h3 id="123-stacking-layers">1.2.3 Stacking Layers<a class="headerlink" href="#123-stacking-layers" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>When we stack many layers together, we create what are traditionally regarded as neural networks. the first and last layers are called the <em>input</em> and <em>output</em> layers, while the inbetween layers are referred to as <em>hidden</em> layers, since their outputs are not directly seen. Tradditionally, a neural network with three or more hidden layers is referred to as a <em>deep</em> neural network.</p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/Y5iwFQZ.png"></img>
</p>

<p>Notice that in this schematic, the last node does not have an activation function. This is typical of a regression task. In a classification task, we might require an activation function here.</p>
<p><a name='x.2.2'></a></p>
<h3 id="124-building-sequential-neural-networks">1.2.4 Building Sequential Neural Networks<a class="headerlink" href="#124-building-sequential-neural-networks" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>Now that we have the essential components of a neural network architecture, we can enter into the domain of overall naming conventions for architecure types. The classic neural network architecture is a <em>feed forward</em> neural network, where every preceding layer feeds into the next layer. We will practice building that with keras.</p>
<h3 id="exercise-3-building-sequential-layers">🏋️ Exercise 3: Building Sequential Layers<a class="headerlink" href="#exercise-3-building-sequential-layers" title="Permanent link">&para;</a></h3>
<p>In the cell bellow, use keras to build a 3-layer network with <code>activation='relu'</code> and 512 units. Create the output layer so that it can predict 1 continuous value.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code cell for exercise 3</span>

<span class="c1"># DECLARE THE MODEL</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>

    <span class="c1">### YOUR CODE HERE ###</span>

    <span class="c1"># the hidden ReLU layers</span>

    <span class="c1"># the linear output layer </span>

<span class="p">])</span>
</code></pre></div>

<h3 id="exercise-4-other-activation-functions">🏋️ Exercise 4: Other Activation Functions<a class="headerlink" href="#exercise-4-other-activation-functions" title="Permanent link">&para;</a></h3>
<p>There are other activation functions we can use after the summation in a neural node. Use the code below to plot and inspect them!</p>
<p>Pick one and do a quick google search on what that activation function's best use case is.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code cell for exercise 4</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># YOUR CODE HERE: Change &#39;relu&#39; to &#39;elu&#39;, &#39;selu&#39;, &#39;swish&#39;... or something else</span>
<span class="n">activation_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># once created, a layer is callable just like a function</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Input&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Output&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="png" src="../S1_Multilayer_Perceptron_files/S1_Multilayer_Perceptron_30_0.png" /></p>
<p><a name='x.3'></a></p>
<h2 id="13-neural-network-training">1.3 Neural Network Training<a class="headerlink" href="#13-neural-network-training" title="Permanent link">&para;</a></h2>
<p><a href="#top">back to top</a></p>
<p>We've defined neural network architectures, now how do we train them? There are two main concepts here: the <em>loss function</em> which we've encountered before, and the <em>optimizer</em> the means by which we improve the loss function</p>
<p><a name='x.3.1'></a></p>
<h3 id="131-the-loss-function">1.3.1 The Loss Function<a class="headerlink" href="#131-the-loss-function" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>In previous sessions, we've envountered MSE:</p>
<p>
<script type="math/tex; mode=display"> MSE = \frac{1}{N}\sum{(y-\hat{y})^2}</script>
</p>
<p align=center>
<img src="https://cdn.corporatefinanceinstitute.com/assets/Sum-of-Squares-1024x712.png" width=400></img>
</p>

<p>Another common loss for neural networks is the mean absolute error (MAE):</p>
<p>
<script type="math/tex; mode=display"> MAE = \frac{1}{N}\sum{|y-\hat{y}|}</script>
</p>
<p align=center>
<img src="https://cdn-media-1.freecodecamp.org/images/MNskFmGPKuQfMLdmpkT-X7-8w2cJXulP3683" width=400></img>
</p>

<p>In anycase, the loss function describes the difference between the actual and predicted output of the model. The important thing to note, is that the weights in the neural network are systematically updated according to this loss function, they do this via an optimization algorithm.</p>
<p><a name='x.3.2'></a></p>
<h3 id="132-the-optimizer">1.3.2 The Optimizer<a class="headerlink" href="#132-the-optimizer" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>In order to update the neural network weights to improve the loss function, we require an algorithm. Virtually all available algorithms for this purpose fall within the family of <em>stochastic gradient descent</em>. This works essentially in these iterative steps:</p>
<ol>
<li>a subsample of the input data is passed through the network </li>
<li>a loss is computed</li>
<li>the weights are adjusted in a direction to improve the loss</li>
</ol>
<blockquote>
<p>The key here is in step 3. The brilliance of neural networks, is that the loss function is differentiable with respect to the weights in the network, and so the change in loss can be ascribed to certain weight changes. We refer to this as <em>assigning blame</em> in the network, and it works through the mathematical <em>chain rule</em> of differentiation. We won't go into great detail here, other than to make a nod to it, and that this algorithm (step 3) is referred to as <em>back propagation</em>. </p>
</blockquote>
<p>The three step process is repeated until a stop criteria is reached, the simplest being the loss stops improving above some threshold or a desired loss is achieved. </p>
<p align=center>
<img src="https://raw.githubusercontent.com/wesleybeckner/general_applications_of_neural_networks/main/assets/rFI1tIk.gif"></img>
</p>

<p>In the above animation, the black line represents the output of the model, the red dots make up a <em>minibatch</em> or simply a <em>batch</em> while the opaque red dots represent the whole training dataset. Exposing the model to an entire round of the training data is referred to as an <em>epoch</em>. The training loss improves with additional rounds of trianing (middle panel) and the weights are adjusted to update the model (right panel).</p>
<h3 id="133-batches-and-epochs">1.3.3 Batches and Epochs<a class="headerlink" href="#133-batches-and-epochs" title="Permanent link">&para;</a></h3>
<p>An epoch is the number of times the model will see the entire training set</p>
<p>A batch is the number of training samples the model will run before calculating a total error and updating its internal parameters. </p>
<p>Variability of batch (from 1 sample all the way to the entire training set size) leads to different categorizations of the optimizer algorithm:</p>
<ul>
<li>Batch Gradient Descent. Batch Size = Size of Training Set</li>
<li>Stochastic Gradient Descent. Batch Size = 1</li>
<li>Mini-Batch Gradient Descent. 1 &lt; Batch Size &lt; Size of Training Set</li>
</ul>
<p>We will visit additional details about batch and epochs in the next session when we discuss model evaluation.</p>
<p><a name='x.3.3'></a></p>
<h3 id="134-learning-rate">1.3.4 Learning Rate<a class="headerlink" href="#134-learning-rate" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>notice how in the above animation the model makes progressive steps toward a global optimum. The size of these steps is determined by the <em>learning rate</em>. You can think of it as the amount of improvement to make in the direction of steepest descent (the derivative of the loss function in regards to the changes in weights). Sometimes a large step size can result in stepping over crevices in the solution surface and getting stuck, while too small of step sizes can lead to a slow algorithm. Often the optimum learning rate is not obvious, luckily there are some optimizers that are self-calibrating in this regard. <em>Adam</em> is one such optimizer available to us in keras. </p>
<div class="codehilite"><pre><span></span><code><span class="c1"># we can compile the model like so</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mae&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<h3 id="exercise-51-train-your-first-neural-networks">🏋️ Exercise 5.1: Train your first Neural Networks<a class="headerlink" href="#exercise-51-train-your-first-neural-networks" title="Permanent link">&para;</a></h3>
<p><a href="#top">back to top</a></p>
<p>We're going to train our first neural network.</p>
<p>Take the model you created in exercise 3 and paste it in the cell below. Make sure that the <code>input_shape</code> of the first layer matches the number of features in <code>X_train_std</code></p>
<div class="codehilite"><pre><span></span><code><span class="n">X_train_std</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>13
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Code cell for exercise 5</span>

<span class="c1"># DECLARE THE MODEL</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>

    <span class="c1">### YOUR CODE HERE ###</span>

    <span class="c1"># the hidden ReLU layers</span>

    <span class="c1"># the linear output layer </span>

<span class="p">])</span>
</code></pre></div>

<p>Now we'll compile the model</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p>And then train for 10 epochs</p>
<div class="codehilite"><pre><span></span><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train_std</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">,</span> <span class="n">y_val_std</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Epoch 1/10
15/15 [==============================] - 1s 33ms/step - loss: 0.6389 - val_loss: 0.1925
Epoch 2/10
15/15 [==============================] - 0s 22ms/step - loss: 0.1330 - val_loss: 0.1054
Epoch 3/10
15/15 [==============================] - 0s 22ms/step - loss: 0.0739 - val_loss: 0.0587
Epoch 4/10
15/15 [==============================] - 0s 23ms/step - loss: 0.0442 - val_loss: 0.0378
Epoch 5/10
15/15 [==============================] - 0s 22ms/step - loss: 0.0266 - val_loss: 0.0283
Epoch 6/10
15/15 [==============================] - 0s 23ms/step - loss: 0.0193 - val_loss: 0.0233
Epoch 7/10
15/15 [==============================] - 0s 23ms/step - loss: 0.0165 - val_loss: 0.0212
Epoch 8/10
15/15 [==============================] - 0s 22ms/step - loss: 0.0151 - val_loss: 0.0204
Epoch 9/10
15/15 [==============================] - 0s 22ms/step - loss: 0.0144 - val_loss: 0.0199
Epoch 10/10
15/15 [==============================] - 0s 23ms/step - loss: 0.0140 - val_loss: 0.0194
</code></pre></div>

<p>Let's take a look at our training history:</p>
<div class="codehilite"><pre><span></span><code><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>val_loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.638868</td>
      <td>0.192506</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.133038</td>
      <td>0.105378</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.073934</td>
      <td>0.058686</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.044192</td>
      <td>0.037832</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.026648</td>
      <td>0.028253</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.019265</td>
      <td>0.023272</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.016508</td>
      <td>0.021151</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.015126</td>
      <td>0.020368</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.014371</td>
      <td>0.019929</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.014009</td>
      <td>0.019429</td>
    </tr>
  </tbody>
</table>
</div>

<div class="codehilite"><pre><span></span><code><span class="c1"># convert the training history to a dataframe</span>
<span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="c1"># use Pandas native plot method</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">history_df</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">history_df</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">.05</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>(0.0, 0.05)
</code></pre></div>

<p><img alt="png" src="../S1_Multilayer_Perceptron_files/S1_Multilayer_Perceptron_46_1.png" /></p>
<h3 id="exercise-52-improve-loss-by-varying-nodes-and-hidden-layers">🏋️ Exercise 5.2: Improve loss by varying nodes and hidden layers<a class="headerlink" href="#exercise-52-improve-loss-by-varying-nodes-and-hidden-layers" title="Permanent link">&para;</a></h3>
<p>Take your former model as a starting point and now either add nodes or layers to see if the model improves</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1">### YOUR CODE HERE ###</span>

<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train_std</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">,</span> <span class="n">y_val_std</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Epoch 1/10
15/15 [==============================] - 1s 11ms/step - loss: 3.0090 - val_loss: 1.9299
Epoch 2/10
15/15 [==============================] - 0s 4ms/step - loss: 1.3536 - val_loss: 0.7737
Epoch 3/10
15/15 [==============================] - 0s 3ms/step - loss: 0.4739 - val_loss: 0.2300
Epoch 4/10
15/15 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.0781
Epoch 5/10
15/15 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0471
Epoch 6/10
15/15 [==============================] - 0s 4ms/step - loss: 0.0402 - val_loss: 0.0354
Epoch 7/10
15/15 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0325
Epoch 8/10
15/15 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0305
Epoch 9/10
15/15 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0288
Epoch 10/10
15/15 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0271
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_test_std</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>[&lt;matplotlib.lines.Line2D at 0x7f5d26303e10&gt;]
</code></pre></div>

<p><img alt="png" src="../S1_Multilayer_Perceptron_files/S1_Multilayer_Perceptron_49_1.png" /></p>
<h3 id="exercise-53-learning-curves">🏋️ Exercise 5.3: Learning Curves<a class="headerlink" href="#exercise-53-learning-curves" title="Permanent link">&para;</a></h3>
<p>Using 4 hidden layers now create 4 models that run for 30 epochs each:</p>
<ol>
<li>Vary the number of nodes in each layer</li>
<li>Record the train/val/test score (MSE)</li>
<li>Plot either total nodes or total trainable parameters vs score for each of the 5 models</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code Cell for Exercise 1.3.5</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tot. units</th>
      <th>test mse</th>
      <th>val mse</th>
      <th>train mse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4</td>
      <td>0.030658</td>
      <td>0.030919</td>
      <td>0.031088</td>
    </tr>
    <tr>
      <th>1</th>
      <td>25</td>
      <td>0.020217</td>
      <td>0.020164</td>
      <td>0.019062</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100</td>
      <td>0.017429</td>
      <td>0.018110</td>
      <td>0.017416</td>
    </tr>
    <tr>
      <th>3</th>
      <td>289</td>
      <td>0.019124</td>
      <td>0.019371</td>
      <td>0.018190</td>
    </tr>
  </tbody>
</table>
</div>

<p>When we look at our historical loss do we notice that sometimes before the last epoch we actually hit a minimum? We'll discuss how to deal with this in the next session!</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../about/" class="md-footer__link md-footer__link--prev" aria-label="Previous: About" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              About
            </div>
          </div>
        </a>
      
      
        
        <a href="../S2_Feed_Forward_Neural_Networks/" class="md-footer__link md-footer__link--next" aria-label="Next: Feed Forward Neural Networks" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Feed Forward Neural Networks
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.361d90f1.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.289a2a4b.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>