{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesleybeckner/general_applications_of_neural_networks/blob/main/notebooks/extras/X1_Tictactoe_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDtoMTg4U_Pr"
      },
      "source": [
        "# General Applications of Neural Networks <br> X1: Reinforcement Learning Based Agents\n",
        "\n",
        "**Instructor**: Wesley Beckner\n",
        "\n",
        "**Contact**: wesleybeckner@gmail.com\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "In this lesson we'll abandon the world of heuristical agents and embrace the wilds of reinforcement learning\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNtJitcRW51Y"
      },
      "source": [
        "<a name='x.0'></a>\n",
        "\n",
        "## 1.0 Preparing Environment and Importing Data\n",
        "\n",
        "[back to top](#top)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chdcBoBL8SNm"
      },
      "source": [
        "<a name='x.0.1'></a>\n",
        "\n",
        "### 1.0.1 Import Packages\n",
        "\n",
        "[back to top](#top)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tf5mlWGsz0d"
      },
      "source": [
        "baselines requires an older version of TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkUdduscnZKE",
        "outputId": "280b4f93-182b-4f58-b3f9-e6d14f660dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 25 kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 44.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.43.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e3a2fe22299f0c6d241d9b04ee068f2d4963266bcdd239d3b3e261e433003838\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==1.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rjsg2-Zswx7"
      },
      "source": [
        "install baselines from openAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGFA6SQLnbyY",
        "outputId": "57ad750d-1c24-4793-9cf0-26a712d5c6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [930 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [806 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,474 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,596 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,827 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,035 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [937 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,252 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [840 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Fetched 15.0 MB in 5s (2,992 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "python3-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 67 not upgraded.\n",
            "Collecting stable-baselines[mpi]==2.9.0\n",
            "  Downloading stable_baselines-2.9.0-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (4.1.2.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (1.1.0)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.9.0) (1.21.5)\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 17.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.9.0) (2018.9)\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185292 sha256=201eb8ee46e6f84a098ad3d83af5413f76be954aab911419894bec8346068c61\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: stable-baselines, mpi4py\n",
            "Successfully installed mpi4py-3.1.3 stable-baselines-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
        "!pip install \"stable-baselines[mpi]==2.9.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aQnUfVzt1oBK",
        "outputId": "7d51138c-d502-402a-e01d-8aa4b6980db0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Check version of tensorflow\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM9VnGgJnea7",
        "outputId": "0a5449c0-a55a-45bb-c927-b6c5e70ac98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from gym import spaces\n",
        "import gym\n",
        "from stable_baselines.common.env_checker import check_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ7-FDq3JhhI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def n_step_ai(board, win_patterns, player_label, n_steps=3):\n",
        "  opponent = ['X', 'O']\n",
        "  opponent.remove(player_label)\n",
        "  opponent = opponent[0]\n",
        "\n",
        "  avail_moves = {i: 1 for i in board.keys() if board[i] == ' '}\n",
        "  \n",
        "  for move in avail_moves.keys():\n",
        "    temp_board = board.copy()\n",
        "    temp_board[move] = player_label\n",
        "    score = get_minimax(n_steps, temp_board, player_label)\n",
        "    avail_moves[move] = score\n",
        "\n",
        "  ##########################################\n",
        "  ### The rest of our ai agent harness is the same\n",
        "  ##########################################\n",
        "\n",
        "  # first grab max score\n",
        "  max_score = max(avail_moves.values())\n",
        "\n",
        "  # then select all moves that have this max score\n",
        "  valid = []\n",
        "  for key, value in avail_moves.items():\n",
        "    if value == max_score:\n",
        "      valid.append(key)\n",
        "\n",
        "  # return a random selection of the moves with the max score\n",
        "  move = random.choice(valid)\n",
        "\n",
        "  return move\n",
        "\n",
        "def minimax(depth, board, maximizing_player, player_label, verbiose=False):\n",
        "  # infer the opponent\n",
        "  opponent = ['X', 'O']\n",
        "  opponent.remove(player_label)\n",
        "  opponent = opponent[0]\n",
        "\n",
        "  # set the available moves\n",
        "  avail_moves = [i for i in board.keys() if board[i] == ' ']\n",
        "\n",
        "  # check if the depth is 0, or stalemate/winner has been reached\n",
        "  # if so this is the basecase and we want to return get_score()\n",
        "  terminal_move = is_terminal_node(board, avail_moves)\n",
        "\n",
        "  if terminal_move or depth == 0:\n",
        "    score = get_score(board, player_label, win_patterns)\n",
        "    if verbiose:\n",
        "      print('{} score: {}. depth: {}'.format(board, score, depth))\n",
        "    return score\n",
        "  \n",
        "  ### in the following we want to search through every possible board at the \n",
        "  ### current level (the possible moves for the current player, given that the\n",
        "  ### player is either the one whose turn it is or the imagined opponent)\n",
        "\n",
        "  # call minimax where it is the current players turn and so we want to \n",
        "  # maximize the score\n",
        "  if maximizing_player:\n",
        "    score = -np.Inf\n",
        "    for move in avail_moves:\n",
        "      new_board = board.copy()\n",
        "      new_board[move] = player_label\n",
        "      score = max(score, minimax(depth-1, new_board, False, player_label, verbiose))\n",
        "    if verbiose:\n",
        "      print('{} max. score: {}. depth: {}'.format(board, score, depth))\n",
        "    return score\n",
        "\n",
        "  # call minimax where it is the opponent players turn and so we want to\n",
        "  # minimize the score\n",
        "  elif not maximizing_player:\n",
        "    score = np.Inf\n",
        "    for move in avail_moves:\n",
        "      new_board = board.copy()\n",
        "      new_board[move] = opponent\n",
        "      score = min(score, minimax(depth-1, new_board, True, player_label, verbiose))\n",
        "    if verbiose:\n",
        "      print('{} min. score: {}. depth: {}'.format(board, score, depth))\n",
        "    return score\n",
        "\n",
        "def is_terminal_node(board, avail_moves):\n",
        "  if check_winning(board, win_patterns):\n",
        "    return True\n",
        "  elif check_stalemate(board, win_patterns):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def get_score(board, player_label, win_patterns):\n",
        "  # this will look somewhat similar to our 1-step lookahead algorithm\n",
        "  opponent = ['X', 'O']\n",
        "  opponent.remove(player_label)\n",
        "  opponent = opponent[0]\n",
        "  score = 0\n",
        "  for pattern in win_patterns:\n",
        "      values = [board[i] for i in pattern] \n",
        "      # if the opponent wins, the score is -100\n",
        "      if values == [opponent, opponent, opponent]:\n",
        "        score = -100\n",
        "      elif values == [player_label, player_label, player_label]:\n",
        "        score = 100\n",
        "  return score\n",
        "\n",
        "# we're going to pull out and reformat some of our helper functions in the\n",
        "# TicTacToe class\n",
        "\n",
        "win_patterns = [[1,2,3], [4,5,6], [7,8,9],\n",
        "                [1,4,7], [2,5,8], [3,6,9],\n",
        "                [1,5,9], [7,5,3]]\n",
        "\n",
        "def check_winning(board, win_patterns):\n",
        "  for pattern in win_patterns:\n",
        "    values = [board[i] for i in pattern] \n",
        "    if values == ['X', 'X', 'X'] or values == ['O', 'O', 'O']:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def check_stalemate(board, win_patterns):\n",
        "  if (' ' not in board.values()) and (check_winning(board, win_patterns) == ''):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def get_minimax(depth, board, player_label, verbiose=False):\n",
        "  score = minimax(depth-1, board, False, player_label, verbiose=verbiose)\n",
        "  return score\n",
        "\n",
        "def n_step_ai_temp(board, win_patterns, player_label, n_steps, verbiose=False):\n",
        "  opponent = ['X', 'O']\n",
        "  opponent.remove(player_label)\n",
        "  opponent = opponent[0]\n",
        "\n",
        "  avail_moves = {i: 1 for i in board.keys() if board[i] == ' '}\n",
        "  \n",
        "  for move in avail_moves.keys():\n",
        "    temp_board = board.copy()\n",
        "    temp_board[move] = player_label\n",
        "    score = get_minimax(n_steps, temp_board, player_label, verbiose=verbiose)\n",
        "    avail_moves[move] = score\n",
        "  return avail_moves\n",
        "\n",
        "def one_step_ai(board, win_patterns, player_label):\n",
        "  opponent = ['X', 'O']\n",
        "  opponent.remove(player_label)\n",
        "  opponent = opponent[0]\n",
        "\n",
        "  avail_moves = {i: 1 for i in board.keys() if board[i] == ' '}\n",
        "  temp_board = board.copy()\n",
        "  ########################################\n",
        "  # we're going to change the following lines, instead of caring\n",
        "  # whether we've found the best move, we want to update the move\n",
        "  # with a score\n",
        "  ########################################\n",
        "\n",
        "  # check if the opponent has a winning move first, we will overwrite\n",
        "  # the score for this move if it is also a winning move for the current \n",
        "  # player\n",
        "  for move in avail_moves.keys():\n",
        "    temp_board[move] = opponent\n",
        "    for pattern in win_patterns:\n",
        "        values = [temp_board[i] for i in pattern] \n",
        "        if values == [opponent, opponent, opponent]:\n",
        "          avail_moves[move] = 10\n",
        "    temp_board[move] = ' '\n",
        "\n",
        "  for move in avail_moves.keys():\n",
        "    temp_board[move] = player_label\n",
        "    for pattern in win_patterns:\n",
        "        values = [temp_board[i] for i in pattern] \n",
        "        if values == [player_label, player_label, player_label]:\n",
        "          avail_moves[move] = 100\n",
        "    temp_board[move] = ' '\n",
        "\n",
        "  # first grab max score\n",
        "  max_score = max(avail_moves.values())\n",
        "\n",
        "  # then select all moves that have this max score\n",
        "  valid = []\n",
        "  for key, value in avail_moves.items():\n",
        "    if value == max_score:\n",
        "      valid.append(key)\n",
        "\n",
        "  # return a random selection of the moves with the max score\n",
        "  move = random.choice(valid)\n",
        "\n",
        "  return move\n",
        "\n",
        "class TicTacToe:\n",
        "  # can preset winner and starting player\n",
        "  def __init__(self, winner='', start_player=''): \n",
        "    self.winner = winner\n",
        "    self.start_player = start_player\n",
        "    self.board = {1: ' ',\n",
        "         2: ' ',\n",
        "         3: ' ',\n",
        "         4: ' ',\n",
        "         5: ' ',\n",
        "         6: ' ',\n",
        "         7: ' ',\n",
        "         8: ' ',\n",
        "         9: ' ',}\n",
        "    self.win_patterns = [[1,2,3], [4,5,6], [7,8,9],\n",
        "                [1,4,7], [2,5,8], [3,6,9],\n",
        "                [1,5,9], [7,5,3]]\n",
        "         \n",
        "  # the other functions are now passed self\n",
        "  def visualize_board(self):\n",
        "    print(\n",
        "      \"|{}|{}|{}|\\n|{}|{}|{}|\\n|{}|{}|{}|\\n\".format(*self.board.values())\n",
        "      )\n",
        "\n",
        "  def check_winning(self):\n",
        "    for pattern in self.win_patterns:\n",
        "      values = [self.board[i] for i in pattern] \n",
        "      if values == ['X', 'X', 'X']:\n",
        "        self.winner = 'X' # we update the winner status\n",
        "        return \"'X' Won!\"\n",
        "      elif values == ['O', 'O', 'O']:\n",
        "        self.winner = 'O'\n",
        "        return \"'O' Won!\"\n",
        "    return ''\n",
        "\n",
        "  def check_stalemate(self):\n",
        "    if (' ' not in self.board.values()) and (self.check_winning() == ''):\n",
        "      self.winner = 'Stalemate'\n",
        "      return \"It's a stalemate!\"\n",
        "\n",
        "class GameEngine(TicTacToe):\n",
        "  def __init__(self, setup='auto', user_ai=None):\n",
        "    super().__init__()\n",
        "    self.setup = setup\n",
        "    self.user_ai = user_ai\n",
        "\n",
        "  def heuristic_ai(self, player_label):\n",
        "    opponent = ['X', 'O']\n",
        "    opponent.remove(player_label)\n",
        "    opponent = opponent[0]\n",
        "\n",
        "    avail_moves = [i for i in self.board.keys() if self.board[i] == ' ']\n",
        "    temp_board = self.board.copy()\n",
        "    middle = 5\n",
        "    corner = [1,3,7,9]\n",
        "    side = [2,4,6,8]\n",
        "\n",
        "    # first check for a winning move\n",
        "    move_found = False\n",
        "    for move in avail_moves:\n",
        "      temp_board[move] = player_label\n",
        "      for pattern in self.win_patterns:\n",
        "          values = [temp_board[i] for i in pattern] \n",
        "          if values == [player_label, player_label, player_label]:\n",
        "            move_found = True       \n",
        "            break\n",
        "      if move_found:   \n",
        "        break\n",
        "      else:\n",
        "        temp_board[move] = ' '\n",
        "\n",
        "    # check if the opponent has a winning move\n",
        "    if move_found == False:\n",
        "      for move in avail_moves:\n",
        "        temp_board[move] = opponent\n",
        "        for pattern in self.win_patterns:\n",
        "            values = [temp_board[i] for i in pattern] \n",
        "            if values == [opponent, opponent, opponent]:\n",
        "              move_found = True       \n",
        "              break\n",
        "        if move_found:   \n",
        "          break\n",
        "        else:\n",
        "          temp_board[move] = ' '\n",
        "\n",
        "    # check if middle avail\n",
        "    if move_found == False:\n",
        "      if middle in avail_moves:\n",
        "        move_found = True\n",
        "        move = middle\n",
        "\n",
        "    # check corners\n",
        "    if move_found == False:\n",
        "      move_corner = [val for val in avail_moves if val in corner]\n",
        "      if len(move_corner) > 0:\n",
        "        move = random.choice(move_corner)\n",
        "        move_found = True\n",
        "\n",
        "    # check side\n",
        "    if move_found == False:\n",
        "      move_side = [val for val in avail_moves if val in side]\n",
        "      if len(move_side) > 0:\n",
        "        move = random.choice(move_side)\n",
        "        move_found = True\n",
        "\n",
        "    return move\n",
        "\n",
        "  def random_ai(self):\n",
        "    while True:\n",
        "      move = random.randint(1,9)\n",
        "      if self.board[move] != ' ':\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    return move\n",
        "\n",
        "  def setup_game(self):\n",
        "\n",
        "    if self.setup == 'user':\n",
        "      players = int(input(\"How many Players? (type 0, 1, or 2)\"))\n",
        "      self.player_meta = {'first': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                    'second': {'label': 'O',\n",
        "                                    'type': 'human'}}\n",
        "      if players != 2:\n",
        "        ########## \n",
        "        # Allow the user to set the ai level\n",
        "        ########## \n",
        "\n",
        "        ### if they have not provided an ai_agent\n",
        "        if self.user_ai == None:\n",
        "          level = int(input(\"select AI level (1, 2)\"))\n",
        "          if level == 1:\n",
        "            self.ai_level = 1\n",
        "          elif level == 2:\n",
        "            self.ai_level = 2\n",
        "          else:\n",
        "            print(\"Unknown AI level entered, this will cause problems\")\n",
        "        else:\n",
        "          self.ai_level = 3\n",
        "\n",
        "      if players == 1:\n",
        "        first = input(\"who will go first? (X, (AI), or O (Player))\")\n",
        "        if first == 'O':\n",
        "          self.player_meta = {'second': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'first': {'label': 'O',\n",
        "                                    'type': 'human'}}\n",
        "        \n",
        "\n",
        "      elif players == 0:\n",
        "        first = random.choice(['X', 'O'])\n",
        "        if first == 'O':\n",
        "          self.player_meta = {'second': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'first': {'label': 'O',\n",
        "                                    'type': 'ai'}}                                \n",
        "        else:\n",
        "          self.player_meta = {'first': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'second': {'label': 'O',\n",
        "                                    'type': 'ai'}}\n",
        "\n",
        "        \n",
        "    elif self.setup == 'auto':\n",
        "      first = random.choice(['X', 'O'])\n",
        "      if first == 'O':\n",
        "        self.start_player = 'O'\n",
        "        self.player_meta = {'second': {'label': 'X',\n",
        "                                  'type': 'ai'}, \n",
        "                      'first': {'label': 'O',\n",
        "                                  'type': 'ai'}}                                \n",
        "      else:\n",
        "        self.start_player = 'X'\n",
        "        self.player_meta = {'first': {'label': 'X',\n",
        "                                  'type': 'ai'}, \n",
        "                      'second': {'label': 'O',\n",
        "                                  'type': 'ai'}}\n",
        "      ########## \n",
        "      # and automatically set the ai level otherwise\n",
        "      ##########  \n",
        "      if self.user_ai == None:                           \n",
        "        self.ai_level = 2\n",
        "      else:\n",
        "        self.ai_level = 3\n",
        "\n",
        "  def play_game(self):\n",
        "    while True:\n",
        "      for player in ['first', 'second']:  \n",
        "        self.visualize_board()\n",
        "        player_label = self.player_meta[player]['label']\n",
        "        player_type = self.player_meta[player]['type']\n",
        "\n",
        "        if player_type == 'human':\n",
        "          move = input(\"{}, what's your move?\".format(player_label))\n",
        "          # we're going to allow the user to quit the game from the input line\n",
        "          if move in ['q', 'quit']:\n",
        "            self.winner = 'F'\n",
        "            print('quiting the game')\n",
        "            break\n",
        "\n",
        "          move = int(move)\n",
        "          if self.board[move] != ' ':\n",
        "            while True:\n",
        "              move = input(\"{}, that position is already taken! \"\\\n",
        "                          \"What's your move?\".format(player_label))  \n",
        "              move = int(move)            \n",
        "              if self.board[move] != ' ':\n",
        "                continue\n",
        "              else:\n",
        "                break\n",
        "\n",
        "        else:\n",
        "          ##########\n",
        "          # Our level 1 ai agent (random)\n",
        "          ##########\n",
        "          if self.ai_level == 1:\n",
        "            move = self.random_ai()\n",
        "\n",
        "          ##########\n",
        "          # Our level 2 ai agent (heuristic)\n",
        "          ##########\n",
        "          elif self.ai_level == 2:\n",
        "            move = self.heuristic_ai(player_label)\n",
        "\n",
        "          ##########\n",
        "          # Our user-defined AI agent\n",
        "          ##########\n",
        "          elif self.ai_level == 3:\n",
        "            move = self.user_ai(self.board, self.win_patterns, player_label)\n",
        "\n",
        "        self.board[move] = player_label\n",
        "\n",
        "        # the winner varaible will now be check within the board object\n",
        "        self.check_winning()\n",
        "        self.check_stalemate()\n",
        "\n",
        "        if self.winner == '':\n",
        "          continue\n",
        "\n",
        "        elif self.winner == 'Stalemate':\n",
        "          print(self.check_stalemate())\n",
        "          self.visualize_board()\n",
        "          break\n",
        "\n",
        "        else:\n",
        "          print(self.check_winning())\n",
        "          self.visualize_board()\n",
        "          break\n",
        "      if self.winner != '':\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWrl1Nm8ZIVK"
      },
      "source": [
        "### 1.0.2 Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrrF2WCPSHch"
      },
      "outputs": [],
      "source": [
        "def test_n_step_ai():\n",
        "  random.seed(42)\n",
        "  game = GameEngine(setup='auto', user_ai=n_step_ai)\n",
        "  game.setup_game()\n",
        "  game.play_game()\n",
        "  # check that the winner is X\n",
        "  assert game.winner == 'X', \"Winner should be X!\"\n",
        "\n",
        "  # check that the ai level is set to 3 which means our engine is properly\n",
        "  # accessing the user defined ai\n",
        "  assert game.ai_level == 3, \"The engine is not using the user defined AI!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iksp5Y9K1Au0",
        "outputId": "8ade5061-3e36-4867-9025-496ef832cd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "|X| | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "|X| | |\n",
            "| | |O|\n",
            "| | | |\n",
            "\n",
            "|X| |X|\n",
            "| | |O|\n",
            "| | | |\n",
            "\n",
            "|X|O|X|\n",
            "| | |O|\n",
            "| | | |\n",
            "\n",
            "|X|O|X|\n",
            "| |X|O|\n",
            "| | | |\n",
            "\n",
            "|X|O|X|\n",
            "|O|X|O|\n",
            "| | | |\n",
            "\n",
            "'X' Won!\n",
            "|X|O|X|\n",
            "|O|X|O|\n",
            "| | |X|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_n_step_ai()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP4TbDZ_amsy"
      },
      "source": [
        "## 1.1 Reinforcement Learning: Reset, Step, and Reward\n",
        "\n",
        "Firstly, to interact with OpenAI Gym, we need to include a method of reseting the current game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br_FeuHbJSYH"
      },
      "source": [
        "### 1.1.2 Reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GskFC06yZbIm"
      },
      "outputs": [],
      "source": [
        "class GameEngine(TicTacToe):\n",
        "  def __init__(self, setup='auto', user_ai=None):\n",
        "    super().__init__()\n",
        "    self.setup = setup\n",
        "    self.user_ai = user_ai\n",
        "\n",
        "  def heuristic_ai(self, player_label):\n",
        "    opponent = ['X', 'O']\n",
        "    opponent.remove(player_label)\n",
        "    opponent = opponent[0]\n",
        "\n",
        "    avail_moves = [i for i in self.board.keys() if self.board[i] == ' ']\n",
        "    temp_board = self.board.copy()\n",
        "    middle = 5\n",
        "    corner = [1,3,7,9]\n",
        "    side = [2,4,6,8]\n",
        "\n",
        "    # first check for a winning move\n",
        "    move_found = False\n",
        "    for move in avail_moves:\n",
        "      temp_board[move] = player_label\n",
        "      for pattern in self.win_patterns:\n",
        "          values = [temp_board[i] for i in pattern] \n",
        "          if values == [player_label, player_label, player_label]:\n",
        "            move_found = True       \n",
        "            break\n",
        "      if move_found:   \n",
        "        break\n",
        "      else:\n",
        "        temp_board[move] = ' '\n",
        "\n",
        "    # check if the opponent has a winning move\n",
        "    if move_found == False:\n",
        "      for move in avail_moves:\n",
        "        temp_board[move] = opponent\n",
        "        for pattern in self.win_patterns:\n",
        "            values = [temp_board[i] for i in pattern] \n",
        "            if values == [opponent, opponent, opponent]:\n",
        "              move_found = True       \n",
        "              break\n",
        "        if move_found:   \n",
        "          break\n",
        "        else:\n",
        "          temp_board[move] = ' '\n",
        "\n",
        "    # check if middle avail\n",
        "    if move_found == False:\n",
        "      if middle in avail_moves:\n",
        "        move_found = True\n",
        "        move = middle\n",
        "\n",
        "    # check corners\n",
        "    if move_found == False:\n",
        "      move_corner = [val for val in avail_moves if val in corner]\n",
        "      if len(move_corner) > 0:\n",
        "        move = random.choice(move_corner)\n",
        "        move_found = True\n",
        "\n",
        "    # check side\n",
        "    if move_found == False:\n",
        "      move_side = [val for val in avail_moves if val in side]\n",
        "      if len(move_side) > 0:\n",
        "        move = random.choice(move_side)\n",
        "        move_found = True\n",
        "\n",
        "    return move\n",
        "\n",
        "  def random_ai(self):\n",
        "    while True:\n",
        "      move = random.randint(1,9)\n",
        "      if self.board[move] != ' ':\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    return move\n",
        "\n",
        "  def setup_game(self):\n",
        "    if self.setup == 'user':\n",
        "      players = int(input(\"How many Players? (type 0, 1, or 2)\"))\n",
        "      self.player_meta = {'first': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                    'second': {'label': 'O',\n",
        "                                    'type': 'human'}}\n",
        "      if players != 2:\n",
        "        ########## \n",
        "        # Allow the user to set the ai level\n",
        "        ########## \n",
        "\n",
        "        ### if they have not provided an ai_agent\n",
        "        if self.user_ai == None:\n",
        "          level = int(input(\"select AI level (1, 2)\"))\n",
        "          if level == 1:\n",
        "            self.ai_level = 1\n",
        "          elif level == 2:\n",
        "            self.ai_level = 2\n",
        "          else:\n",
        "            print(\"Unknown AI level entered, this will cause problems\")\n",
        "        else:\n",
        "          self.ai_level = 3\n",
        "\n",
        "      if players == 1:\n",
        "        first = input(\"who will go first? (X, (AI), or O (Player))\")\n",
        "        if first == 'O':\n",
        "          self.player_meta = {'second': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'first': {'label': 'O',\n",
        "                                    'type': 'human'}}\n",
        "        \n",
        "\n",
        "      elif players == 0:\n",
        "        first = random.choice(['X', 'O'])\n",
        "        if first == 'O':\n",
        "          self.player_meta = {'second': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'first': {'label': 'O',\n",
        "                                    'type': 'ai'}}                                \n",
        "        else:\n",
        "          self.player_meta = {'first': {'label': 'X',\n",
        "                                    'type': 'ai'}, \n",
        "                        'second': {'label': 'O',\n",
        "                                    'type': 'ai'}}\n",
        "\n",
        "        \n",
        "    elif self.setup == 'auto':\n",
        "      first = random.choice(['X', 'O'])\n",
        "      if first == 'O':\n",
        "        self.start_player = 'O'\n",
        "        self.player_meta = {'second': {'label': 'X',\n",
        "                                  'type': 'ai'}, \n",
        "                      'first': {'label': 'O',\n",
        "                                  'type': 'ai'}}                                \n",
        "      else:\n",
        "        self.start_player = 'X'\n",
        "        self.player_meta = {'first': {'label': 'X',\n",
        "                                  'type': 'ai'}, \n",
        "                      'second': {'label': 'O',\n",
        "                                  'type': 'ai'}}\n",
        "      ########## \n",
        "      # and automatically set the ai level otherwise\n",
        "      ##########  \n",
        "      if self.user_ai == None:                           \n",
        "        self.ai_level = 2\n",
        "      else:\n",
        "        self.ai_level = 3\n",
        "  \n",
        "  def play_game(self):\n",
        "    while True:\n",
        "      for player in ['first', 'second']:  \n",
        "        self.visualize_board()\n",
        "        player_label = self.player_meta[player]['label']\n",
        "        player_type = self.player_meta[player]['type']\n",
        "\n",
        "        if player_type == 'human':\n",
        "          move = input(\"{}, what's your move?\".format(player_label))\n",
        "          # we're going to allow the user to quit the game from the input line\n",
        "          if move in ['q', 'quit']:\n",
        "            self.winner = 'F'\n",
        "            print('quiting the game')\n",
        "            break\n",
        "\n",
        "          move = int(move)\n",
        "          if self.board[move] != ' ':\n",
        "            while True:\n",
        "              move = input(\"{}, that position is already taken! \"\\\n",
        "                          \"What's your move?\".format(player_label))  \n",
        "              move = int(move)            \n",
        "              if self.board[move] != ' ':\n",
        "                continue\n",
        "              else:\n",
        "                break\n",
        "\n",
        "        else:\n",
        "          ##########\n",
        "          # Our level 1 ai agent (random)\n",
        "          ##########\n",
        "          if self.ai_level == 1:\n",
        "            move = self.random_ai()\n",
        "\n",
        "          ##########\n",
        "          # Our level 2 ai agent (heuristic)\n",
        "          ##########\n",
        "          elif self.ai_level == 2:\n",
        "            move = self.heuristic_ai(player_label)\n",
        "\n",
        "          ##########\n",
        "          # Our user-defined AI agent\n",
        "          ##########\n",
        "          elif self.ai_level == 3:\n",
        "            move = self.user_ai(self.board, self.win_patterns, player_label)\n",
        "\n",
        "        self.board[move] = player_label\n",
        "\n",
        "        # the winner varaible will now be check within the board object\n",
        "        self.check_winning()\n",
        "        self.check_stalemate()\n",
        "\n",
        "        if self.winner == '':\n",
        "          continue\n",
        "\n",
        "        elif self.winner == 'Stalemate':\n",
        "          print(self.check_stalemate())\n",
        "          self.visualize_board()\n",
        "          break\n",
        "\n",
        "        else:\n",
        "          print(self.check_winning())\n",
        "          self.visualize_board()\n",
        "          break\n",
        "      if self.winner != '':\n",
        "        return self\n",
        "\n",
        "  ####################################\n",
        "  # Adding our ability to reset the game\n",
        "  ####################################\n",
        "  def reset_game(self):\n",
        "    self.board = {1: ' ',\n",
        "         2: ' ',\n",
        "         3: ' ',\n",
        "         4: ' ',\n",
        "         5: ' ',\n",
        "         6: ' ',\n",
        "         7: ' ',\n",
        "         8: ' ',\n",
        "         9: ' ',}\n",
        "    self.winner = ''\n",
        "    self.setup_game()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywUNJ-WnJzJQ"
      },
      "source": [
        "Let's test our reset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UjAbF9iLGwF",
        "outputId": "15f185ad-b6bd-4934-b0b2-0be921a83be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "| | | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "| | | |\n",
            "| |X| |\n",
            "| | |O|\n",
            "\n",
            "|X| | |\n",
            "| |X| |\n",
            "| | |O|\n",
            "\n",
            "|X| |O|\n",
            "| |X| |\n",
            "| | |O|\n",
            "\n",
            "|X| |O|\n",
            "| |X|X|\n",
            "| | |O|\n",
            "\n",
            "|X| |O|\n",
            "|O|X|X|\n",
            "| | |O|\n",
            "\n",
            "|X| |O|\n",
            "|O|X|X|\n",
            "|X| |O|\n",
            "\n",
            "|X|O|O|\n",
            "|O|X|X|\n",
            "|X| |O|\n",
            "\n",
            "It's a stalemate!\n",
            "|X|O|O|\n",
            "|O|X|X|\n",
            "|X|X|O|\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GameEngine at 0x7f8d4b163bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "game = GameEngine('auto')\n",
        "game.setup_game()\n",
        "game.play_game()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YkdyyoQLEhm",
        "outputId": "3d536bf4-f2bb-4bb7-a7c7-0a450b6e8ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "| | | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "| | |O|\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "| | |O|\n",
            "| |X| |\n",
            "| | |X|\n",
            "\n",
            "|O| |O|\n",
            "| |X| |\n",
            "| | |X|\n",
            "\n",
            "|O|X|O|\n",
            "| |X| |\n",
            "| | |X|\n",
            "\n",
            "|O|X|O|\n",
            "| |X| |\n",
            "| |O|X|\n",
            "\n",
            "|O|X|O|\n",
            "| |X| |\n",
            "|X|O|X|\n",
            "\n",
            "|O|X|O|\n",
            "|O|X| |\n",
            "|X|O|X|\n",
            "\n",
            "It's a stalemate!\n",
            "|O|X|O|\n",
            "|O|X|X|\n",
            "|X|O|X|\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GameEngine at 0x7f8d4b163bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "game.reset_game()\n",
        "game.play_game()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht8u2WuFGCIr"
      },
      "source": [
        "This `reset_game` function works the way we intend. However, the big step we will have to make from our current tic tac toe module to one usable by OpenAI is to work with integers rather than strings in our board representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdD7EknmEywr"
      },
      "source": [
        "### 1.1.3 Observation and Action Spaces\n",
        "\n",
        "The following are the important changes we will have to make to our game class in order to work with OpenAI's built-in reinforcement learning algorithms:\n",
        "\n",
        "```\n",
        "# the board now has integers as values instead of strings\n",
        "self.board = {1: 0,\n",
        "      2: 0,\n",
        "      3: 0,\n",
        "      4: 0,\n",
        "      5: 0,\n",
        "      6: 0,\n",
        "      7: 0,\n",
        "      8: 0,\n",
        "      9: 0,}\n",
        "\n",
        "# the available token spaces, note that in order to access our board\n",
        "# dictionary these actions will need to be re-indexed to 1\n",
        "self.action_space = spaces.Discrete(9)\n",
        "\n",
        "# the observation space requires int rep for player tokens\n",
        "self.observation_space = spaces.Box(low=0, high=2, shape=(9,), dtype=np.int)\n",
        "self.reward_range = (-10, 1)\n",
        "\n",
        "# we will redefine our player labels as ints\n",
        "self.player_label = 1\n",
        "self.opponent_label = 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzREYcZaG_5C"
      },
      "source": [
        "Let's take a look at our redefined action space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy1V_1EBG0V6",
        "outputId": "79e1b4aa-7a94-4e45-f0c0-fd9e3bea79e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "board = {1: 0,\n",
        "      2: 0,\n",
        "      3: 0,\n",
        "      4: 0,\n",
        "      5: 0,\n",
        "      6: 0,\n",
        "      7: 0,\n",
        "      8: 0,\n",
        "      9: 0,}\n",
        "state = np.array(list(board.values())).reshape(9,)\n",
        "state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV5NHxqoHJIg"
      },
      "source": [
        "Does this align with a random sample of the observation space? It should if it is going to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCIFwoQVHIKF",
        "outputId": "77d0a119-1c6b-4f93-81e4-66772c634d2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 1, 2, 2, 1, 0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "box = spaces.Box(low=0, high=2, shape=(9,), dtype=int)\n",
        "box.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaJIstAeHOj1"
      },
      "source": [
        "Let's break this down. For 1 of 9 spaces (defined by shape in `spaces.Box`), the game board can take on the value of 0, 1, or 2 (defined by low and high in `spaces.Box`). When we sample from box we get a random snapshot of how the bored could possibly look. The way we've defined `state` is such that it too, represents how the board could possibly look. `state` will be returned by both `reset` and `step` when we go to wrap all of this in our game environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPRBt3T7JP-o"
      },
      "source": [
        "### 1.1.4 Step-Reward\n",
        "\n",
        "Our Reinforcement Learning (RL) agent will have much less information available to them than our prior algorithms. For this we need to define our reward system a little differently. Given a current board the agent receives:\n",
        "\n",
        "* +10 for playing a winning move\n",
        "* -100 for playing an invalid move \n",
        "* -10 if the opponent wins the next move\n",
        "* 1/9 for playing a valid move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ipwlRMxaGS"
      },
      "outputs": [],
      "source": [
        "class TicTacToeGym(GameEngine, gym.Env):\n",
        "  def __init__(self, user_ai=None, ai_level=1):\n",
        "    super().__init__()\n",
        "    self.setup = 'auto'\n",
        "    # the default behavior will be no user_ai and ai_level set to 1 (random)\n",
        "    self.user_ai = user_ai\n",
        "    self.ai_level = ai_level\n",
        "\n",
        "    # the board now has integers as values instead of strings\n",
        "    self.board = {1: 0,\n",
        "         2: 0,\n",
        "         3: 0,\n",
        "         4: 0,\n",
        "         5: 0,\n",
        "         6: 0,\n",
        "         7: 0,\n",
        "         8: 0,\n",
        "         9: 0,}\n",
        "    \n",
        "    # the available token spaces, note that in order to access our board\n",
        "    # dictionary these actions will need to be re-indexed to 1\n",
        "    self.action_space = spaces.Discrete(9)\n",
        "\n",
        "    # the observation space requires int rep for player tokens\n",
        "    self.observation_space = spaces.Box(low=0, high=2, shape=(9,), dtype=int)\n",
        "    self.reward_range = (-10, 1)\n",
        "\n",
        "    # we will redefine our player labels as ints\n",
        "    self.player_label = 1\n",
        "    self.opponent_label = 2\n",
        "\n",
        "    # for StableBaselines\n",
        "    self.spec = None\n",
        "    self.metadata = None\n",
        "\n",
        "  ##############################################################################\n",
        "  # we will have to redefine any function in our previous module that makes use\n",
        "  # of the string entries, X and O on the board. We need to replace the logic\n",
        "  # with 1's and 2's\n",
        "  ##############################################################################\n",
        "  def check_winning(self):\n",
        "    for pattern in self.win_patterns:\n",
        "      values = [self.board[i] for i in pattern] \n",
        "      if values == [1, 1, 1]:\n",
        "        self.winner = 'X' # we update the winner status\n",
        "        return \"'X' Won!\"\n",
        "      elif values == [2, 2, 2]:\n",
        "        self.winner = 'O'\n",
        "        return \"'O' Won!\"\n",
        "    return ''\n",
        "\n",
        "  def check_stalemate(self):\n",
        "    if (0 not in self.board.values()) and (self.check_winning() == ''):\n",
        "      self.winner = 'Stalemate'\n",
        "      return \"It's a stalemate!\"\n",
        "\n",
        "  def reset_game(self):\n",
        "    overwrite_ai = self.ai_level\n",
        "    self.board = {1: 0,\n",
        "         2: 0,\n",
        "         3: 0,\n",
        "         4: 0,\n",
        "         5: 0,\n",
        "         6: 0,\n",
        "         7: 0,\n",
        "         8: 0,\n",
        "         9: 0,}\n",
        "    self.winner = ''\n",
        "    self.setup_game()\n",
        "    self.ai_level = overwrite_ai\n",
        "    # depending now on if X or O is first will need to take the AI's first step\n",
        "    if self.start_player == 'O':\n",
        "      move = self.random_ai()\n",
        "      self.board[move] = 2\n",
        "\n",
        "  def reset(self):\n",
        "    self.reset_game()\n",
        "    state = np.array(list(self.board.values())).reshape(9,)\n",
        "    return state\n",
        "\n",
        "  def random_ai(self):\n",
        "    while True:\n",
        "      move = random.randint(1,9)\n",
        "      if self.board[move] != 0:\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    return move\n",
        "\n",
        "  ##############################################################################\n",
        "  # we will have to recycle a lot of what was previously wrapped up in \n",
        "  # play_game() since gym needs access to every point after the Reinf AI\n",
        "  # makes a move\n",
        "  ##############################################################################\n",
        "  def step(self, action):\n",
        "\n",
        "      # gym discrete indexes at 0, our board indexes at 1\n",
        "      move = action + 1\n",
        "      # Check if agent's move is valid\n",
        "      avail_moves = [i for i in self.board.keys() if self.board[i] == 0]\n",
        "      is_valid = move in avail_moves\n",
        "\n",
        "      # if valid, then play the move, and let the other opponent make a move\n",
        "      # as well\n",
        "      if is_valid: # Play the move\n",
        "          # update board\n",
        "          self.board[move] = self.player_label\n",
        "          self.check_winning()\n",
        "          self.check_stalemate()\n",
        "\n",
        "          if self.winner == '':\n",
        "            ##################################################################\n",
        "            # instead of continuing as we did in our play_game loop we will\n",
        "            # take one additional step for the AI and then let openAI gym\n",
        "            # handle incrementing between steps.\n",
        "            ##################################################################\n",
        "\n",
        "            ##########\n",
        "            # Our level 1 ai agent (random)\n",
        "            ##########\n",
        "            # if self.ai_level == 1:\n",
        "            move = self.random_ai()\n",
        "\n",
        "            # ##########\n",
        "            # # Our level 2 ai agent (heuristic)\n",
        "            # ##########\n",
        "            # elif self.ai_level == 2:\n",
        "            #   move = self.heuristic_ai('O')\n",
        "\n",
        "            # ##########\n",
        "            # # Our user-defined AI agent\n",
        "            # ##########\n",
        "            # elif self.ai_level == 3:\n",
        "            #   move = self.user_ai(self.board, self.win_patterns, 'O')\n",
        "\n",
        "            self.board[move] = self.opponent_label\n",
        "            self.check_winning()\n",
        "            self.check_stalemate()\n",
        "\n",
        "            if self.winner == '':\n",
        "              reward, done, info = 1/9, False, {}\n",
        "          \n",
        "          if self.winner == 'Stalemate':\n",
        "            reward, done, info = -1, True, {}\n",
        "\n",
        "          elif self.winner == 'X':\n",
        "            reward, done, info = 100, True, {}\n",
        "\n",
        "          elif self.winner == 'O':\n",
        "            reward, done, info = -10, True, {}\n",
        "\n",
        "      else: # End the game and penalize agent\n",
        "          reward, done, info = -100, True, {}\n",
        "\n",
        "      state = np.array(list(self.board.values())).reshape(9,)\n",
        "      return state, reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSgpPe3yG4g"
      },
      "source": [
        "### 1.1.5 Testing the Environment\n",
        "\n",
        "We can check that the environment is compatible with gym using `check_env`. Notice the below doesn't return any error messages. This means everything is working ok!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCbGNSTJmVNJ"
      },
      "outputs": [],
      "source": [
        "env = TicTacToeGym()\n",
        "check_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giJil-te4FqY"
      },
      "source": [
        "We can also define a model from OpenAI and see how our game board updates in a single step with the new wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR58CX3AI6bn",
        "outputId": "0f323668-aac2-44a7-a220-4ee87cc987a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env in a DummyVecEnv.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:115: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:560: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/a2c/utils.py:156: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:194: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:202: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:210: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:246: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "env = TicTacToeGym()\n",
        "model = PPO2(MlpPolicy, env, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWbJ9utt3LQV",
        "outputId": "1a884302-6432-4c84-9099-45e58be05974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the start player: O\n",
            "the taken action: 7\n",
            "AI level: 1\n",
            "[0 0 0 0 2 2 0 1 0]\n",
            "Should be blank if no winner: []\n"
          ]
        }
      ],
      "source": [
        "# test our reset function\n",
        "obs = env.reset()\n",
        "\n",
        "# the start player should randomly select between X and O\n",
        "print('the start player: {}'.format(env.start_player))\n",
        "\n",
        "# we should return an action from model.predict\n",
        "action, _states = model.predict(obs)\n",
        "print(\"the taken action: {}\".format(action))\n",
        "\n",
        "\n",
        "# we divert default behavior of setup_game by saving and reestablishing our\n",
        "# user input ai_level\n",
        "print(\"AI level: {}\".format(env.ai_level))\n",
        "\n",
        "# check the board update from env.step()\n",
        "obs, rewards, dones, info = env.step(action)\n",
        "print(obs)\n",
        "\n",
        "print(\"Should be blank if no winner: [{}]\".format(env.check_winning()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGwjzF9mIoiN"
      },
      "source": [
        "And we can still visualize the board:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbwBVgKq8-zJ",
        "outputId": "3057b46e-e9df-48fe-d8be-2c7e6e2c6f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|0|0|0|\n",
            "|0|2|2|\n",
            "|0|1|0|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.visualize_board()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbhmcy4ZWc-L"
      },
      "source": [
        "And check that our untrained model will win approx half the time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UMjDpbjWUgx",
        "outputId": "5dd71a54-3d84-483e-ede7-b9e8116495f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O            385\n",
              "X            322\n",
              "Stalemate     61\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "winners = []\n",
        "for j in range(1000):\n",
        "  obs = env.reset()\n",
        "  for i in range(10):\n",
        "      action, _states = model.predict(obs)\n",
        "      # print(action)\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      # env.visualize_board()\n",
        "      if env.winner != '':\n",
        "        winners.append(env.winner)\n",
        "        break\n",
        "\n",
        "pd.DataFrame(winners).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9ioyElIrGH"
      },
      "source": [
        "### 1.1.6 Training the Model\n",
        "\n",
        "Now we will train the PPO2 model on our environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbvPd-T2Ids"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "env = TicTacToeGym()\n",
        "model = PPO2(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx0xdzbk-ukK",
        "outputId": "5ae5c8cd-efb9-4248-f1f9-879afdbed006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "|0|0|0|\n",
            "|0|1|2|\n",
            "|0|0|0|\n",
            "\n",
            "8\n",
            "|2|0|0|\n",
            "|0|1|2|\n",
            "|0|0|1|\n",
            "\n",
            "6\n",
            "|2|0|2|\n",
            "|0|1|2|\n",
            "|1|0|1|\n",
            "\n",
            "7\n",
            "|2|0|2|\n",
            "|0|1|2|\n",
            "|1|1|1|\n",
            "\n",
            "X\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "for i in range(10):\n",
        "    action, _states = model.predict(obs)\n",
        "    print(action)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.visualize_board()\n",
        "    if env.winner != '':\n",
        "      print(env.winner)\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcSjh0khBEow"
      },
      "outputs": [],
      "source": [
        "winners = []\n",
        "for j in range(1000):\n",
        "  obs = env.reset()\n",
        "  for i in range(10):\n",
        "      action, _states = model.predict(obs)\n",
        "      # print(action)\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      # env.visualize_board()\n",
        "      if env.winner != '':\n",
        "        winners.append(env.winner)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXSp1l5JLsP"
      },
      "source": [
        "Let's see how many times our trained model won:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELMfzwDeBMYQ",
        "outputId": "fb12a08d-4310-40fb-bb91-9015ccc51a53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X            795\n",
              "O            172\n",
              "Stalemate     27\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "pd.DataFrame(winners).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxZlwJPKJOED"
      },
      "source": [
        "Not terrible! Could be better! Let's play against our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUFOHQB2JTjt"
      },
      "source": [
        "### 1.1.7 Play Against the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kbf9O3tLqs_"
      },
      "source": [
        "To make our model compatible with the old `play_game` method, we will need a way to convert to and from int vs string representations on our board. Let's test this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmYaHjLnKeFM",
        "outputId": "e74b2899-9006-47e2-f329-2dc33e0a5615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "value_map = {' ': 0,\n",
        "             'X': 1,\n",
        "             'O': 2}\n",
        "\n",
        "board = {1: 'X',\n",
        "         2: ' ',\n",
        "         3: ' ',\n",
        "         4: ' ',\n",
        "         5: ' ',\n",
        "         6: ' ',\n",
        "         7: ' ',\n",
        "         8: ' ',\n",
        "         9: ' ',}\n",
        "\n",
        "for key in board.keys():\n",
        "  board[key] = value_map[board[key]]\n",
        "\n",
        "board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvwY0i6CLye9"
      },
      "source": [
        "And now we can wrap it up into a new ai function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC8cSoEnJS0v"
      },
      "outputs": [],
      "source": [
        "def rl_ai(board, win_patterns, player_label, model=model):\n",
        "  # note that we are simply leaving win_patterns and player_label\n",
        "  # here so that we can use the game engine as defined in prior\n",
        "  # sessions, these inputs are ignored.\n",
        "  \n",
        "  ai_board = board.copy()\n",
        "  value_map = {' ': 0,\n",
        "             'X': 1,\n",
        "             'O': 2}\n",
        "  for key in ai_board.keys():\n",
        "    ai_board[key] = value_map[ai_board[key]]\n",
        "  \n",
        "  obs = np.array(list(ai_board.values())).reshape(9,)\n",
        "  action, _states = model.predict(obs)\n",
        "  move = action + 1\n",
        "  return move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nC58SXxL-nO",
        "outputId": "6e4fc041-ed14-476a-c0a4-f0ff7b1f4513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many Players? (type 0, 1, or 2)1\n",
            "who will go first? (X, (AI), or O (Player))X\n",
            "| | | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "| | | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "O, what's your move?1\n",
            "|O| | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "|O| |X|\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "O, what's your move?7\n",
            "|O| |X|\n",
            "| |X| |\n",
            "|O| | |\n",
            "\n",
            "|O| |X|\n",
            "| |X| |\n",
            "|O| |X|\n",
            "\n",
            "O, what's your move?4\n",
            "'O' Won!\n",
            "|O| |X|\n",
            "|O|X| |\n",
            "|O| |X|\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GameEngine at 0x7f8d40c4fb10>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "game = GameEngine('user', user_ai=rl_ai)\n",
        "game.setup_game()\n",
        "game.play_game()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KPJROzZQtIF"
      },
      "source": [
        "Notice any interesting behaviors about the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTMuqy_KNE9j"
      },
      "source": [
        "## 1.2 Improve the Model\n",
        "\n",
        "How can we improve this puppy? What about training the model against a smarter opponent? changing the reward values? training for longer? OR trying a different reinforcement learning model? Try any or all of these and see what works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQc1mHhkMAWJ"
      },
      "outputs": [],
      "source": [
        "class TicTacToeGym(GameEngine, gym.Env):\n",
        "  def __init__(self, user_ai=None, ai_level=1):\n",
        "    super().__init__()\n",
        "    self.setup = 'auto'\n",
        "    # the default behavior will be no user_ai and ai_level set to 1 (random)\n",
        "    self.user_ai = user_ai\n",
        "    self.ai_level = ai_level\n",
        "\n",
        "    # the board now has integers as values instead of strings\n",
        "    self.board = {1: 0,\n",
        "         2: 0,\n",
        "         3: 0,\n",
        "         4: 0,\n",
        "         5: 0,\n",
        "         6: 0,\n",
        "         7: 0,\n",
        "         8: 0,\n",
        "         9: 0,}\n",
        "    \n",
        "    # the available token spaces, note that in order to access our board\n",
        "    # dictionary these actions will need to be re-indexed to 1\n",
        "    self.action_space = spaces.Discrete(9)\n",
        "\n",
        "    # the observation space requires int rep for player tokens\n",
        "    self.observation_space = spaces.Box(low=0, high=2, shape=(9,), dtype=np.int)\n",
        "    self.reward_range = (-10, 1)\n",
        "\n",
        "    # we will redefine our player labels as ints\n",
        "    self.player_label = 1\n",
        "    self.opponent_label = 2\n",
        "\n",
        "    # for StableBaselines\n",
        "    self.spec = None\n",
        "    self.metadata = None\n",
        "\n",
        "  ##############################################################################\n",
        "  # we will have to redefine any function in our previous module that makes use\n",
        "  # of the string entries, X and O on the board. We need to replace the logic\n",
        "  # with 1's and 2's\n",
        "  ##############################################################################\n",
        "  def check_winning(self):\n",
        "    for pattern in self.win_patterns:\n",
        "      values = [self.board[i] for i in pattern] \n",
        "      if values == [1, 1, 1]:\n",
        "        self.winner = 'X' # we update the winner status\n",
        "        return \"'X' Won!\"\n",
        "      elif values == [2, 2, 2]:\n",
        "        self.winner = 'O'\n",
        "        return \"'O' Won!\"\n",
        "    return ''\n",
        "\n",
        "  def check_stalemate(self):\n",
        "    if (0 not in self.board.values()) and (self.check_winning() == ''):\n",
        "      self.winner = 'Stalemate'\n",
        "      return \"It's a stalemate!\"\n",
        "\n",
        "  def reset_game(self):\n",
        "    overwrite_ai = self.ai_level\n",
        "    self.board = {1: 0,\n",
        "         2: 0,\n",
        "         3: 0,\n",
        "         4: 0,\n",
        "         5: 0,\n",
        "         6: 0,\n",
        "         7: 0,\n",
        "         8: 0,\n",
        "         9: 0,}\n",
        "    self.winner = ''\n",
        "    self.setup_game()\n",
        "    self.ai_level = overwrite_ai\n",
        "    # depending now on if X or O is first will need to take the AI's first step\n",
        "    if self.start_player == 'O':\n",
        "      move = self.random_ai()\n",
        "      self.board[move] = 2\n",
        "\n",
        "  def reset(self):\n",
        "    self.reset_game()\n",
        "    state = np.array(list(self.board.values())).reshape(9,)\n",
        "    return state\n",
        "\n",
        "  def random_ai(self):\n",
        "    while True:\n",
        "      move = random.randint(1,9)\n",
        "      if self.board[move] != 0:\n",
        "        continue\n",
        "      else:\n",
        "        break\n",
        "    return move\n",
        "\n",
        "  def heuristic_ai(self, player_label):\n",
        "    opponent = [1, 2]\n",
        "    opponent.remove(player_label)\n",
        "    opponent = opponent[0]\n",
        "\n",
        "    avail_moves = [i for i in self.board.keys() if self.board[i] == 0]\n",
        "    temp_board = self.board.copy()\n",
        "    middle = 5\n",
        "    corner = [1,3,7,9]\n",
        "    side = [2,4,6,8]\n",
        "\n",
        "    # first check for a winning move\n",
        "    move_found = False\n",
        "    for move in avail_moves:\n",
        "      temp_board[move] = player_label\n",
        "      for pattern in self.win_patterns:\n",
        "          values = [temp_board[i] for i in pattern] \n",
        "          if values == [player_label, player_label, player_label]:\n",
        "            move_found = True       \n",
        "            break\n",
        "      if move_found:   \n",
        "        break\n",
        "      else:\n",
        "        temp_board[move] = 0\n",
        "\n",
        "    # check if the opponent has a winning move\n",
        "    if move_found == False:\n",
        "      for move in avail_moves:\n",
        "        temp_board[move] = opponent\n",
        "        for pattern in self.win_patterns:\n",
        "            values = [temp_board[i] for i in pattern] \n",
        "            if values == [opponent, opponent, opponent]:\n",
        "              move_found = True       \n",
        "              break\n",
        "        if move_found:   \n",
        "          break\n",
        "        else:\n",
        "          temp_board[move] = 0\n",
        "\n",
        "    # check if middle avail\n",
        "    if move_found == False:\n",
        "      if middle in avail_moves:\n",
        "        move_found = True\n",
        "        move = middle\n",
        "\n",
        "    # check corners\n",
        "    if move_found == False:\n",
        "      move_corner = [val for val in avail_moves if val in corner]\n",
        "      if len(move_corner) > 0:\n",
        "        move = random.choice(move_corner)\n",
        "        move_found = True\n",
        "\n",
        "    # check side\n",
        "    if move_found == False:\n",
        "      move_side = [val for val in avail_moves if val in side]\n",
        "      if len(move_side) > 0:\n",
        "        move = random.choice(move_side)\n",
        "        move_found = True\n",
        "\n",
        "    return move\n",
        "\n",
        "  ##############################################################################\n",
        "  # we will have to recycle a lot of what was previously wrapped up in \n",
        "  # play_game() since gym needs access to every point after the Reinf AI\n",
        "  # makes a move\n",
        "  ##############################################################################\n",
        "  def step(self, action):\n",
        "\n",
        "      # gym discrete indexes at 0, our board indexes at 1\n",
        "      move = action + 1\n",
        "      # Check if agent's move is valid\n",
        "      avail_moves = [i for i in self.board.keys() if self.board[i] == 0]\n",
        "      is_valid = move in avail_moves\n",
        "\n",
        "      # if valid, then play the move, and let the other opponent make a move\n",
        "      # as well\n",
        "      if is_valid: # Play the move\n",
        "          # update board\n",
        "          self.board[move] = self.player_label\n",
        "          self.check_winning()\n",
        "          self.check_stalemate()\n",
        "\n",
        "          if self.winner == '':\n",
        "            ##################################################################\n",
        "            # instead of continuing as we did in our play_game loop we will\n",
        "            # take one additional step for the AI and then let openAI gym\n",
        "            # handle incrementing between steps.\n",
        "            ##################################################################\n",
        "\n",
        "            ##########\n",
        "            # Our level 1 ai agent (random)\n",
        "            ##########\n",
        "            if self.ai_level == 1:\n",
        "              move = self.random_ai()\n",
        "\n",
        "            # ##########\n",
        "            # # Our level 2 ai agent (heuristic)\n",
        "            # ##########\n",
        "            elif self.ai_level == 2:\n",
        "              move = self.heuristic_ai(self.player_label)\n",
        "\n",
        "            # ##########\n",
        "            # # Our user-defined AI agent\n",
        "            # ##########\n",
        "            # elif self.ai_level == 3:\n",
        "            #   move = self.user_ai(self.board, self.win_patterns, 'O')\n",
        "\n",
        "            self.board[move] = self.opponent_label\n",
        "            self.check_winning()\n",
        "            self.check_stalemate()\n",
        "\n",
        "            if self.winner == '':\n",
        "              reward, done, info = 1/9, False, {}\n",
        "          \n",
        "          if self.winner == 'Stalemate':\n",
        "            reward, done, info = -10, True, {}\n",
        "\n",
        "          elif self.winner == 'X':\n",
        "            reward, done, info = 50, True, {}\n",
        "\n",
        "          elif self.winner == 'O':\n",
        "            reward, done, info = -50, True, {}\n",
        "\n",
        "      else: # End the game and penalize agent\n",
        "          reward, done, info = -100, True, {}\n",
        "\n",
        "      state = np.array(list(self.board.values())).reshape(9,)\n",
        "      return state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_SpRJFCOCFU"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "env = TicTacToeGym(ai_level=1)\n",
        "model = PPO2(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMCjNFxNTzh6"
      },
      "outputs": [],
      "source": [
        "winners = []\n",
        "for j in range(1000):\n",
        "  obs = env.reset()\n",
        "  for i in range(10):\n",
        "      action, _states = model.predict(obs)\n",
        "      # print(action)\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      # env.visualize_board()\n",
        "      if env.winner != '':\n",
        "        winners.append(env.winner)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svziRCqFT4Pe",
        "outputId": "f8eb8407-bb90-46f9-d9a9-3d5b284bba4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "X            791\n",
              "O            191\n",
              "Stalemate     10\n",
              "dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(winners).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IMMEyA5OGAM",
        "outputId": "f2b2323e-fbd0-4eaf-abcf-f79248122945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many Players? (type 0, 1, or 2)1\n",
            "who will go first? (X, (AI), or O (Player))X\n",
            "| | | |\n",
            "| | | |\n",
            "| | | |\n",
            "\n",
            "| | | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "O, what's your move?1\n",
            "|O| | |\n",
            "| |X| |\n",
            "| | | |\n",
            "\n",
            "|O| | |\n",
            "| |X| |\n",
            "|X| | |\n",
            "\n",
            "O, what's your move?2\n",
            "|O|O| |\n",
            "| |X| |\n",
            "|X| | |\n",
            "\n",
            "|O|O| |\n",
            "| |X|X|\n",
            "|X| | |\n",
            "\n",
            "O, what's your move?3\n",
            "'O' Won!\n",
            "|O|O|O|\n",
            "| |X|X|\n",
            "|X| | |\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.GameEngine at 0x7f6433286510>"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "game = GameEngine('user', user_ai=rl_ai)\n",
        "game.setup_game()\n",
        "game.play_game()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "X1_Tictactoe_RNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}