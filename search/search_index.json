{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General Applications of Neural Networks \u00b6 Welcome to General Applications of Neural Networks, my name is Wesley . Whether you've landed here by accident or on purpose, welcome! Visit the github repo to access the original jupyter notebooks for this class and happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"General Applications of Neural Networks"},{"location":"#general-applications-of-neural-networks","text":"Welcome to General Applications of Neural Networks, my name is Wesley . Whether you've landed here by accident or on purpose, welcome! Visit the github repo to access the original jupyter notebooks for this class and happy learning \ud83e\uddd1\u200d\ud83c\udfeb","title":"General Applications of Neural Networks"},{"location":"S1_Multilayer_Perceptron/","text":"General Applications of Neural Networks Session 1: The Multilayer Perceptron \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will introduce Neural Networks! We'll cover the over arching concepts used to talk about network architecture as well as their building blocks. images in this notebook borrowed from Ryan Holbrook 1.0 Preparing Environment and Importing Data \u00b6 back to top ! pip uninstall scikit - learn - y ! pip install - U scikit - learn Found existing installation: scikit-learn 0.24.2 Uninstalling scikit-learn-0.24.2: Successfully uninstalled scikit-learn-0.24.2 Collecting scikit-learn Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0) Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Installing collected packages: scikit-learn Successfully installed scikit-learn-0.24.2 1.0.1 Import Packages \u00b6 back to top from tensorflow import keras from keras import backend as K from tensorflow.keras import layers import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from sklearn.model_selection import train_test_split import plotly.express as px from sklearn.impute import SimpleImputer from copy import copy sns . set () import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler , OneHotEncoder from sklearn.compose import make_column_transformer , make_column_selector from sklearn import set_config from sklearn.pipeline import make_pipeline from sklearn.metrics import mean_squared_error set_config ( display = 'diagram' ) 1.0.2 Load Dataset \u00b6 back to top Before diving in I want to compare two methods of reading and preprocessing our data: # import wine data wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), # impute with median StandardScaler ()) # scale and center # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), # impute with placeholder OneHotEncoder ( handle_unknown = 'ignore' )) # one hot encode # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test and validation X_val_std = preprocessor . transform ( X_val ) y_train_std = np . log ( y_train ) # log output y y_val_std = np . log ( y_val ) # log output y y_test_std = np . log ( y_test ) # log output y preprocessor #sk-81b6bb9b-6f93-4195-900b-55d5f542e910 {color: black;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 pre{padding: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable {background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator:hover {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-item {z-index: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:only-child::after {width: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') 1.1 Neural Network Building Blocks \u00b6 back to top 1.1.1 The Perceptron \u00b6 back to top The simplest unit of a neural network is the perceptron. Given an input vector \\(x\\) and an output vector \\(y\\), we can illustrate this like so: where \\(w\\) is a weight applied to \\(x\\) and \\(b\\) is an unweighted term that we call the bias . We include a bias so that the perceptron is not entirely dependent on the input data. A neural network learns by updating \\(w\\) and \\(b\\) so that it can accurately model \\(x\\) to \\(y\\). When we write out the perceptron mathematically we get the following: y = xw+b which should look familiar! This is our equation for a linear function. In fact, we will see that a neural network is essentially many instances of linear regression along side, and being fed into, one another. Often, we will have not an input feature vector \\(x\\) but an input feature matrix, \\(X\\). We can update our schematic for a perceptron to account for this: We can write the mathematical formula for this neuron as follows: y = x_2 w_2 + x_1 w_1 + x_0 w_0 + b In tensorflow/keras we can define this perceptron: from tensorflow import keras from tensorflow.keras import layers # Create a network with 1 linear unit model = keras . Sequential ([ layers . Dense ( units = 1 , # number of units (the + filled circle above) input_shape = [ 3 ]) # number of x_ (the x filled circle above) ]) In order to build this single perceptron with keras, I had to use some additional language here: layers, dense, sequential. We'll explain what these are referring to in a moment. What I want to draw your attention to now, however, is that we tell layers.Dense that we want 1 unit , the single perceptron, and input_shape=[3] , the number of features. Notice that b is automatically included without having it as a parameter. Just as we always have a y intercept in a linear model. After we introduce the other aspects of the neural network architecture, we will train a single perceptron model and compare it with a linear model, we will see that they are functionally no different. \ud83c\udfcb\ufe0f Exercise 1: Single Perceptron \u00b6 define a single perceptron that could be used to predict wine density from acidity. Inpsect the weights. Use the untrained model to predict y and plot this against true y # Code cell for exercise 1 # DECLARE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . weights [<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.4809548]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>] And now use the untrained model to predict wine['density'] from wine['fixed acidity'] # in the line below, use model.predict() and provide wine['fixed acidity'] as # the input data to predict what wine['density'] will be # y_pred = plt . plot ( y_pred , wine [ 'density' ], ls = '' , marker = 'o' , alpha = 0.3 ) [<matplotlib.lines.Line2D at 0x7f5a9269a890>] 1.2 Neural Network Architectures \u00b6 back to top 1.2.1 Neural Network Layers \u00b6 back to top Now that we have the idea of the most basic building block of a neural network, we will start to discuss the larger architecture. The reason we focused on the lowest building block, is that neural networks are modular . They are made up of instances of these perceptrons or neurons. neurons in parallel make up a layer These layers feed into one another. When each node of a preceding layer is connected to every node of a following layer, we say they are fully connected and the receiving layer is a dense layer . In a moment we will talk about input, output and hidden layers, for neural networks with three or more layers. 1.2.2 The Activation Function \u00b6 back to top It turns out that stringing together a bunch of linear functions, will still result in overall linear relationships. We need a way to break out of this. A neat trick is introduced at the output of each neuron. The output passes through an activation function . There are a handful of different activation functions used in practice, the most common is known as the rectifier function: max(f(x), 0) and the resulting node can be schematically drawn like this: with the inset of the summation node indicating that at a minimum the resultant y value is 0. \ud83c\udfcb\ufe0f Exercise 2: The Rectifier Function \u00b6 Write a function called my_perceptron that takes x, a length 2 array, as an input. Have your function return the maximum of \\((0, w*x + b)\\) where w is a length 2 weight vector. # Code cell for exercise 2 def my_perceptron ( x ): \"\"\" a simple 2 input feature perceptron with predefined weights, intercept, and a rectifier activation function Parameters ---------- x: array the input array of length 2 Returns ------- rect: int the rectified output of the perceptron \"\"\" # # define b, w, and y (y=mx+b) # w = # b = # y = # # return the max of 0 and y # rect = return rect After you write your function make sure it returns 0 when the output of the linear component is negative. def test_zero_output (): x = np . array ([ - 10 , - 10 ]) # 10 * 1 + (-10) * 2 + 1 = -11 assert my_perceptron ( x ) == 0 , \"The output is not zero when it should be!\" test_zero_output () print ( \"test passing\" ) test passing 1.2.3 Stacking Layers \u00b6 back to top When we stack many layers together, we create what are traditionally regarded as neural networks. the first and last layers are called the input and output layers, while the inbetween layers are referred to as hidden layers, since their outputs are not directly seen. Tradditionally, a neural network with three or more hidden layers is referred to as a deep neural network. Notice that in this schematic, the last node does not have an activation function. This is typical of a regression task. In a classification task, we might require an activation function here. 1.2.4 Building Sequential Neural Networks \u00b6 back to top Now that we have the essential components of a neural network architecture, we can enter into the domain of overall naming conventions for architecure types. The classic neural network architecture is a feed forward neural network, where every preceding layer feeds into the next layer. We will practice building that with keras. \ud83c\udfcb\ufe0f Exercise 3: Building Sequential Layers \u00b6 In the cell bellow, use keras to build a 3-layer network with activation='relu' and 512 units. Create the output layer so that it can predict 1 continuous value. # Code cell for exercise 3 # DECLARE THE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### # the hidden ReLU layers # the linear output layer ]) \ud83c\udfcb\ufe0f Exercise 4: Other Activation Functions \u00b6 There are other activation functions we can use after the summation in a neural node. Use the code below to plot and inspect them! Pick one and do a quick google search on what that activation function's best use case is. # Code cell for exercise 4 import tensorflow as tf # YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else activation_layer = layers . Activation ( 'relu' ) x = tf . linspace ( - 3.0 , 3.0 , 100 ) y = activation_layer ( x ) # once created, a layer is callable just like a function plt . plot ( x , y ) plt . xlim ( - 3 , 3 ) plt . xlabel ( \"Input\" ) plt . ylabel ( \"Output\" ) plt . show () 1.3 Neural Network Training \u00b6 back to top We've defined neural network architectures, now how do we train them? There are two main concepts here: the loss function which we've encountered before, and the optimizer the means by which we improve the loss function 1.3.1 The Loss Function \u00b6 back to top In previous sessions, we've envountered MSE: MSE = \\frac{1}{N}\\sum{(y-\\hat{y})^2} Another common loss for neural networks is the mean absolute error (MAE): MAE = \\frac{1}{N}\\sum{|y-\\hat{y}|} In anycase, the loss function describes the difference between the actual and predicted output of the model. The important thing to note, is that the weights in the neural network are systematically updated according to this loss function, they do this via an optimization algorithm. 1.3.2 The Optimizer \u00b6 back to top In order to update the neural network weights to improve the loss function, we require an algorithm. Virtually all available algorithms for this purpose fall within the family of stochastic gradient descent . This works essentially in these iterative steps: a subsample of the input data is passed through the network a loss is computed the weights are adjusted in a direction to improve the loss The key here is in step 3. The brilliance of neural networks, is that the loss function is differentiable with respect to the weights in the network, and so the change in loss can be ascribed to certain weight changes. We refer to this as assigning blame in the network, and it works through the mathematical chain rule of differentiation. We won't go into great detail here, other than to make a nod to it, and that this algorithm (step 3) is referred to as back propagation . The three step process is repeated until a stop criteria is reached, the simplest being the loss stops improving above some threshold or a desired loss is achieved. In the above animation, the black line represents the output of the model, the red dots make up a minibatch or simply a batch while the opaque red dots represent the whole training dataset. Exposing the model to an entire round of the training data is referred to as an epoch . The training loss improves with additional rounds of trianing (middle panel) and the weights are adjusted to update the model (right panel). 1.3.3 Batches and Epochs \u00b6 An epoch is the number of times the model will see the entire training set A batch is the number of training samples the model will run before calculating a total error and updating its internal parameters. Variability of batch (from 1 sample all the way to the entire training set size) leads to different categorizations of the optimizer algorithm: Batch Gradient Descent. Batch Size = Size of Training Set Stochastic Gradient Descent. Batch Size = 1 Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set We will visit additional details about batch and epochs in the next session when we discuss model evaluation. 1.3.4 Learning Rate \u00b6 back to top notice how in the above animation the model makes progressive steps toward a global optimum. The size of these steps is determined by the learning rate . You can think of it as the amount of improvement to make in the direction of steepest descent (the derivative of the loss function in regards to the changes in weights). Sometimes a large step size can result in stepping over crevices in the solution surface and getting stuck, while too small of step sizes can lead to a slow algorithm. Often the optimum learning rate is not obvious, luckily there are some optimizers that are self-calibrating in this regard. Adam is one such optimizer available to us in keras. # we can compile the model like so model . compile ( optimizer = \"adam\" , loss = \"mae\" , ) \ud83c\udfcb\ufe0f Exercise 5.1: Train your first Neural Networks \u00b6 back to top We're going to train our first neural network. Take the model you created in exercise 3 and paste it in the cell below. Make sure that the input_shape of the first layer matches the number of features in X_train_std X_train_std . shape [ 1 ] 13 # Code cell for exercise 5 # DECLARE THE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### # the hidden ReLU layers # the linear output layer ]) Now we'll compile the model model . compile ( optimizer = 'adam' , loss = 'mse' , ) And then train for 10 epochs history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 10 , ) Epoch 1/10 15/15 [==============================] - 1s 33ms/step - loss: 0.6389 - val_loss: 0.1925 Epoch 2/10 15/15 [==============================] - 0s 22ms/step - loss: 0.1330 - val_loss: 0.1054 Epoch 3/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0739 - val_loss: 0.0587 Epoch 4/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0442 - val_loss: 0.0378 Epoch 5/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0266 - val_loss: 0.0283 Epoch 6/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0193 - val_loss: 0.0233 Epoch 7/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0165 - val_loss: 0.0212 Epoch 8/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0151 - val_loss: 0.0204 Epoch 9/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0144 - val_loss: 0.0199 Epoch 10/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0140 - val_loss: 0.0194 Let's take a look at our training history: pd . DataFrame ( history . history ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss val_loss 0 0.638868 0.192506 1 0.133038 0.105378 2 0.073934 0.058686 3 0.044192 0.037832 4 0.026648 0.028253 5 0.019265 0.023272 6 0.016508 0.021151 7 0.015126 0.020368 8 0.014371 0.019929 9 0.014009 0.019429 # convert the training history to a dataframe history_df = pd . DataFrame ( history . history ) # use Pandas native plot method fig , ax = plt . subplots ( figsize = ( 10 , 5 )) history_df [ 'loss' ] . plot ( ax = ax ) history_df [ 'val_loss' ] . plot ( ax = ax ) ax . set_ylim ( 0 , .05 ) (0.0, 0.05) \ud83c\udfcb\ufe0f Exercise 5.2: Improve loss by varying nodes and hidden layers \u00b6 Take your former model as a starting point and now either add nodes or layers to see if the model improves model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 10 , ) Epoch 1/10 15/15 [==============================] - 1s 11ms/step - loss: 3.0090 - val_loss: 1.9299 Epoch 2/10 15/15 [==============================] - 0s 4ms/step - loss: 1.3536 - val_loss: 0.7737 Epoch 3/10 15/15 [==============================] - 0s 3ms/step - loss: 0.4739 - val_loss: 0.2300 Epoch 4/10 15/15 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.0781 Epoch 5/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0471 Epoch 6/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0402 - val_loss: 0.0354 Epoch 7/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0325 Epoch 8/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0305 Epoch 9/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0288 Epoch 10/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0271 y_pred = model . predict ( X_test_std ) plt . plot ( np . exp ( y_test_std ), np . exp ( y_pred ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f5d26303e10>] \ud83c\udfcb\ufe0f Exercise 5.3: Learning Curves \u00b6 Using 4 hidden layers now create 4 models that run for 30 epochs each: Vary the number of nodes in each layer Record the train/val/test score (MSE) Plot either total nodes or total trainable parameters vs score for each of the 5 models # Code Cell for Exercise 1.3.5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tot. units test mse val mse train mse 0 4 0.030658 0.030919 0.031088 1 25 0.020217 0.020164 0.019062 2 100 0.017429 0.018110 0.017416 3 289 0.019124 0.019371 0.018190 When we look at our historical loss do we notice that sometimes before the last epoch we actually hit a minimum? We'll discuss how to deal with this in the next session!","title":"The Multilayer Perceptron"},{"location":"S1_Multilayer_Perceptron/#general-applications-of-neural-networks-session-1-the-multilayer-perceptron","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will introduce Neural Networks! We'll cover the over arching concepts used to talk about network architecture as well as their building blocks. images in this notebook borrowed from Ryan Holbrook","title":"General Applications of Neural Networks  Session 1: The Multilayer Perceptron"},{"location":"S1_Multilayer_Perceptron/#10-preparing-environment-and-importing-data","text":"back to top ! pip uninstall scikit - learn - y ! pip install - U scikit - learn Found existing installation: scikit-learn 0.24.2 Uninstalling scikit-learn-0.24.2: Successfully uninstalled scikit-learn-0.24.2 Collecting scikit-learn Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0) Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Installing collected packages: scikit-learn Successfully installed scikit-learn-0.24.2","title":"1.0 Preparing Environment and Importing Data"},{"location":"S1_Multilayer_Perceptron/#101-import-packages","text":"back to top from tensorflow import keras from keras import backend as K from tensorflow.keras import layers import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from sklearn.model_selection import train_test_split import plotly.express as px from sklearn.impute import SimpleImputer from copy import copy sns . set () import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler , OneHotEncoder from sklearn.compose import make_column_transformer , make_column_selector from sklearn import set_config from sklearn.pipeline import make_pipeline from sklearn.metrics import mean_squared_error set_config ( display = 'diagram' )","title":"1.0.1 Import Packages"},{"location":"S1_Multilayer_Perceptron/#102-load-dataset","text":"back to top Before diving in I want to compare two methods of reading and preprocessing our data: # import wine data wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), # impute with median StandardScaler ()) # scale and center # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), # impute with placeholder OneHotEncoder ( handle_unknown = 'ignore' )) # one hot encode # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test and validation X_val_std = preprocessor . transform ( X_val ) y_train_std = np . log ( y_train ) # log output y y_val_std = np . log ( y_val ) # log output y y_test_std = np . log ( y_test ) # log output y preprocessor #sk-81b6bb9b-6f93-4195-900b-55d5f542e910 {color: black;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 pre{padding: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable {background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-estimator:hover {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-item {z-index: 1;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-parallel-item:only-child::after {width: 0;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-81b6bb9b-6f93-4195-900b-55d5f542e910 div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore')","title":"1.0.2 Load Dataset"},{"location":"S1_Multilayer_Perceptron/#11-neural-network-building-blocks","text":"back to top","title":"1.1 Neural Network Building Blocks"},{"location":"S1_Multilayer_Perceptron/#111-the-perceptron","text":"back to top The simplest unit of a neural network is the perceptron. Given an input vector \\(x\\) and an output vector \\(y\\), we can illustrate this like so: where \\(w\\) is a weight applied to \\(x\\) and \\(b\\) is an unweighted term that we call the bias . We include a bias so that the perceptron is not entirely dependent on the input data. A neural network learns by updating \\(w\\) and \\(b\\) so that it can accurately model \\(x\\) to \\(y\\). When we write out the perceptron mathematically we get the following: y = xw+b which should look familiar! This is our equation for a linear function. In fact, we will see that a neural network is essentially many instances of linear regression along side, and being fed into, one another. Often, we will have not an input feature vector \\(x\\) but an input feature matrix, \\(X\\). We can update our schematic for a perceptron to account for this: We can write the mathematical formula for this neuron as follows: y = x_2 w_2 + x_1 w_1 + x_0 w_0 + b In tensorflow/keras we can define this perceptron: from tensorflow import keras from tensorflow.keras import layers # Create a network with 1 linear unit model = keras . Sequential ([ layers . Dense ( units = 1 , # number of units (the + filled circle above) input_shape = [ 3 ]) # number of x_ (the x filled circle above) ]) In order to build this single perceptron with keras, I had to use some additional language here: layers, dense, sequential. We'll explain what these are referring to in a moment. What I want to draw your attention to now, however, is that we tell layers.Dense that we want 1 unit , the single perceptron, and input_shape=[3] , the number of features. Notice that b is automatically included without having it as a parameter. Just as we always have a y intercept in a linear model. After we introduce the other aspects of the neural network architecture, we will train a single perceptron model and compare it with a linear model, we will see that they are functionally no different.","title":"1.1.1 The Perceptron"},{"location":"S1_Multilayer_Perceptron/#exercise-1-single-perceptron","text":"define a single perceptron that could be used to predict wine density from acidity. Inpsect the weights. Use the untrained model to predict y and plot this against true y # Code cell for exercise 1 # DECLARE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . weights [<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.4809548]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>] And now use the untrained model to predict wine['density'] from wine['fixed acidity'] # in the line below, use model.predict() and provide wine['fixed acidity'] as # the input data to predict what wine['density'] will be # y_pred = plt . plot ( y_pred , wine [ 'density' ], ls = '' , marker = 'o' , alpha = 0.3 ) [<matplotlib.lines.Line2D at 0x7f5a9269a890>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Single Perceptron"},{"location":"S1_Multilayer_Perceptron/#12-neural-network-architectures","text":"back to top","title":"1.2 Neural Network Architectures"},{"location":"S1_Multilayer_Perceptron/#121-neural-network-layers","text":"back to top Now that we have the idea of the most basic building block of a neural network, we will start to discuss the larger architecture. The reason we focused on the lowest building block, is that neural networks are modular . They are made up of instances of these perceptrons or neurons. neurons in parallel make up a layer These layers feed into one another. When each node of a preceding layer is connected to every node of a following layer, we say they are fully connected and the receiving layer is a dense layer . In a moment we will talk about input, output and hidden layers, for neural networks with three or more layers.","title":"1.2.1 Neural Network Layers"},{"location":"S1_Multilayer_Perceptron/#122-the-activation-function","text":"back to top It turns out that stringing together a bunch of linear functions, will still result in overall linear relationships. We need a way to break out of this. A neat trick is introduced at the output of each neuron. The output passes through an activation function . There are a handful of different activation functions used in practice, the most common is known as the rectifier function: max(f(x), 0) and the resulting node can be schematically drawn like this: with the inset of the summation node indicating that at a minimum the resultant y value is 0.","title":"1.2.2 The Activation Function"},{"location":"S1_Multilayer_Perceptron/#exercise-2-the-rectifier-function","text":"Write a function called my_perceptron that takes x, a length 2 array, as an input. Have your function return the maximum of \\((0, w*x + b)\\) where w is a length 2 weight vector. # Code cell for exercise 2 def my_perceptron ( x ): \"\"\" a simple 2 input feature perceptron with predefined weights, intercept, and a rectifier activation function Parameters ---------- x: array the input array of length 2 Returns ------- rect: int the rectified output of the perceptron \"\"\" # # define b, w, and y (y=mx+b) # w = # b = # y = # # return the max of 0 and y # rect = return rect After you write your function make sure it returns 0 when the output of the linear component is negative. def test_zero_output (): x = np . array ([ - 10 , - 10 ]) # 10 * 1 + (-10) * 2 + 1 = -11 assert my_perceptron ( x ) == 0 , \"The output is not zero when it should be!\" test_zero_output () print ( \"test passing\" ) test passing","title":"\ud83c\udfcb\ufe0f Exercise 2: The Rectifier Function"},{"location":"S1_Multilayer_Perceptron/#123-stacking-layers","text":"back to top When we stack many layers together, we create what are traditionally regarded as neural networks. the first and last layers are called the input and output layers, while the inbetween layers are referred to as hidden layers, since their outputs are not directly seen. Tradditionally, a neural network with three or more hidden layers is referred to as a deep neural network. Notice that in this schematic, the last node does not have an activation function. This is typical of a regression task. In a classification task, we might require an activation function here.","title":"1.2.3 Stacking Layers"},{"location":"S1_Multilayer_Perceptron/#124-building-sequential-neural-networks","text":"back to top Now that we have the essential components of a neural network architecture, we can enter into the domain of overall naming conventions for architecure types. The classic neural network architecture is a feed forward neural network, where every preceding layer feeds into the next layer. We will practice building that with keras.","title":"1.2.4 Building Sequential Neural Networks"},{"location":"S1_Multilayer_Perceptron/#exercise-3-building-sequential-layers","text":"In the cell bellow, use keras to build a 3-layer network with activation='relu' and 512 units. Create the output layer so that it can predict 1 continuous value. # Code cell for exercise 3 # DECLARE THE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### # the hidden ReLU layers # the linear output layer ])","title":"\ud83c\udfcb\ufe0f Exercise 3: Building Sequential Layers"},{"location":"S1_Multilayer_Perceptron/#exercise-4-other-activation-functions","text":"There are other activation functions we can use after the summation in a neural node. Use the code below to plot and inspect them! Pick one and do a quick google search on what that activation function's best use case is. # Code cell for exercise 4 import tensorflow as tf # YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else activation_layer = layers . Activation ( 'relu' ) x = tf . linspace ( - 3.0 , 3.0 , 100 ) y = activation_layer ( x ) # once created, a layer is callable just like a function plt . plot ( x , y ) plt . xlim ( - 3 , 3 ) plt . xlabel ( \"Input\" ) plt . ylabel ( \"Output\" ) plt . show ()","title":"\ud83c\udfcb\ufe0f Exercise 4: Other Activation Functions"},{"location":"S1_Multilayer_Perceptron/#13-neural-network-training","text":"back to top We've defined neural network architectures, now how do we train them? There are two main concepts here: the loss function which we've encountered before, and the optimizer the means by which we improve the loss function","title":"1.3 Neural Network Training"},{"location":"S1_Multilayer_Perceptron/#131-the-loss-function","text":"back to top In previous sessions, we've envountered MSE: MSE = \\frac{1}{N}\\sum{(y-\\hat{y})^2} Another common loss for neural networks is the mean absolute error (MAE): MAE = \\frac{1}{N}\\sum{|y-\\hat{y}|} In anycase, the loss function describes the difference between the actual and predicted output of the model. The important thing to note, is that the weights in the neural network are systematically updated according to this loss function, they do this via an optimization algorithm.","title":"1.3.1 The Loss Function"},{"location":"S1_Multilayer_Perceptron/#132-the-optimizer","text":"back to top In order to update the neural network weights to improve the loss function, we require an algorithm. Virtually all available algorithms for this purpose fall within the family of stochastic gradient descent . This works essentially in these iterative steps: a subsample of the input data is passed through the network a loss is computed the weights are adjusted in a direction to improve the loss The key here is in step 3. The brilliance of neural networks, is that the loss function is differentiable with respect to the weights in the network, and so the change in loss can be ascribed to certain weight changes. We refer to this as assigning blame in the network, and it works through the mathematical chain rule of differentiation. We won't go into great detail here, other than to make a nod to it, and that this algorithm (step 3) is referred to as back propagation . The three step process is repeated until a stop criteria is reached, the simplest being the loss stops improving above some threshold or a desired loss is achieved. In the above animation, the black line represents the output of the model, the red dots make up a minibatch or simply a batch while the opaque red dots represent the whole training dataset. Exposing the model to an entire round of the training data is referred to as an epoch . The training loss improves with additional rounds of trianing (middle panel) and the weights are adjusted to update the model (right panel).","title":"1.3.2 The Optimizer"},{"location":"S1_Multilayer_Perceptron/#133-batches-and-epochs","text":"An epoch is the number of times the model will see the entire training set A batch is the number of training samples the model will run before calculating a total error and updating its internal parameters. Variability of batch (from 1 sample all the way to the entire training set size) leads to different categorizations of the optimizer algorithm: Batch Gradient Descent. Batch Size = Size of Training Set Stochastic Gradient Descent. Batch Size = 1 Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set We will visit additional details about batch and epochs in the next session when we discuss model evaluation.","title":"1.3.3 Batches and Epochs"},{"location":"S1_Multilayer_Perceptron/#134-learning-rate","text":"back to top notice how in the above animation the model makes progressive steps toward a global optimum. The size of these steps is determined by the learning rate . You can think of it as the amount of improvement to make in the direction of steepest descent (the derivative of the loss function in regards to the changes in weights). Sometimes a large step size can result in stepping over crevices in the solution surface and getting stuck, while too small of step sizes can lead to a slow algorithm. Often the optimum learning rate is not obvious, luckily there are some optimizers that are self-calibrating in this regard. Adam is one such optimizer available to us in keras. # we can compile the model like so model . compile ( optimizer = \"adam\" , loss = \"mae\" , )","title":"1.3.4 Learning Rate"},{"location":"S1_Multilayer_Perceptron/#exercise-51-train-your-first-neural-networks","text":"back to top We're going to train our first neural network. Take the model you created in exercise 3 and paste it in the cell below. Make sure that the input_shape of the first layer matches the number of features in X_train_std X_train_std . shape [ 1 ] 13 # Code cell for exercise 5 # DECLARE THE MODEL model = keras . Sequential ([ ### YOUR CODE HERE ### # the hidden ReLU layers # the linear output layer ]) Now we'll compile the model model . compile ( optimizer = 'adam' , loss = 'mse' , ) And then train for 10 epochs history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 10 , ) Epoch 1/10 15/15 [==============================] - 1s 33ms/step - loss: 0.6389 - val_loss: 0.1925 Epoch 2/10 15/15 [==============================] - 0s 22ms/step - loss: 0.1330 - val_loss: 0.1054 Epoch 3/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0739 - val_loss: 0.0587 Epoch 4/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0442 - val_loss: 0.0378 Epoch 5/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0266 - val_loss: 0.0283 Epoch 6/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0193 - val_loss: 0.0233 Epoch 7/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0165 - val_loss: 0.0212 Epoch 8/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0151 - val_loss: 0.0204 Epoch 9/10 15/15 [==============================] - 0s 22ms/step - loss: 0.0144 - val_loss: 0.0199 Epoch 10/10 15/15 [==============================] - 0s 23ms/step - loss: 0.0140 - val_loss: 0.0194 Let's take a look at our training history: pd . DataFrame ( history . history ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss val_loss 0 0.638868 0.192506 1 0.133038 0.105378 2 0.073934 0.058686 3 0.044192 0.037832 4 0.026648 0.028253 5 0.019265 0.023272 6 0.016508 0.021151 7 0.015126 0.020368 8 0.014371 0.019929 9 0.014009 0.019429 # convert the training history to a dataframe history_df = pd . DataFrame ( history . history ) # use Pandas native plot method fig , ax = plt . subplots ( figsize = ( 10 , 5 )) history_df [ 'loss' ] . plot ( ax = ax ) history_df [ 'val_loss' ] . plot ( ax = ax ) ax . set_ylim ( 0 , .05 ) (0.0, 0.05)","title":"\ud83c\udfcb\ufe0f Exercise 5.1: Train your first Neural Networks"},{"location":"S1_Multilayer_Perceptron/#exercise-52-improve-loss-by-varying-nodes-and-hidden-layers","text":"Take your former model as a starting point and now either add nodes or layers to see if the model improves model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 10 , ) Epoch 1/10 15/15 [==============================] - 1s 11ms/step - loss: 3.0090 - val_loss: 1.9299 Epoch 2/10 15/15 [==============================] - 0s 4ms/step - loss: 1.3536 - val_loss: 0.7737 Epoch 3/10 15/15 [==============================] - 0s 3ms/step - loss: 0.4739 - val_loss: 0.2300 Epoch 4/10 15/15 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.0781 Epoch 5/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0471 Epoch 6/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0402 - val_loss: 0.0354 Epoch 7/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0325 Epoch 8/10 15/15 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0305 Epoch 9/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0288 Epoch 10/10 15/15 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0271 y_pred = model . predict ( X_test_std ) plt . plot ( np . exp ( y_test_std ), np . exp ( y_pred ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f5d26303e10>]","title":"\ud83c\udfcb\ufe0f Exercise 5.2: Improve loss by varying nodes and hidden layers"},{"location":"S1_Multilayer_Perceptron/#exercise-53-learning-curves","text":"Using 4 hidden layers now create 4 models that run for 30 epochs each: Vary the number of nodes in each layer Record the train/val/test score (MSE) Plot either total nodes or total trainable parameters vs score for each of the 5 models # Code Cell for Exercise 1.3.5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tot. units test mse val mse train mse 0 4 0.030658 0.030919 0.031088 1 25 0.020217 0.020164 0.019062 2 100 0.017429 0.018110 0.017416 3 289 0.019124 0.019371 0.018190 When we look at our historical loss do we notice that sometimes before the last epoch we actually hit a minimum? We'll discuss how to deal with this in the next session!","title":"\ud83c\udfcb\ufe0f Exercise 5.3: Learning Curves"},{"location":"S2_Feed_Forward_Neural_Networks/","text":"General Applications of Neural Networks Session 2: Feed Forward Neural Networks \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will continue with our discussion on neural networks. Specifically, we will revisit the concept of learning curves, regularization and classification tasks, this time as they pertain to neural networks! images in this notebook borrowed from Ryan Holbrook 2.0 Preparing Environment and Importing Data \u00b6 back to top ! pip uninstall scikit - learn - y ! pip install - U scikit - learn Found existing installation: scikit-learn 0.22.2.post1 Uninstalling scikit-learn-0.22.2.post1: Successfully uninstalled scikit-learn-0.22.2.post1 Collecting scikit-learn Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22.3 MB 2.2 MB/s \u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Collecting threadpoolctl>=2.0.0 Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB) Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Installing collected packages: threadpoolctl, scikit-learn Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0 2.0.1 Import Packages \u00b6 back to top from tensorflow import keras from tensorflow.keras import layers import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from copy import copy import numpy as np sns . set () from sklearn.preprocessing import StandardScaler , OneHotEncoder , LabelEncoder from sklearn.compose import make_column_transformer , make_column_selector from sklearn.metrics import classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import set_config from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer set_config ( display = 'diagram' ) 2.0.2 Load Dataset \u00b6 back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = np . log ( y_train ) # log output y y_val_std = np . log ( y_val ) # log output y y_test_std = np . log ( y_test ) # log output y preprocessor #sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c {color: black;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c pre{padding: 0;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable {background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator:hover {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-item {z-index: 1;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:only-child::after {width: 0;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') 2.1 Evaluating the Model \u00b6 back to top In the last session, we randomly chose 10 and then 30 epochs (10 rounds of the training data) to train our neural network. Now we'll garner the language to be more systematic in our approach. 2.1.1 Learning Curves \u00b6 Recall learning curves from last week, where we plot model score (accuracy/mse/r2/etc.) over model complexity (trees in a forest, degrees in a polynomial, etc.): img src We have a similar situation with neural networks, accept here, the model complexity is defined by both the number of epochs, and the capacity of the model. 2.1.1.1 Training Data (Epochs) \u00b6 A greater number of epochs allows the model to further tune itself to the training data. Remember that our model solves its weights by a solver function, most often a form of stochastic gradient descent. Because this solver is iterative, the longer we allow it run the closer it will get to finding its true weights. The caveat here is that, like we have seen before, the model is only learning the proper weights according to the training data, which we know includes noise, otherwise known as irreducible error. Our data science senses should be tingling: there's an optimum number of epochs here, and it will counter balance the trade-off between error due to bias (too few epochs) and error due to variance (too many epochs). 2.1.1.2 Complexity (Capacity) \u00b6 The capacity of the model is defined by the architecture. It is the total number of trainable weights available to the solver function. The more weights, the more capacity. Capacity determines the upper limit for which our model can learn relationships between the data. Again we should recall from our session on feature engineering: the more training data we have, the more capacity we should give our model to account for that abundance of training data. This is also influenced by the actual complexity between the input and output data, X and y, who's function we are attempting to approximate with the neural network. The more complicated the relationship, the more capacity we should give to our model. Capacity can be increased either by widening our model (increasing the neurons in a layer) or deepening our model(increasing the number of layers in our model). 2.1.2 Early Stopping \u00b6 back to top Without knowing the true relationship between X and y, or the degree to which there is irreducible error in our data, we return to our familiar learning curves to pragmaticaly determine how long we should train our model, that is, how many epochs should be ran and how many neurons we should give our model. When dealing with the number of epochs, we can program this into the training session automatically with early stopping. Early stopping allows us to discontinue training the model when either the validation score stops improving, or stops improving by some margin. This allows us to both save time during training and to avoid overfitting our model. To account for underfitting (not training the model long enough) we can simply set our number of training epochs to some large number and allow early stopping to take care of the rest. In TF/Keras, we can envoke early stopping by setting a callback a callback is simply a function that is called every so often. from tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) In the above, we are saying detect when the validation loss stops improving by 0.001; check this over the past 20 epochs to avoid stopping early due to noise; and restore the best weights over that past 20 epoch period when early stopping is envoked. To envoke early stopping we would enlist it in our call to fit like so: model.fit( # X, y, batch size etc, ... callbacks=[early_stopping], ) \ud83c\udfcb\ufe0f Exercise 1: Try Early Stopping \u00b6 Take your best model from the last exercise in session 1 and apply early stopping from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import EarlyStopping model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_5 (Dense) (None, 32) 448 _________________________________________________________________ dense_6 (Dense) (None, 1024) 33792 _________________________________________________________________ dense_7 (Dense) (None, 1024) 1049600 _________________________________________________________________ dense_8 (Dense) (None, 32) 32800 _________________________________________________________________ dense_9 (Dense) (None, 1) 33 ================================================================= Total params: 1,116,673 Trainable params: 1,116,673 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 2s 50ms/step - loss: 0.4680 - val_loss: 0.1534 Epoch 2/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.1286 - val_loss: 0.0802 Epoch 3/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0770 - val_loss: 0.0502 Epoch 4/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0511 - val_loss: 0.0378 Epoch 5/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0362 - val_loss: 0.0321 Epoch 6/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0298 - val_loss: 0.0272 Epoch 7/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0268 - val_loss: 0.0288 Epoch 8/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0223 - val_loss: 0.0206 Epoch 9/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0190 - val_loss: 0.0204 Epoch 10/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0177 - val_loss: 0.0218 Epoch 11/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0166 - val_loss: 0.0185 Epoch 12/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0152 - val_loss: 0.0207 Epoch 13/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0153 - val_loss: 0.0197 Epoch 14/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0147 - val_loss: 0.0204 Epoch 15/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0143 - val_loss: 0.0178 Epoch 16/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0133 - val_loss: 0.0165 Epoch 17/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0124 - val_loss: 0.0168 Epoch 18/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0129 - val_loss: 0.0166 Epoch 19/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0128 - val_loss: 0.0159 Epoch 20/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0119 - val_loss: 0.0159 Epoch 21/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0115 - val_loss: 0.0159 Epoch 22/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0112 - val_loss: 0.0158 Epoch 23/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0113 - val_loss: 0.0161 Epoch 24/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0113 - val_loss: 0.0173 Epoch 25/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0105 - val_loss: 0.0160 Epoch 26/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0104 - val_loss: 0.0172 Epoch 27/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0107 - val_loss: 0.0160 Epoch 28/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0099 - val_loss: 0.0159 Epoch 29/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0096 - val_loss: 0.0153 Epoch 30/1000 15/15 [==============================] - 1s 51ms/step - loss: 0.0097 - val_loss: 0.0176 Epoch 31/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0103 - val_loss: 0.0180 Epoch 32/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0102 - val_loss: 0.0161 Epoch 33/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0095 - val_loss: 0.0173 Epoch 34/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0101 - val_loss: 0.0171 Epoch 35/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0102 - val_loss: 0.0176 Epoch 36/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0092 - val_loss: 0.0160 Epoch 37/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0090 - val_loss: 0.0158 Epoch 38/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0088 - val_loss: 0.0200 Epoch 39/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0110 - val_loss: 0.0189 Epoch 40/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0096 - val_loss: 0.0172 Epoch 41/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0093 - val_loss: 0.0151 Epoch 42/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0082 - val_loss: 0.0153 Epoch 43/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0075 - val_loss: 0.0167 Epoch 44/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0080 - val_loss: 0.0157 Epoch 45/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0079 - val_loss: 0.0152 Epoch 46/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0077 - val_loss: 0.0173 Epoch 47/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0088 - val_loss: 0.0161 Epoch 48/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0081 - val_loss: 0.0166 Epoch 49/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0080 - val_loss: 0.0195 # Show the learning curves history_df = pd . DataFrame ( history . history ) fig , ax = plt . subplots ( 1 , 1 ) history_df . plot ( ax = ax ) ax . set_ylim ( 0 , .1 ) (0.0, 0.1) y_test_pred = np . exp ( model . predict ( X_test_std )) plt . plot ( y_test_pred , np . exp ( y_test_std ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f4c409a6210>] 2.2 Regularizing Layers: Dropout and Batch Normalization \u00b6 back to top There are dozens of different layer types for accomplishing different tasks (and more are being generated by researchers all the time). While we're on the topic of general model performance, there are two layer types we'll want to introduce here: dropout and batch normalization or just batchnorm. What we'll see with both of these types is that while they don't contain any neurons, they're useful to think of as layers, because they are an extra processing step between neural layers. 2.2.1 Dropout \u00b6 back to top Dropout is the Neural Network response to the wide success of ensemble learning. In a dropout layer, random neurons are dropped in each batch of training, i.e. their weighted updates are not sent to the next neural layer. Just as with random forests, the end result is that the neural network can be thought of as many independent models that vote on the final output. Put another way, when a network does not contain dropout layers, and has a capacity that exceeds that which would be suited for the true, underlying complexity level of the data, it can begin to fit to noise. This ability to fit to noise is based on very specific relationships between neurons, which fire uniquely given the particular training example. Adding dropout breaks these specific neural connections, and so the neural network as a whole is forced to find weights that apply generally, as there is no guarantee they will be turned on when their specific training example they would usually overfit for comes around again. Network with 50% dropout A last thing to note, is that after adding dropout, we will typically need to add additional layers to our network to maintain the overall capacity of the network. keras . Sequential ([ # ... layers . Dropout ( rate = 0.3 ), # apply 30% dropout to the next layer layers . Dense ( 16 ), # ... ]) <tensorflow.python.keras.engine.sequential.Sequential at 0x7f2ae011ba50> When adding dropout to a model we will usually want to increase the depth: # Dropout model model = keras . Sequential ([ layers . Dense ( 512 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . Dropout ( 0.3 ), layers . Dense ( 512 * 2 , activation = 'relu' ), layers . Dropout ( 0.3 ), layers . Dense ( 512 , activation = 'relu' ), layers . Dense ( 1 ), ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 512) 7168 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense_11 (Dense) (None, 1024) 525312 _________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_12 (Dense) (None, 512) 524800 _________________________________________________________________ dense_13 (Dense) (None, 1) 513 ================================================================= Total params: 1,057,793 Trainable params: 1,057,793 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 1s 51ms/step - loss: 0.6459 - val_loss: 0.1513 Epoch 2/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1613 - val_loss: 0.1027 Epoch 3/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.1056 - val_loss: 0.0918 Epoch 4/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0730 - val_loss: 0.0689 Epoch 5/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0566 - val_loss: 0.0742 Epoch 6/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0466 - val_loss: 0.0360 Epoch 7/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0409 - val_loss: 0.0673 Epoch 8/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0396 - val_loss: 0.0652 Epoch 9/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0339 - val_loss: 0.0702 Epoch 10/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0337 - val_loss: 0.0623 Epoch 11/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0329 - val_loss: 0.0561 Epoch 12/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0307 - val_loss: 0.0721 Epoch 13/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0294 - val_loss: 0.0828 Epoch 14/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0273 - val_loss: 0.0763 Epoch 15/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0267 - val_loss: 0.0672 Epoch 16/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0258 - val_loss: 0.0740 Epoch 17/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0249 - val_loss: 0.0462 Epoch 18/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0263 - val_loss: 0.0770 Epoch 19/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0246 - val_loss: 0.0679 Epoch 20/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0238 - val_loss: 0.0984 Epoch 21/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0246 - val_loss: 0.0536 Epoch 22/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0237 - val_loss: 0.0837 Epoch 23/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0214 - val_loss: 0.0754 Epoch 24/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0216 - val_loss: 0.0664 Epoch 25/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0222 - val_loss: 0.0631 Epoch 26/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0215 - val_loss: 0.0858 2.2.2 Batch Normalization \u00b6 back to top Batch normalization ( batchnorm ) accomplishes a task very similar to sklearn's StandardScaler , and in fact, it can be added to the front of a neural network in place of sklearn's StandardScaler to scale and center the input data. But batchnorm is also used between layers, why would we want to rescale the output from one layer before feeding into the next? For the same reason we would want to scale and center our data in the first place: it levels the playing field for the following layer to find the important relationships. The optimizer algorithm, SGD, will shift weights in proportion to how large the activation is out of each neuron, and so large differences can lead to spurious behavior. In practice, batchnorm is often implemented to make the overall training time faster, rather than to improve the final loss (although in some cases it does do this as well). It is good to envoke batchnorm in particular, if you are dealing with neural networks that take a long time to train. Batchnorm can be added between layers, as well as between a layer and its activation function. # BatchNorm model model = keras . Sequential ([ layers . Dense ( 512 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . BatchNormalization (), layers . Dense ( 512 * 2 , activation = 'relu' ), layers . BatchNormalization (), layers . Dense ( 512 , activation = 'relu' ), layers . Dense ( 1 ), ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_14 (Dense) (None, 512) 7168 _________________________________________________________________ batch_normalization (BatchNo (None, 512) 2048 _________________________________________________________________ dense_15 (Dense) (None, 1024) 525312 _________________________________________________________________ batch_normalization_1 (Batch (None, 1024) 4096 _________________________________________________________________ dense_16 (Dense) (None, 512) 524800 _________________________________________________________________ dense_17 (Dense) (None, 1) 513 ================================================================= Total params: 1,063,937 Trainable params: 1,060,865 Non-trainable params: 3,072 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 2s 54ms/step - loss: 2.7890 - val_loss: 2.5754 Epoch 2/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.7404 - val_loss: 2.2122 Epoch 3/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.2845 - val_loss: 1.5998 Epoch 4/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1671 - val_loss: 1.4054 Epoch 5/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1195 - val_loss: 1.0034 Epoch 6/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.1000 - val_loss: 0.9543 Epoch 7/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1353 - val_loss: 0.6496 Epoch 8/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1688 - val_loss: 0.4768 Epoch 9/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1562 - val_loss: 0.3414 Epoch 10/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1296 - val_loss: 0.3393 Epoch 11/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0750 - val_loss: 0.2155 Epoch 12/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0512 - val_loss: 0.1708 Epoch 13/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0332 - val_loss: 0.1139 Epoch 14/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0329 - val_loss: 0.0851 Epoch 15/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0411 - val_loss: 0.0597 Epoch 16/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0455 - val_loss: 0.0591 Epoch 17/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0510 - val_loss: 0.0424 Epoch 18/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0381 - val_loss: 0.0549 Epoch 19/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0403 - val_loss: 0.0466 Epoch 20/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0290 - val_loss: 0.0468 Epoch 21/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0379 - val_loss: 0.0504 Epoch 22/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0444 - val_loss: 0.0553 Epoch 23/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0479 - val_loss: 0.0480 Epoch 24/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0331 - val_loss: 0.0400 Epoch 25/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0416 - val_loss: 0.0333 Epoch 26/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0396 - val_loss: 0.0276 Epoch 27/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0426 - val_loss: 0.0386 Epoch 28/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0308 - val_loss: 0.0427 Epoch 29/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0365 - val_loss: 0.0369 Epoch 30/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0337 - val_loss: 0.0393 Epoch 31/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0383 - val_loss: 0.0314 Epoch 32/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0358 - val_loss: 0.0533 Epoch 33/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0354 - val_loss: 0.0336 Epoch 34/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0332 - val_loss: 0.0363 Epoch 35/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0326 - val_loss: 0.0638 Epoch 36/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0459 - val_loss: 0.0427 Epoch 37/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0382 - val_loss: 0.0293 Epoch 38/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0440 - val_loss: 0.0446 Epoch 39/1000 15/15 [==============================] - 1s 47ms/step - loss: 0.0436 - val_loss: 0.0673 Epoch 40/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0535 - val_loss: 0.0354 Epoch 41/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0245 - val_loss: 0.0373 Epoch 42/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0225 - val_loss: 0.0313 Epoch 43/1000 15/15 [==============================] - 1s 47ms/step - loss: 0.0179 - val_loss: 0.0267 Epoch 44/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0174 - val_loss: 0.0427 Epoch 45/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0211 - val_loss: 0.0240 Epoch 46/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0139 - val_loss: 0.0275 Epoch 47/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0138 - val_loss: 0.0319 Epoch 48/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0205 - val_loss: 0.0237 Epoch 49/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0163 - val_loss: 0.0310 Epoch 50/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0169 - val_loss: 0.0259 Epoch 51/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0210 - val_loss: 0.0527 Epoch 52/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0261 - val_loss: 0.0432 Epoch 53/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0255 - val_loss: 0.0282 Epoch 54/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0246 - val_loss: 0.0268 Epoch 55/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0183 - val_loss: 0.0577 Epoch 56/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0236 - val_loss: 0.0276 Epoch 57/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0235 - val_loss: 0.0524 Epoch 58/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0308 - val_loss: 0.0517 Epoch 59/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0243 - val_loss: 0.0433 Epoch 60/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0207 - val_loss: 0.0293 Epoch 61/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0191 - val_loss: 0.0307 Epoch 62/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0183 - val_loss: 0.0350 Epoch 63/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0252 - val_loss: 0.0460 Epoch 64/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0281 - val_loss: 0.0365 Epoch 65/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0459 - val_loss: 0.0656 # Show the learning curves history_df = pd . DataFrame ( history . history ) fig , ax = plt . subplots ( 1 , 1 ) history_df . plot ( ax = ax ) ax . set_ylim ( 0 , .2 ) (0.0, 0.2) y_test_pred = np . exp ( model . predict ( X_test_std )) plt . plot ( y_test_pred , np . exp ( y_test_std ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f4c3ceb5150>] Model with BatchNorm and Dropout: from tensorflow import keras from tensorflow.keras import layers # Dropout and BatchNorm Model model = keras . Sequential ([ layers . Dense ( 512 , input_shape = [ X_train_std . shape [ 1 ]]), layers . BatchNormalization (), layers . Activation ( 'relu' ), # separate activation layers . Dropout ( 0.3 ), layers . Dense ( 1024 , activation = 'relu' ), # built-in activation layers . Dropout ( 0.3 ), layers . BatchNormalization (), layers . Dense ( 512 , activation = 'relu' ), layers . Dropout ( 0.3 ), layers . BatchNormalization (), layers . Dense ( 1 ), ]) print ( model . summary ()) Model: \"sequential_4\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 512) 7168 _________________________________________________________________ batch_normalization_2 (Batch (None, 512) 2048 _________________________________________________________________ activation (Activation) (None, 512) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________ dense_19 (Dense) (None, 1024) 525312 _________________________________________________________________ dropout_3 (Dropout) (None, 1024) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 1024) 4096 _________________________________________________________________ dense_20 (Dense) (None, 512) 524800 _________________________________________________________________ dropout_4 (Dropout) (None, 512) 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 512) 2048 _________________________________________________________________ dense_21 (Dense) (None, 1) 513 ================================================================= Total params: 1,065,985 Trainable params: 1,061,889 Non-trainable params: 4,096 _________________________________________________________________ None \ud83c\udfcb\ufe0f Exercise 2: Try Batch Normalization and Dropout \u00b6 Create 3 models and train for 100 epochs with early stopping: compile and train the model previously defined above recreate the architecture but remove the batchnorm layers recreate the architecture but remove the dropout layers and one hidden, dense layer Compare the number of epochs required to converge, and the overall loss # Dropout and BatchNorm Model model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mae' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 100 , verbose = 1 , callbacks = [ early_stopping ], ) Epoch 1/100 15/15 [==============================] - 1s 16ms/step - loss: 1.7653 - val_loss: 1.3092 Epoch 2/100 15/15 [==============================] - 0s 6ms/step - loss: 1.3614 - val_loss: 0.9521 Epoch 3/100 15/15 [==============================] - 0s 7ms/step - loss: 1.0655 - val_loss: 0.6253 Epoch 4/100 15/15 [==============================] - 0s 7ms/step - loss: 0.8148 - val_loss: 0.3236 Epoch 5/100 15/15 [==============================] - 0s 6ms/step - loss: 0.6566 - val_loss: 0.2231 Epoch 6/100 15/15 [==============================] - 0s 6ms/step - loss: 0.5569 - val_loss: 0.1752 Epoch 7/100 15/15 [==============================] - 0s 6ms/step - loss: 0.5185 - val_loss: 0.1731 Epoch 8/100 15/15 [==============================] - 0s 6ms/step - loss: 0.4466 - val_loss: 0.1596 Epoch 9/100 15/15 [==============================] - 0s 6ms/step - loss: 0.4092 - val_loss: 0.1712 Epoch 10/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3836 - val_loss: 0.1516 Epoch 11/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3530 - val_loss: 0.1603 Epoch 12/100 15/15 [==============================] - 0s 6ms/step - loss: 0.3323 - val_loss: 0.1423 Epoch 13/100 15/15 [==============================] - 0s 6ms/step - loss: 0.3423 - val_loss: 0.1385 Epoch 14/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3116 - val_loss: 0.1289 Epoch 15/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3055 - val_loss: 0.1269 Epoch 16/100 15/15 [==============================] - 0s 7ms/step - loss: 0.2849 - val_loss: 0.1067 Epoch 17/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2813 - val_loss: 0.1116 Epoch 18/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2737 - val_loss: 0.1163 Epoch 19/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2520 - val_loss: 0.1149 Epoch 20/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2454 - val_loss: 0.1366 Epoch 21/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2406 - val_loss: 0.1094 Epoch 22/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2334 - val_loss: 0.1069 Epoch 23/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2283 - val_loss: 0.1109 Epoch 24/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2116 - val_loss: 0.1092 Epoch 25/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2048 - val_loss: 0.1024 Epoch 26/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1885 - val_loss: 0.1004 Epoch 27/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1979 - val_loss: 0.1115 Epoch 28/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1863 - val_loss: 0.1077 Epoch 29/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1713 - val_loss: 0.1106 Epoch 30/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1756 - val_loss: 0.1057 Epoch 31/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1598 - val_loss: 0.1120 Epoch 32/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1706 - val_loss: 0.1153 Epoch 33/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1553 - val_loss: 0.1175 Epoch 34/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1628 - val_loss: 0.1051 Epoch 35/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1538 - val_loss: 0.0985 Epoch 36/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1472 - val_loss: 0.1005 Epoch 37/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1447 - val_loss: 0.0950 Epoch 38/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1439 - val_loss: 0.1088 Epoch 39/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1530 - val_loss: 0.1176 Epoch 40/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1400 - val_loss: 0.1240 Epoch 41/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1438 - val_loss: 0.0981 Epoch 42/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1410 - val_loss: 0.0963 Epoch 43/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1420 - val_loss: 0.0972 Epoch 44/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1468 - val_loss: 0.1168 Epoch 45/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1419 - val_loss: 0.0949 Epoch 46/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1401 - val_loss: 0.0940 Epoch 47/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1394 - val_loss: 0.0970 Epoch 48/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1368 - val_loss: 0.1291 Epoch 49/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1445 - val_loss: 0.1008 Epoch 50/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1467 - val_loss: 0.1030 Epoch 51/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1404 - val_loss: 0.1066 Epoch 52/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1420 - val_loss: 0.0946 Epoch 53/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1370 - val_loss: 0.0946 Epoch 54/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1367 - val_loss: 0.1009 Epoch 55/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1403 - val_loss: 0.1037 Epoch 56/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1377 - val_loss: 0.1040 Epoch 57/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1377 - val_loss: 0.0935 Epoch 58/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1420 - val_loss: 0.0921 Epoch 59/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1333 - val_loss: 0.0990 Epoch 60/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1305 - val_loss: 0.1067 Epoch 61/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1362 - val_loss: 0.1123 Epoch 62/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1344 - val_loss: 0.0942 Epoch 63/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1267 - val_loss: 0.0981 Epoch 64/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1247 - val_loss: 0.0904 Epoch 65/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1307 - val_loss: 0.0979 Epoch 66/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1321 - val_loss: 0.0976 Epoch 67/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1325 - val_loss: 0.1214 Epoch 68/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1340 - val_loss: 0.1112 Epoch 69/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1290 - val_loss: 0.1022 Epoch 70/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1330 - val_loss: 0.1014 Epoch 71/100 15/15 [==============================] - 0s 8ms/step - loss: 0.1274 - val_loss: 0.0989 Epoch 72/100 15/15 [==============================] - 0s 8ms/step - loss: 0.1179 - val_loss: 0.0913 Epoch 73/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1157 - val_loss: 0.0970 Epoch 74/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1280 - val_loss: 0.0909 Epoch 75/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1203 - val_loss: 0.0940 Epoch 76/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1293 - val_loss: 0.0934 Epoch 77/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1300 - val_loss: 0.0975 Epoch 78/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1257 - val_loss: 0.0915 Epoch 79/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1265 - val_loss: 0.0975 Epoch 80/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1192 - val_loss: 0.0906 Epoch 81/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1214 - val_loss: 0.0902 Epoch 82/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1209 - val_loss: 0.0994 Epoch 83/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1200 - val_loss: 0.0971 Epoch 84/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1238 - val_loss: 0.0942 2.3 Binary Classification with Neural Networks \u00b6 When dealing with classification tasks, everything we've covered up till now with regression still applies. The main difference is the format of the last layer's outputs and the choice of loss function. 2.3.1 Accuracy and Cross Entropy \u00b6 Formerly, we've encountered accuracy in classification tasks. It is the ratio of correct predictions over total predictions: accuracy = number_corect / total (and when classes were heavily imballanced we used a weighted accuracy). The problem with using accuracy as a loss function, is that it does not change smoothly (there are jumps in the numerator since it is just a count in correct predictions), which the SGD algorithm requires in order to work properly. We need another metric. the further a model is from predicting the correct class, the higher the loss. Instead we use cross-entropy , we won't go into detail here, other than that it is a distance measure between two probabilities (the probability of predicting the class or the incorrect class). We want the probability for predicting the correct class to be 1 (100%) and cross-entropy will measure the distance the current probability of the model is from 1. We set cross-entropy as our loss when we compile the model. In this case we have two classes so we use binary_crossentropy model.compile( # optimizer... loss='binary_crossentropy', metrics=['binary_accuracy'], ) 2.3.2 0 or 1: The Sigmoid Function \u00b6 Finally, we need to introduce a special activation function that will map our last layer outputs from 0 to 1, to feed into our loss function. The traditional function we use for this is the sigmoid function . The sigmoid function maps values on the interval [0, 1] To get the final class prediction, we need a threshold probability, typically 0.5 where we will round up to the class label. Keras will set to 0.5 by default. We set the sigmoid activation function in the last layer: model = keras.Sequential([ # ... previous layers ... layers.Dense(1, activation='sigmoid'), ]) 2.3.3 Classification Example \u00b6 wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'type' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = y_train . map ({ 'white' : 0 , 'red' : 1 }) # convert to int y_val_std = y_val . map ({ 'white' : 0 , 'red' : 1 }) # convert to int y_test_std = y_test . map ({ 'white' : 0 , 'red' : 1 }) # convert to int preprocessor #sk-540523ed-2679-4d85-a24d-3d67318116c3 {color: black;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 pre{padding: 0;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable {background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-item {z-index: 1;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:only-child::after {width: 0;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Dense ( 4 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . Dense ( 4 , activation = 'relu' ), layers . Dense ( 1 , activation = 'sigmoid' ), ]) print ( model . summary ()) Model: \"sequential_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_22 (Dense) (None, 4) 52 _________________________________________________________________ dense_23 (Dense) (None, 4) 20 _________________________________________________________________ dense_24 (Dense) (None, 1) 5 ================================================================= Total params: 77 Trainable params: 77 Non-trainable params: 0 _________________________________________________________________ None tf . keras . utils . plot_model ( model , show_layer_names = True , # show_dtype=True, show_shapes = True ) We set cross-entropy as our loss when we compile the model. In this case we have two classes so we use binary_crossentropy Introducing metrics : we can track other forms of performance during training with the metrics parameter in model.compile . We will look at the results when we finish training. model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], # new metrics flag here ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 512 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 1 , ) Epoch 1/1000 8/8 [==============================] - 1s 21ms/step - loss: 0.6466 - binary_accuracy: 0.6292 - val_loss: 0.6328 - val_binary_accuracy: 0.6371 Epoch 2/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6336 - binary_accuracy: 0.6623 - val_loss: 0.6196 - val_binary_accuracy: 0.6658 Epoch 3/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6207 - binary_accuracy: 0.6891 - val_loss: 0.6066 - val_binary_accuracy: 0.6954 Epoch 4/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6079 - binary_accuracy: 0.7154 - val_loss: 0.5938 - val_binary_accuracy: 0.7192 Epoch 5/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5951 - binary_accuracy: 0.7365 - val_loss: 0.5812 - val_binary_accuracy: 0.7381 Epoch 6/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5826 - binary_accuracy: 0.7597 - val_loss: 0.5687 - val_binary_accuracy: 0.7594 Epoch 7/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5700 - binary_accuracy: 0.7756 - val_loss: 0.5559 - val_binary_accuracy: 0.7750 Epoch 8/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5573 - binary_accuracy: 0.7909 - val_loss: 0.5428 - val_binary_accuracy: 0.7939 Epoch 9/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5444 - binary_accuracy: 0.8041 - val_loss: 0.5295 - val_binary_accuracy: 0.8095 Epoch 10/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.8158 - val_loss: 0.5162 - val_binary_accuracy: 0.8194 Epoch 11/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.5184 - binary_accuracy: 0.8281 - val_loss: 0.5028 - val_binary_accuracy: 0.8300 Epoch 12/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.5051 - binary_accuracy: 0.8388 - val_loss: 0.4890 - val_binary_accuracy: 0.8415 Epoch 13/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4913 - binary_accuracy: 0.8498 - val_loss: 0.4748 - val_binary_accuracy: 0.8539 Epoch 14/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4771 - binary_accuracy: 0.8580 - val_loss: 0.4601 - val_binary_accuracy: 0.8629 Epoch 15/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4627 - binary_accuracy: 0.8675 - val_loss: 0.4454 - val_binary_accuracy: 0.8695 Epoch 16/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4478 - binary_accuracy: 0.8725 - val_loss: 0.4309 - val_binary_accuracy: 0.8727 Epoch 17/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4327 - binary_accuracy: 0.8788 - val_loss: 0.4163 - val_binary_accuracy: 0.8785 Epoch 18/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4172 - binary_accuracy: 0.8831 - val_loss: 0.4014 - val_binary_accuracy: 0.8826 Epoch 19/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4015 - binary_accuracy: 0.8886 - val_loss: 0.3863 - val_binary_accuracy: 0.8892 Epoch 20/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3857 - binary_accuracy: 0.8944 - val_loss: 0.3710 - val_binary_accuracy: 0.8957 Epoch 21/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3695 - binary_accuracy: 0.9015 - val_loss: 0.3554 - val_binary_accuracy: 0.9064 Epoch 22/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3536 - binary_accuracy: 0.9078 - val_loss: 0.3400 - val_binary_accuracy: 0.9130 Epoch 23/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.3380 - binary_accuracy: 0.9168 - val_loss: 0.3247 - val_binary_accuracy: 0.9195 Epoch 24/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3226 - binary_accuracy: 0.9228 - val_loss: 0.3098 - val_binary_accuracy: 0.9286 Epoch 25/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.3074 - binary_accuracy: 0.9297 - val_loss: 0.2953 - val_binary_accuracy: 0.9392 Epoch 26/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.2926 - binary_accuracy: 0.9360 - val_loss: 0.2811 - val_binary_accuracy: 0.9417 Epoch 27/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2780 - binary_accuracy: 0.9409 - val_loss: 0.2671 - val_binary_accuracy: 0.9499 Epoch 28/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2639 - binary_accuracy: 0.9480 - val_loss: 0.2537 - val_binary_accuracy: 0.9548 Epoch 29/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2502 - binary_accuracy: 0.9540 - val_loss: 0.2408 - val_binary_accuracy: 0.9581 Epoch 30/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.2373 - binary_accuracy: 0.9589 - val_loss: 0.2285 - val_binary_accuracy: 0.9606 Epoch 31/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2248 - binary_accuracy: 0.9625 - val_loss: 0.2168 - val_binary_accuracy: 0.9663 Epoch 32/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2129 - binary_accuracy: 0.9658 - val_loss: 0.2056 - val_binary_accuracy: 0.9672 Epoch 33/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2017 - binary_accuracy: 0.9699 - val_loss: 0.1948 - val_binary_accuracy: 0.9688 Epoch 34/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1910 - binary_accuracy: 0.9729 - val_loss: 0.1849 - val_binary_accuracy: 0.9713 Epoch 35/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1811 - binary_accuracy: 0.9762 - val_loss: 0.1755 - val_binary_accuracy: 0.9745 Epoch 36/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1718 - binary_accuracy: 0.9787 - val_loss: 0.1665 - val_binary_accuracy: 0.9770 Epoch 37/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1630 - binary_accuracy: 0.9795 - val_loss: 0.1582 - val_binary_accuracy: 0.9778 Epoch 38/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.1546 - binary_accuracy: 0.9806 - val_loss: 0.1504 - val_binary_accuracy: 0.9778 Epoch 39/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1468 - binary_accuracy: 0.9806 - val_loss: 0.1431 - val_binary_accuracy: 0.9778 Epoch 40/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1394 - binary_accuracy: 0.9817 - val_loss: 0.1362 - val_binary_accuracy: 0.9803 Epoch 41/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1326 - binary_accuracy: 0.9819 - val_loss: 0.1297 - val_binary_accuracy: 0.9819 Epoch 42/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1262 - binary_accuracy: 0.9839 - val_loss: 0.1237 - val_binary_accuracy: 0.9819 Epoch 43/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1202 - binary_accuracy: 0.9849 - val_loss: 0.1183 - val_binary_accuracy: 0.9828 Epoch 44/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1147 - binary_accuracy: 0.9849 - val_loss: 0.1132 - val_binary_accuracy: 0.9828 Epoch 45/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.1096 - binary_accuracy: 0.9852 - val_loss: 0.1085 - val_binary_accuracy: 0.9844 Epoch 46/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1048 - binary_accuracy: 0.9858 - val_loss: 0.1042 - val_binary_accuracy: 0.9844 Epoch 47/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1003 - binary_accuracy: 0.9866 - val_loss: 0.1001 - val_binary_accuracy: 0.9852 Epoch 48/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0962 - binary_accuracy: 0.9866 - val_loss: 0.0963 - val_binary_accuracy: 0.9860 Epoch 49/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - binary_accuracy: 0.9869 - val_loss: 0.0929 - val_binary_accuracy: 0.9869 Epoch 50/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.0889 - binary_accuracy: 0.9869 - val_loss: 0.0898 - val_binary_accuracy: 0.9877 Epoch 51/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0856 - binary_accuracy: 0.9871 - val_loss: 0.0868 - val_binary_accuracy: 0.9877 Epoch 52/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0826 - binary_accuracy: 0.9874 - val_loss: 0.0840 - val_binary_accuracy: 0.9877 Epoch 53/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0798 - binary_accuracy: 0.9871 - val_loss: 0.0813 - val_binary_accuracy: 0.9877 Epoch 54/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0771 - binary_accuracy: 0.9877 - val_loss: 0.0788 - val_binary_accuracy: 0.9877 Epoch 55/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0747 - binary_accuracy: 0.9880 - val_loss: 0.0765 - val_binary_accuracy: 0.9877 Epoch 56/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0725 - binary_accuracy: 0.9885 - val_loss: 0.0744 - val_binary_accuracy: 0.9877 Epoch 57/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0704 - binary_accuracy: 0.9882 - val_loss: 0.0726 - val_binary_accuracy: 0.9877 Epoch 58/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0685 - binary_accuracy: 0.9882 - val_loss: 0.0708 - val_binary_accuracy: 0.9877 Epoch 59/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0667 - binary_accuracy: 0.9880 - val_loss: 0.0691 - val_binary_accuracy: 0.9869 Epoch 60/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0651 - binary_accuracy: 0.9882 - val_loss: 0.0675 - val_binary_accuracy: 0.9869 Epoch 61/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0635 - binary_accuracy: 0.9888 - val_loss: 0.0661 - val_binary_accuracy: 0.9869 Epoch 62/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0620 - binary_accuracy: 0.9888 - val_loss: 0.0647 - val_binary_accuracy: 0.9869 Epoch 63/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0607 - binary_accuracy: 0.9893 - val_loss: 0.0635 - val_binary_accuracy: 0.9869 Epoch 64/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0594 - binary_accuracy: 0.9896 - val_loss: 0.0623 - val_binary_accuracy: 0.9877 Epoch 65/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0582 - binary_accuracy: 0.9893 - val_loss: 0.0612 - val_binary_accuracy: 0.9877 Epoch 66/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0571 - binary_accuracy: 0.9896 - val_loss: 0.0601 - val_binary_accuracy: 0.9877 Epoch 67/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0559 - binary_accuracy: 0.9896 - val_loss: 0.0591 - val_binary_accuracy: 0.9877 Epoch 68/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0549 - binary_accuracy: 0.9896 - val_loss: 0.0581 - val_binary_accuracy: 0.9877 Epoch 69/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0539 - binary_accuracy: 0.9896 - val_loss: 0.0572 - val_binary_accuracy: 0.9877 Epoch 70/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - binary_accuracy: 0.9896 - val_loss: 0.0563 - val_binary_accuracy: 0.9885 Epoch 71/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0521 - binary_accuracy: 0.9899 - val_loss: 0.0554 - val_binary_accuracy: 0.9885 Epoch 72/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0512 - binary_accuracy: 0.9901 - val_loss: 0.0546 - val_binary_accuracy: 0.9885 Epoch 73/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0505 - binary_accuracy: 0.9901 - val_loss: 0.0539 - val_binary_accuracy: 0.9885 Epoch 74/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0498 - binary_accuracy: 0.9901 - val_loss: 0.0532 - val_binary_accuracy: 0.9885 Epoch 75/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0492 - binary_accuracy: 0.9901 - val_loss: 0.0525 - val_binary_accuracy: 0.9885 Epoch 76/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0485 - binary_accuracy: 0.9901 - val_loss: 0.0519 - val_binary_accuracy: 0.9885 Epoch 77/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0479 - binary_accuracy: 0.9901 - val_loss: 0.0514 - val_binary_accuracy: 0.9885 Epoch 78/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0473 - binary_accuracy: 0.9901 - val_loss: 0.0509 - val_binary_accuracy: 0.9885 Epoch 79/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0468 - binary_accuracy: 0.9901 - val_loss: 0.0504 - val_binary_accuracy: 0.9885 Epoch 80/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - binary_accuracy: 0.9901 - val_loss: 0.0499 - val_binary_accuracy: 0.9885 Epoch 81/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0458 - binary_accuracy: 0.9901 - val_loss: 0.0494 - val_binary_accuracy: 0.9893 Epoch 82/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0454 - binary_accuracy: 0.9901 - val_loss: 0.0490 - val_binary_accuracy: 0.9893 Epoch 83/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0450 - binary_accuracy: 0.9901 - val_loss: 0.0486 - val_binary_accuracy: 0.9901 Epoch 84/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0446 - binary_accuracy: 0.9901 - val_loss: 0.0481 - val_binary_accuracy: 0.9901 Epoch 85/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0442 - binary_accuracy: 0.9904 - val_loss: 0.0478 - val_binary_accuracy: 0.9901 Epoch 86/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0438 - binary_accuracy: 0.9904 - val_loss: 0.0475 - val_binary_accuracy: 0.9901 Epoch 87/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0434 - binary_accuracy: 0.9904 - val_loss: 0.0472 - val_binary_accuracy: 0.9901 Epoch 88/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0431 - binary_accuracy: 0.9904 - val_loss: 0.0469 - val_binary_accuracy: 0.9901 Epoch 89/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0428 - binary_accuracy: 0.9904 - val_loss: 0.0465 - val_binary_accuracy: 0.9910 Epoch 90/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0425 - binary_accuracy: 0.9904 - val_loss: 0.0463 - val_binary_accuracy: 0.9910 Epoch 91/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0422 - binary_accuracy: 0.9904 - val_loss: 0.0460 - val_binary_accuracy: 0.9910 Epoch 92/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0419 - binary_accuracy: 0.9907 - val_loss: 0.0457 - val_binary_accuracy: 0.9910 Epoch 93/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0416 - binary_accuracy: 0.9907 - val_loss: 0.0455 - val_binary_accuracy: 0.9910 Epoch 94/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0413 - binary_accuracy: 0.9907 - val_loss: 0.0452 - val_binary_accuracy: 0.9910 Epoch 95/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0410 - binary_accuracy: 0.9907 - val_loss: 0.0450 - val_binary_accuracy: 0.9910 Epoch 96/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0408 - binary_accuracy: 0.9907 - val_loss: 0.0448 - val_binary_accuracy: 0.9910 Epoch 97/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0405 - binary_accuracy: 0.9907 - val_loss: 0.0446 - val_binary_accuracy: 0.9910 Epoch 98/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0403 - binary_accuracy: 0.9907 - val_loss: 0.0444 - val_binary_accuracy: 0.9910 Epoch 99/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0400 - binary_accuracy: 0.9912 - val_loss: 0.0442 - val_binary_accuracy: 0.9910 Epoch 100/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0398 - binary_accuracy: 0.9915 - val_loss: 0.0439 - val_binary_accuracy: 0.9910 Epoch 101/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0396 - binary_accuracy: 0.9915 - val_loss: 0.0437 - val_binary_accuracy: 0.9910 Epoch 102/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0394 - binary_accuracy: 0.9915 - val_loss: 0.0436 - val_binary_accuracy: 0.9910 Epoch 103/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0391 - binary_accuracy: 0.9915 - val_loss: 0.0434 - val_binary_accuracy: 0.9910 Epoch 104/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0390 - binary_accuracy: 0.9915 - val_loss: 0.0432 - val_binary_accuracy: 0.9910 Epoch 105/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0388 - binary_accuracy: 0.9915 - val_loss: 0.0430 - val_binary_accuracy: 0.9910 Epoch 106/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0385 - binary_accuracy: 0.9918 - val_loss: 0.0428 - val_binary_accuracy: 0.9910 Epoch 107/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0383 - binary_accuracy: 0.9918 - val_loss: 0.0427 - val_binary_accuracy: 0.9918 Epoch 108/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0381 - binary_accuracy: 0.9918 - val_loss: 0.0425 - val_binary_accuracy: 0.9918 Epoch 109/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0380 - binary_accuracy: 0.9918 - val_loss: 0.0423 - val_binary_accuracy: 0.9918 Epoch 110/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0378 - binary_accuracy: 0.9921 - val_loss: 0.0421 - val_binary_accuracy: 0.9918 Epoch 111/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0376 - binary_accuracy: 0.9921 - val_loss: 0.0420 - val_binary_accuracy: 0.9918 Epoch 112/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0374 - binary_accuracy: 0.9921 - val_loss: 0.0419 - val_binary_accuracy: 0.9918 Epoch 113/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0372 - binary_accuracy: 0.9918 - val_loss: 0.0417 - val_binary_accuracy: 0.9918 Epoch 114/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0371 - binary_accuracy: 0.9918 - val_loss: 0.0415 - val_binary_accuracy: 0.9918 Epoch 115/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0369 - binary_accuracy: 0.9921 - val_loss: 0.0414 - val_binary_accuracy: 0.9918 Epoch 116/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0368 - binary_accuracy: 0.9921 - val_loss: 0.0413 - val_binary_accuracy: 0.9918 Epoch 117/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0366 - binary_accuracy: 0.9921 - val_loss: 0.0411 - val_binary_accuracy: 0.9926 Epoch 118/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0365 - binary_accuracy: 0.9921 - val_loss: 0.0410 - val_binary_accuracy: 0.9918 Epoch 119/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0364 - binary_accuracy: 0.9923 - val_loss: 0.0409 - val_binary_accuracy: 0.9918 Epoch 120/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - binary_accuracy: 0.9923 - val_loss: 0.0408 - val_binary_accuracy: 0.9918 Epoch 121/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0361 - binary_accuracy: 0.9923 - val_loss: 0.0406 - val_binary_accuracy: 0.9918 Epoch 122/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0360 - binary_accuracy: 0.9923 - val_loss: 0.0405 - val_binary_accuracy: 0.9918 Epoch 123/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0359 - binary_accuracy: 0.9923 - val_loss: 0.0403 - val_binary_accuracy: 0.9918 Epoch 124/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0357 - binary_accuracy: 0.9923 - val_loss: 0.0402 - val_binary_accuracy: 0.9918 Epoch 125/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0356 - binary_accuracy: 0.9923 - val_loss: 0.0401 - val_binary_accuracy: 0.9918 Epoch 126/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0355 - binary_accuracy: 0.9923 - val_loss: 0.0399 - val_binary_accuracy: 0.9926 Epoch 127/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0354 - binary_accuracy: 0.9926 - val_loss: 0.0398 - val_binary_accuracy: 0.9926 Epoch 128/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0353 - binary_accuracy: 0.9926 - val_loss: 0.0397 - val_binary_accuracy: 0.9926 Epoch 129/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0352 - binary_accuracy: 0.9926 - val_loss: 0.0395 - val_binary_accuracy: 0.9926 Epoch 130/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0351 - binary_accuracy: 0.9926 - val_loss: 0.0394 - val_binary_accuracy: 0.9926 Epoch 131/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0350 - binary_accuracy: 0.9926 - val_loss: 0.0393 - val_binary_accuracy: 0.9918 Epoch 132/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0349 - binary_accuracy: 0.9926 - val_loss: 0.0392 - val_binary_accuracy: 0.9918 Epoch 133/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0348 - binary_accuracy: 0.9926 - val_loss: 0.0392 - val_binary_accuracy: 0.9918 Epoch 134/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0347 - binary_accuracy: 0.9929 - val_loss: 0.0391 - val_binary_accuracy: 0.9918 Epoch 135/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0345 - binary_accuracy: 0.9926 - val_loss: 0.0390 - val_binary_accuracy: 0.9918 Epoch 136/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0344 - binary_accuracy: 0.9929 - val_loss: 0.0389 - val_binary_accuracy: 0.9918 Epoch 137/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0344 - binary_accuracy: 0.9929 - val_loss: 0.0388 - val_binary_accuracy: 0.9918 Epoch 138/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0343 - binary_accuracy: 0.9932 - val_loss: 0.0387 - val_binary_accuracy: 0.9926 Epoch 139/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0342 - binary_accuracy: 0.9932 - val_loss: 0.0386 - val_binary_accuracy: 0.9918 Epoch 140/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0341 - binary_accuracy: 0.9932 - val_loss: 0.0385 - val_binary_accuracy: 0.9918 Epoch 141/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0339 - binary_accuracy: 0.9929 - val_loss: 0.0384 - val_binary_accuracy: 0.9918 Epoch 142/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0338 - binary_accuracy: 0.9929 - val_loss: 0.0383 - val_binary_accuracy: 0.9918 Epoch 143/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0337 - binary_accuracy: 0.9929 - val_loss: 0.0382 - val_binary_accuracy: 0.9918 Epoch 144/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0336 - binary_accuracy: 0.9929 - val_loss: 0.0382 - val_binary_accuracy: 0.9918 history_df = pd . DataFrame ( history . history ) history_df . loc [:, [ 'loss' , 'val_loss' ]] . plot () history_df . loc [:, [ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot () print (( \"Best Validation Loss: {:0.4f} \" + \\ \" \\n Best Validation Accuracy: {:0.4f} \" ) \\ . format ( history_df [ 'val_loss' ] . min (), history_df [ 'val_binary_accuracy' ] . max ())) Best Validation Loss: 0.0382 Best Validation Accuracy: 0.9926 # predict test set pred_probability = model . predict ( X_test_std ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score print ( classification_report ( y_test_std , predictions )) precision recall f1-score support 0 1.00 1.00 1.00 1231 1 0.99 0.99 0.99 394 accuracy 1.00 1625 macro avg 0.99 1.00 0.99 1625 weighted avg 1.00 1.00 1.00 1625 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( y_test_std , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f4c3c439c90> 2.4 Multi-Class Classification \u00b6 back to top It is good practice, and often necessary to one-hot encode the target values for a multi-class problem. We will need to do that with our wine data here. \ud83c\udfcb\ufe0f Exercise 3: Multi-Class Classification \u00b6 1) Define Model Define a model with both batch normalization and dropout layers. 2) Add Optimizer, Loss, and Metric 3) Train and Evaluate from keras.utils import np_utils wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # preprocess y for quality and/or type encoder = LabelEncoder () encoder . fit ( y ) encoded_y = encoder . transform ( y ) y = np_utils . to_categorical ( encoded_y ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = y_train y_val_std = y_val y_test_std = y_test preprocessor #sk-babe4437-2322-43ce-8b3c-05d68e06278f {color: black;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f pre{padding: 0;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable {background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator:hover {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-item {z-index: 1;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:only-child::after {width: 0;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ ### YOUR MODEL HERE ### ]) print ( model . summary ()) Model: \"sequential_8\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_33 (Dense) (None, 4) 56 _________________________________________________________________ dense_34 (Dense) (None, 4) 20 _________________________________________________________________ dense_35 (Dense) (None, 7) 35 ================================================================= Total params: 111 Trainable params: 111 Non-trainable params: 0 _________________________________________________________________ None model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ], ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 512 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 1 , # hide the output because we have so many epochs ) Epoch 1/1000 8/8 [==============================] - 1s 25ms/step - loss: 1.9526 - accuracy: 0.1089 - val_loss: 1.9443 - val_accuracy: 0.1667 Epoch 2/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.9330 - accuracy: 0.1656 - val_loss: 1.9251 - val_accuracy: 0.2192 Epoch 3/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.9144 - accuracy: 0.2288 - val_loss: 1.9066 - val_accuracy: 0.3005 Epoch 4/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.8962 - accuracy: 0.3049 - val_loss: 1.8879 - val_accuracy: 0.3259 Epoch 5/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8778 - accuracy: 0.3309 - val_loss: 1.8686 - val_accuracy: 0.3563 Epoch 6/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8586 - accuracy: 0.3629 - val_loss: 1.8485 - val_accuracy: 0.3834 Epoch 7/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.8387 - accuracy: 0.3829 - val_loss: 1.8270 - val_accuracy: 0.4015 Epoch 8/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8174 - accuracy: 0.3960 - val_loss: 1.8045 - val_accuracy: 0.4097 Epoch 9/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7950 - accuracy: 0.4056 - val_loss: 1.7808 - val_accuracy: 0.4146 Epoch 10/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7713 - accuracy: 0.4146 - val_loss: 1.7556 - val_accuracy: 0.4220 Epoch 11/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7465 - accuracy: 0.4187 - val_loss: 1.7291 - val_accuracy: 0.4220 Epoch 12/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7204 - accuracy: 0.4220 - val_loss: 1.7017 - val_accuracy: 0.4245 Epoch 13/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6936 - accuracy: 0.4245 - val_loss: 1.6735 - val_accuracy: 0.4245 Epoch 14/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6663 - accuracy: 0.4261 - val_loss: 1.6452 - val_accuracy: 0.4294 Epoch 15/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6390 - accuracy: 0.4302 - val_loss: 1.6171 - val_accuracy: 0.4261 Epoch 16/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6114 - accuracy: 0.4335 - val_loss: 1.5890 - val_accuracy: 0.4310 Epoch 17/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5836 - accuracy: 0.4387 - val_loss: 1.5613 - val_accuracy: 0.4286 Epoch 18/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5562 - accuracy: 0.4417 - val_loss: 1.5347 - val_accuracy: 0.4319 Epoch 19/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5300 - accuracy: 0.4469 - val_loss: 1.5096 - val_accuracy: 0.4376 Epoch 20/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5058 - accuracy: 0.4518 - val_loss: 1.4865 - val_accuracy: 0.4433 Epoch 21/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.4831 - accuracy: 0.4559 - val_loss: 1.4651 - val_accuracy: 0.4442 Epoch 22/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.4626 - accuracy: 0.4570 - val_loss: 1.4460 - val_accuracy: 0.4466 Epoch 23/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4440 - accuracy: 0.4576 - val_loss: 1.4287 - val_accuracy: 0.4507 Epoch 24/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4274 - accuracy: 0.4592 - val_loss: 1.4126 - val_accuracy: 0.4507 Epoch 25/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4120 - accuracy: 0.4587 - val_loss: 1.3978 - val_accuracy: 0.4548 Epoch 26/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3984 - accuracy: 0.4589 - val_loss: 1.3841 - val_accuracy: 0.4548 Epoch 27/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3859 - accuracy: 0.4617 - val_loss: 1.3713 - val_accuracy: 0.4548 Epoch 28/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3744 - accuracy: 0.4603 - val_loss: 1.3598 - val_accuracy: 0.4532 Epoch 29/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3640 - accuracy: 0.4625 - val_loss: 1.3494 - val_accuracy: 0.4565 Epoch 30/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3545 - accuracy: 0.4636 - val_loss: 1.3401 - val_accuracy: 0.4581 Epoch 31/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3459 - accuracy: 0.4641 - val_loss: 1.3317 - val_accuracy: 0.4581 Epoch 32/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3380 - accuracy: 0.4641 - val_loss: 1.3236 - val_accuracy: 0.4573 Epoch 33/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3310 - accuracy: 0.4636 - val_loss: 1.3165 - val_accuracy: 0.4565 Epoch 34/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3244 - accuracy: 0.4644 - val_loss: 1.3100 - val_accuracy: 0.4589 Epoch 35/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3182 - accuracy: 0.4652 - val_loss: 1.3039 - val_accuracy: 0.4573 Epoch 36/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3126 - accuracy: 0.4631 - val_loss: 1.2982 - val_accuracy: 0.4565 Epoch 37/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3071 - accuracy: 0.4628 - val_loss: 1.2929 - val_accuracy: 0.4622 Epoch 38/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3022 - accuracy: 0.4628 - val_loss: 1.2881 - val_accuracy: 0.4639 Epoch 39/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2976 - accuracy: 0.4639 - val_loss: 1.2838 - val_accuracy: 0.4647 Epoch 40/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2937 - accuracy: 0.4636 - val_loss: 1.2797 - val_accuracy: 0.4639 Epoch 41/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2897 - accuracy: 0.4639 - val_loss: 1.2758 - val_accuracy: 0.4639 Epoch 42/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2862 - accuracy: 0.4631 - val_loss: 1.2722 - val_accuracy: 0.4631 Epoch 43/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2830 - accuracy: 0.4639 - val_loss: 1.2686 - val_accuracy: 0.4614 Epoch 44/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2798 - accuracy: 0.4636 - val_loss: 1.2658 - val_accuracy: 0.4614 Epoch 45/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.2768 - accuracy: 0.4644 - val_loss: 1.2631 - val_accuracy: 0.4639 Epoch 46/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2740 - accuracy: 0.4661 - val_loss: 1.2608 - val_accuracy: 0.4647 Epoch 47/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2712 - accuracy: 0.4658 - val_loss: 1.2582 - val_accuracy: 0.4647 Epoch 48/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2686 - accuracy: 0.4661 - val_loss: 1.2559 - val_accuracy: 0.4672 Epoch 49/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2657 - accuracy: 0.4680 - val_loss: 1.2529 - val_accuracy: 0.4713 Epoch 50/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2631 - accuracy: 0.4669 - val_loss: 1.2500 - val_accuracy: 0.4713 Epoch 51/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2606 - accuracy: 0.4688 - val_loss: 1.2470 - val_accuracy: 0.4778 Epoch 52/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2580 - accuracy: 0.4735 - val_loss: 1.2438 - val_accuracy: 0.4811 Epoch 53/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2556 - accuracy: 0.4726 - val_loss: 1.2408 - val_accuracy: 0.4860 Epoch 54/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2530 - accuracy: 0.4751 - val_loss: 1.2377 - val_accuracy: 0.4852 Epoch 55/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2502 - accuracy: 0.4770 - val_loss: 1.2358 - val_accuracy: 0.4901 Epoch 56/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2477 - accuracy: 0.4781 - val_loss: 1.2343 - val_accuracy: 0.4885 Epoch 57/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2451 - accuracy: 0.4787 - val_loss: 1.2317 - val_accuracy: 0.4926 Epoch 58/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2422 - accuracy: 0.4825 - val_loss: 1.2286 - val_accuracy: 0.4959 Epoch 59/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2396 - accuracy: 0.4828 - val_loss: 1.2265 - val_accuracy: 0.4992 Epoch 60/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2371 - accuracy: 0.4839 - val_loss: 1.2242 - val_accuracy: 0.5000 Epoch 61/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2342 - accuracy: 0.4839 - val_loss: 1.2221 - val_accuracy: 0.4967 Epoch 62/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2313 - accuracy: 0.4877 - val_loss: 1.2200 - val_accuracy: 0.4967 Epoch 63/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2287 - accuracy: 0.4893 - val_loss: 1.2178 - val_accuracy: 0.5025 Epoch 64/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2262 - accuracy: 0.4891 - val_loss: 1.2158 - val_accuracy: 0.5016 Epoch 65/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2237 - accuracy: 0.4915 - val_loss: 1.2135 - val_accuracy: 0.5016 Epoch 66/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.2213 - accuracy: 0.4918 - val_loss: 1.2119 - val_accuracy: 0.5025 Epoch 67/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2189 - accuracy: 0.4926 - val_loss: 1.2100 - val_accuracy: 0.5000 Epoch 68/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2166 - accuracy: 0.4951 - val_loss: 1.2072 - val_accuracy: 0.5074 Epoch 69/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2137 - accuracy: 0.4973 - val_loss: 1.2052 - val_accuracy: 0.5025 Epoch 70/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.2114 - accuracy: 0.4995 - val_loss: 1.2028 - val_accuracy: 0.5025 Epoch 71/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2088 - accuracy: 0.5003 - val_loss: 1.1996 - val_accuracy: 0.5041 Epoch 72/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2062 - accuracy: 0.5019 - val_loss: 1.1964 - val_accuracy: 0.5066 Epoch 73/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2036 - accuracy: 0.5027 - val_loss: 1.1936 - val_accuracy: 0.5049 Epoch 74/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2013 - accuracy: 0.5055 - val_loss: 1.1917 - val_accuracy: 0.5082 Epoch 75/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1988 - accuracy: 0.5041 - val_loss: 1.1900 - val_accuracy: 0.5057 Epoch 76/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1959 - accuracy: 0.5038 - val_loss: 1.1879 - val_accuracy: 0.5099 Epoch 77/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1934 - accuracy: 0.5049 - val_loss: 1.1854 - val_accuracy: 0.5131 Epoch 78/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1908 - accuracy: 0.5077 - val_loss: 1.1830 - val_accuracy: 0.5107 Epoch 79/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1879 - accuracy: 0.5071 - val_loss: 1.1810 - val_accuracy: 0.5074 Epoch 80/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1852 - accuracy: 0.5060 - val_loss: 1.1786 - val_accuracy: 0.5074 Epoch 81/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1824 - accuracy: 0.5082 - val_loss: 1.1767 - val_accuracy: 0.5090 Epoch 82/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1798 - accuracy: 0.5093 - val_loss: 1.1743 - val_accuracy: 0.5066 Epoch 83/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1774 - accuracy: 0.5093 - val_loss: 1.1719 - val_accuracy: 0.5041 Epoch 84/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1749 - accuracy: 0.5088 - val_loss: 1.1693 - val_accuracy: 0.5066 Epoch 85/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1721 - accuracy: 0.5099 - val_loss: 1.1665 - val_accuracy: 0.5140 Epoch 86/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1696 - accuracy: 0.5112 - val_loss: 1.1640 - val_accuracy: 0.5189 Epoch 87/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1674 - accuracy: 0.5129 - val_loss: 1.1616 - val_accuracy: 0.5197 Epoch 88/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1649 - accuracy: 0.5126 - val_loss: 1.1597 - val_accuracy: 0.5172 Epoch 89/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1625 - accuracy: 0.5142 - val_loss: 1.1580 - val_accuracy: 0.5172 Epoch 90/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1601 - accuracy: 0.5142 - val_loss: 1.1562 - val_accuracy: 0.5148 Epoch 91/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1579 - accuracy: 0.5153 - val_loss: 1.1543 - val_accuracy: 0.5156 Epoch 92/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1557 - accuracy: 0.5142 - val_loss: 1.1526 - val_accuracy: 0.5181 Epoch 93/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1534 - accuracy: 0.5140 - val_loss: 1.1515 - val_accuracy: 0.5172 Epoch 94/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.1514 - accuracy: 0.5151 - val_loss: 1.1505 - val_accuracy: 0.5156 Epoch 95/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1494 - accuracy: 0.5151 - val_loss: 1.1489 - val_accuracy: 0.5115 Epoch 96/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1475 - accuracy: 0.5170 - val_loss: 1.1475 - val_accuracy: 0.5172 Epoch 97/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1453 - accuracy: 0.5189 - val_loss: 1.1452 - val_accuracy: 0.5148 Epoch 98/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1430 - accuracy: 0.5192 - val_loss: 1.1430 - val_accuracy: 0.5148 Epoch 99/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1410 - accuracy: 0.5186 - val_loss: 1.1409 - val_accuracy: 0.5148 Epoch 100/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1391 - accuracy: 0.5208 - val_loss: 1.1392 - val_accuracy: 0.5164 Epoch 101/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1374 - accuracy: 0.5205 - val_loss: 1.1380 - val_accuracy: 0.5197 Epoch 102/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1357 - accuracy: 0.5235 - val_loss: 1.1369 - val_accuracy: 0.5156 Epoch 103/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1338 - accuracy: 0.5244 - val_loss: 1.1354 - val_accuracy: 0.5156 Epoch 104/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1322 - accuracy: 0.5241 - val_loss: 1.1349 - val_accuracy: 0.5172 Epoch 105/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1303 - accuracy: 0.5227 - val_loss: 1.1331 - val_accuracy: 0.5181 Epoch 106/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1285 - accuracy: 0.5257 - val_loss: 1.1311 - val_accuracy: 0.5189 Epoch 107/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1268 - accuracy: 0.5276 - val_loss: 1.1298 - val_accuracy: 0.5181 Epoch 108/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1249 - accuracy: 0.5315 - val_loss: 1.1287 - val_accuracy: 0.5238 Epoch 109/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1232 - accuracy: 0.5353 - val_loss: 1.1271 - val_accuracy: 0.5271 Epoch 110/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1217 - accuracy: 0.5372 - val_loss: 1.1254 - val_accuracy: 0.5255 Epoch 111/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1201 - accuracy: 0.5397 - val_loss: 1.1235 - val_accuracy: 0.5312 Epoch 112/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1183 - accuracy: 0.5397 - val_loss: 1.1224 - val_accuracy: 0.5312 Epoch 113/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1167 - accuracy: 0.5416 - val_loss: 1.1213 - val_accuracy: 0.5304 Epoch 114/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1152 - accuracy: 0.5449 - val_loss: 1.1202 - val_accuracy: 0.5312 Epoch 115/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1136 - accuracy: 0.5457 - val_loss: 1.1189 - val_accuracy: 0.5328 Epoch 116/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1117 - accuracy: 0.5468 - val_loss: 1.1179 - val_accuracy: 0.5353 Epoch 117/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1103 - accuracy: 0.5471 - val_loss: 1.1175 - val_accuracy: 0.5353 Epoch 118/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1089 - accuracy: 0.5465 - val_loss: 1.1171 - val_accuracy: 0.5345 Epoch 119/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1075 - accuracy: 0.5471 - val_loss: 1.1165 - val_accuracy: 0.5378 Epoch 120/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1062 - accuracy: 0.5487 - val_loss: 1.1153 - val_accuracy: 0.5411 Epoch 121/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1049 - accuracy: 0.5498 - val_loss: 1.1142 - val_accuracy: 0.5378 Epoch 122/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1036 - accuracy: 0.5501 - val_loss: 1.1134 - val_accuracy: 0.5312 Epoch 123/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1024 - accuracy: 0.5484 - val_loss: 1.1121 - val_accuracy: 0.5353 Epoch 124/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1011 - accuracy: 0.5501 - val_loss: 1.1115 - val_accuracy: 0.5402 Epoch 125/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1000 - accuracy: 0.5523 - val_loss: 1.1109 - val_accuracy: 0.5411 Epoch 126/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0987 - accuracy: 0.5506 - val_loss: 1.1092 - val_accuracy: 0.5378 Epoch 127/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0972 - accuracy: 0.5561 - val_loss: 1.1080 - val_accuracy: 0.5361 Epoch 128/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0958 - accuracy: 0.5534 - val_loss: 1.1080 - val_accuracy: 0.5337 Epoch 129/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0947 - accuracy: 0.5558 - val_loss: 1.1077 - val_accuracy: 0.5353 Epoch 130/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0933 - accuracy: 0.5531 - val_loss: 1.1074 - val_accuracy: 0.5369 Epoch 131/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0922 - accuracy: 0.5536 - val_loss: 1.1070 - val_accuracy: 0.5394 Epoch 132/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0911 - accuracy: 0.5523 - val_loss: 1.1059 - val_accuracy: 0.5394 Epoch 133/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0900 - accuracy: 0.5536 - val_loss: 1.1047 - val_accuracy: 0.5378 Epoch 134/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0890 - accuracy: 0.5545 - val_loss: 1.1034 - val_accuracy: 0.5411 Epoch 135/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0879 - accuracy: 0.5564 - val_loss: 1.1017 - val_accuracy: 0.5427 Epoch 136/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0869 - accuracy: 0.5545 - val_loss: 1.1010 - val_accuracy: 0.5435 Epoch 137/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0860 - accuracy: 0.5564 - val_loss: 1.1006 - val_accuracy: 0.5435 Epoch 138/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0849 - accuracy: 0.5567 - val_loss: 1.1004 - val_accuracy: 0.5443 Epoch 139/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0842 - accuracy: 0.5553 - val_loss: 1.0999 - val_accuracy: 0.5452 Epoch 140/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0831 - accuracy: 0.5558 - val_loss: 1.0988 - val_accuracy: 0.5484 Epoch 141/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0822 - accuracy: 0.5550 - val_loss: 1.0979 - val_accuracy: 0.5525 Epoch 142/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0816 - accuracy: 0.5577 - val_loss: 1.0975 - val_accuracy: 0.5493 Epoch 143/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0809 - accuracy: 0.5556 - val_loss: 1.0971 - val_accuracy: 0.5517 Epoch 144/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0800 - accuracy: 0.5545 - val_loss: 1.0970 - val_accuracy: 0.5484 Epoch 145/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0792 - accuracy: 0.5545 - val_loss: 1.0964 - val_accuracy: 0.5493 Epoch 146/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0785 - accuracy: 0.5572 - val_loss: 1.0956 - val_accuracy: 0.5517 Epoch 147/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0777 - accuracy: 0.5569 - val_loss: 1.0943 - val_accuracy: 0.5517 Epoch 148/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0771 - accuracy: 0.5586 - val_loss: 1.0937 - val_accuracy: 0.5542 Epoch 149/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0764 - accuracy: 0.5588 - val_loss: 1.0930 - val_accuracy: 0.5542 Epoch 150/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0759 - accuracy: 0.5586 - val_loss: 1.0926 - val_accuracy: 0.5599 Epoch 151/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0756 - accuracy: 0.5580 - val_loss: 1.0926 - val_accuracy: 0.5575 Epoch 152/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0750 - accuracy: 0.5577 - val_loss: 1.0926 - val_accuracy: 0.5567 Epoch 153/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0743 - accuracy: 0.5564 - val_loss: 1.0926 - val_accuracy: 0.5534 Epoch 154/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0736 - accuracy: 0.5569 - val_loss: 1.0918 - val_accuracy: 0.5525 Epoch 155/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0732 - accuracy: 0.5577 - val_loss: 1.0912 - val_accuracy: 0.5558 Epoch 156/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0727 - accuracy: 0.5567 - val_loss: 1.0906 - val_accuracy: 0.5567 Epoch 157/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0724 - accuracy: 0.5577 - val_loss: 1.0908 - val_accuracy: 0.5525 Epoch 158/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0720 - accuracy: 0.5553 - val_loss: 1.0906 - val_accuracy: 0.5534 Epoch 159/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0715 - accuracy: 0.5561 - val_loss: 1.0903 - val_accuracy: 0.5525 Epoch 160/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0710 - accuracy: 0.5556 - val_loss: 1.0897 - val_accuracy: 0.5534 Epoch 161/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0706 - accuracy: 0.5558 - val_loss: 1.0887 - val_accuracy: 0.5525 Epoch 162/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0704 - accuracy: 0.5577 - val_loss: 1.0883 - val_accuracy: 0.5542 Epoch 163/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0700 - accuracy: 0.5564 - val_loss: 1.0882 - val_accuracy: 0.5534 Epoch 164/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0697 - accuracy: 0.5553 - val_loss: 1.0880 - val_accuracy: 0.5558 Epoch 165/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0694 - accuracy: 0.5553 - val_loss: 1.0879 - val_accuracy: 0.5534 Epoch 166/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0693 - accuracy: 0.5558 - val_loss: 1.0880 - val_accuracy: 0.5550 Epoch 167/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0690 - accuracy: 0.5569 - val_loss: 1.0876 - val_accuracy: 0.5567 Epoch 168/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0685 - accuracy: 0.5550 - val_loss: 1.0871 - val_accuracy: 0.5575 Epoch 169/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0684 - accuracy: 0.5564 - val_loss: 1.0873 - val_accuracy: 0.5599 Epoch 170/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0679 - accuracy: 0.5558 - val_loss: 1.0867 - val_accuracy: 0.5567 Epoch 171/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0679 - accuracy: 0.5577 - val_loss: 1.0871 - val_accuracy: 0.5575 Epoch 172/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0673 - accuracy: 0.5575 - val_loss: 1.0865 - val_accuracy: 0.5542 Epoch 173/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0670 - accuracy: 0.5556 - val_loss: 1.0866 - val_accuracy: 0.5591 Epoch 174/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0665 - accuracy: 0.5539 - val_loss: 1.0862 - val_accuracy: 0.5583 Epoch 175/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0662 - accuracy: 0.5547 - val_loss: 1.0862 - val_accuracy: 0.5599 Epoch 176/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0659 - accuracy: 0.5556 - val_loss: 1.0857 - val_accuracy: 0.5583 Epoch 177/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0654 - accuracy: 0.5539 - val_loss: 1.0851 - val_accuracy: 0.5558 Epoch 178/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0652 - accuracy: 0.5542 - val_loss: 1.0841 - val_accuracy: 0.5575 Epoch 179/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0650 - accuracy: 0.5523 - val_loss: 1.0840 - val_accuracy: 0.5575 Epoch 180/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0647 - accuracy: 0.5534 - val_loss: 1.0834 - val_accuracy: 0.5534 Epoch 181/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0644 - accuracy: 0.5553 - val_loss: 1.0833 - val_accuracy: 0.5542 Epoch 182/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0641 - accuracy: 0.5534 - val_loss: 1.0832 - val_accuracy: 0.5542 Epoch 183/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0637 - accuracy: 0.5539 - val_loss: 1.0835 - val_accuracy: 0.5575 Epoch 184/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0636 - accuracy: 0.5550 - val_loss: 1.0839 - val_accuracy: 0.5599 Epoch 185/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0634 - accuracy: 0.5506 - val_loss: 1.0837 - val_accuracy: 0.5575 Epoch 186/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0634 - accuracy: 0.5501 - val_loss: 1.0826 - val_accuracy: 0.5517 Epoch 187/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0634 - accuracy: 0.5534 - val_loss: 1.0823 - val_accuracy: 0.5542 Epoch 188/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0631 - accuracy: 0.5517 - val_loss: 1.0820 - val_accuracy: 0.5534 Epoch 189/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0626 - accuracy: 0.5528 - val_loss: 1.0822 - val_accuracy: 0.5558 Epoch 190/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0626 - accuracy: 0.5495 - val_loss: 1.0824 - val_accuracy: 0.5575 Epoch 191/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0623 - accuracy: 0.5523 - val_loss: 1.0818 - val_accuracy: 0.5599 Epoch 192/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0621 - accuracy: 0.5542 - val_loss: 1.0817 - val_accuracy: 0.5575 Epoch 193/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0618 - accuracy: 0.5558 - val_loss: 1.0812 - val_accuracy: 0.5558 Epoch 194/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0619 - accuracy: 0.5553 - val_loss: 1.0812 - val_accuracy: 0.5575 Epoch 195/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0617 - accuracy: 0.5564 - val_loss: 1.0813 - val_accuracy: 0.5567 Epoch 196/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0614 - accuracy: 0.5547 - val_loss: 1.0810 - val_accuracy: 0.5624 Epoch 197/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0613 - accuracy: 0.5531 - val_loss: 1.0812 - val_accuracy: 0.5640 Epoch 198/1000 8/8 [==============================] - 0s 9ms/step - loss: 1.0613 - accuracy: 0.5536 - val_loss: 1.0817 - val_accuracy: 0.5649 Epoch 199/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0611 - accuracy: 0.5520 - val_loss: 1.0817 - val_accuracy: 0.5640 Epoch 200/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0610 - accuracy: 0.5509 - val_loss: 1.0815 - val_accuracy: 0.5632 Epoch 201/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0606 - accuracy: 0.5506 - val_loss: 1.0812 - val_accuracy: 0.5640 Epoch 202/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0606 - accuracy: 0.5487 - val_loss: 1.0814 - val_accuracy: 0.5599 Epoch 203/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0606 - accuracy: 0.5515 - val_loss: 1.0807 - val_accuracy: 0.5599 history_df = pd . DataFrame ( history . history ) history_df . loc [:, [ 'loss' , 'val_loss' ]] . plot () history_df . loc [:, [ 'accuracy' , 'val_accuracy' ]] . plot () print (( \"Best Validation Loss: {:0.4f} \" + \\ \" \\n Best Validation Accuracy: {:0.4f} \" ) \\ . format ( history_df [ 'val_loss' ] . min (), history_df [ 'val_accuracy' ] . max ())) Best Validation Loss: 1.0807 Best Validation Accuracy: 0.5649 # predict test set pred_probability = model . predict ( X_test_std ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score print ( classification_report ( y_test_std , predictions )) precision recall f1-score support 0 0.06 0.22 0.09 9 1 0.06 0.78 0.11 55 2 0.36 0.99 0.53 535 3 0.43 1.00 0.60 698 4 0.19 1.00 0.31 286 5 0.05 0.85 0.09 41 6 0.00 0.00 0.00 1 micro avg 0.26 0.98 0.41 1625 macro avg 0.16 0.69 0.25 1625 weighted avg 0.34 0.98 0.49 1625 samples avg 0.26 0.98 0.41 1625 /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) References \u00b6 Difference between a batch and an epoch Cross Entropy","title":"Feed Forward Neural Networks"},{"location":"S2_Feed_Forward_Neural_Networks/#general-applications-of-neural-networks-session-2-feed-forward-neural-networks","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will continue with our discussion on neural networks. Specifically, we will revisit the concept of learning curves, regularization and classification tasks, this time as they pertain to neural networks! images in this notebook borrowed from Ryan Holbrook","title":"General Applications of Neural Networks  Session 2: Feed Forward Neural Networks"},{"location":"S2_Feed_Forward_Neural_Networks/#20-preparing-environment-and-importing-data","text":"back to top ! pip uninstall scikit - learn - y ! pip install - U scikit - learn Found existing installation: scikit-learn 0.22.2.post1 Uninstalling scikit-learn-0.22.2.post1: Successfully uninstalled scikit-learn-0.22.2.post1 Collecting scikit-learn Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22.3 MB 2.2 MB/s \u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Collecting threadpoolctl>=2.0.0 Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB) Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Installing collected packages: threadpoolctl, scikit-learn Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0","title":"2.0 Preparing Environment and Importing Data"},{"location":"S2_Feed_Forward_Neural_Networks/#201-import-packages","text":"back to top from tensorflow import keras from tensorflow.keras import layers import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import tensorflow as tf from copy import copy import numpy as np sns . set () from sklearn.preprocessing import StandardScaler , OneHotEncoder , LabelEncoder from sklearn.compose import make_column_transformer , make_column_selector from sklearn.metrics import classification_report , confusion_matrix from sklearn.model_selection import train_test_split from sklearn import set_config from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer set_config ( display = 'diagram' )","title":"2.0.1 Import Packages"},{"location":"S2_Feed_Forward_Neural_Networks/#202-load-dataset","text":"back to top wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = np . log ( y_train ) # log output y y_val_std = np . log ( y_val ) # log output y y_test_std = np . log ( y_test ) # log output y preprocessor #sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c {color: black;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c pre{padding: 0;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable {background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-estimator:hover {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-item {z-index: 1;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-parallel-item:only-child::after {width: 0;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-32e8b842-4ed7-42ac-bb8d-cb9a2e776d5c div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore')","title":"2.0.2 Load Dataset"},{"location":"S2_Feed_Forward_Neural_Networks/#21-evaluating-the-model","text":"back to top In the last session, we randomly chose 10 and then 30 epochs (10 rounds of the training data) to train our neural network. Now we'll garner the language to be more systematic in our approach.","title":"2.1 Evaluating the Model"},{"location":"S2_Feed_Forward_Neural_Networks/#211-learning-curves","text":"Recall learning curves from last week, where we plot model score (accuracy/mse/r2/etc.) over model complexity (trees in a forest, degrees in a polynomial, etc.): img src We have a similar situation with neural networks, accept here, the model complexity is defined by both the number of epochs, and the capacity of the model.","title":"2.1.1 Learning Curves"},{"location":"S2_Feed_Forward_Neural_Networks/#2111-training-data-epochs","text":"A greater number of epochs allows the model to further tune itself to the training data. Remember that our model solves its weights by a solver function, most often a form of stochastic gradient descent. Because this solver is iterative, the longer we allow it run the closer it will get to finding its true weights. The caveat here is that, like we have seen before, the model is only learning the proper weights according to the training data, which we know includes noise, otherwise known as irreducible error. Our data science senses should be tingling: there's an optimum number of epochs here, and it will counter balance the trade-off between error due to bias (too few epochs) and error due to variance (too many epochs).","title":"2.1.1.1 Training Data (Epochs)"},{"location":"S2_Feed_Forward_Neural_Networks/#2112-complexity-capacity","text":"The capacity of the model is defined by the architecture. It is the total number of trainable weights available to the solver function. The more weights, the more capacity. Capacity determines the upper limit for which our model can learn relationships between the data. Again we should recall from our session on feature engineering: the more training data we have, the more capacity we should give our model to account for that abundance of training data. This is also influenced by the actual complexity between the input and output data, X and y, who's function we are attempting to approximate with the neural network. The more complicated the relationship, the more capacity we should give to our model. Capacity can be increased either by widening our model (increasing the neurons in a layer) or deepening our model(increasing the number of layers in our model).","title":"2.1.1.2 Complexity (Capacity)"},{"location":"S2_Feed_Forward_Neural_Networks/#212-early-stopping","text":"back to top Without knowing the true relationship between X and y, or the degree to which there is irreducible error in our data, we return to our familiar learning curves to pragmaticaly determine how long we should train our model, that is, how many epochs should be ran and how many neurons we should give our model. When dealing with the number of epochs, we can program this into the training session automatically with early stopping. Early stopping allows us to discontinue training the model when either the validation score stops improving, or stops improving by some margin. This allows us to both save time during training and to avoid overfitting our model. To account for underfitting (not training the model long enough) we can simply set our number of training epochs to some large number and allow early stopping to take care of the rest. In TF/Keras, we can envoke early stopping by setting a callback a callback is simply a function that is called every so often. from tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) In the above, we are saying detect when the validation loss stops improving by 0.001; check this over the past 20 epochs to avoid stopping early due to noise; and restore the best weights over that past 20 epoch period when early stopping is envoked. To envoke early stopping we would enlist it in our call to fit like so: model.fit( # X, y, batch size etc, ... callbacks=[early_stopping], )","title":"2.1.2 Early Stopping"},{"location":"S2_Feed_Forward_Neural_Networks/#exercise-1-try-early-stopping","text":"Take your best model from the last exercise in session 1 and apply early stopping from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import EarlyStopping model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_5 (Dense) (None, 32) 448 _________________________________________________________________ dense_6 (Dense) (None, 1024) 33792 _________________________________________________________________ dense_7 (Dense) (None, 1024) 1049600 _________________________________________________________________ dense_8 (Dense) (None, 32) 32800 _________________________________________________________________ dense_9 (Dense) (None, 1) 33 ================================================================= Total params: 1,116,673 Trainable params: 1,116,673 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 2s 50ms/step - loss: 0.4680 - val_loss: 0.1534 Epoch 2/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.1286 - val_loss: 0.0802 Epoch 3/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0770 - val_loss: 0.0502 Epoch 4/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0511 - val_loss: 0.0378 Epoch 5/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0362 - val_loss: 0.0321 Epoch 6/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0298 - val_loss: 0.0272 Epoch 7/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0268 - val_loss: 0.0288 Epoch 8/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0223 - val_loss: 0.0206 Epoch 9/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0190 - val_loss: 0.0204 Epoch 10/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0177 - val_loss: 0.0218 Epoch 11/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0166 - val_loss: 0.0185 Epoch 12/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0152 - val_loss: 0.0207 Epoch 13/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0153 - val_loss: 0.0197 Epoch 14/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0147 - val_loss: 0.0204 Epoch 15/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0143 - val_loss: 0.0178 Epoch 16/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0133 - val_loss: 0.0165 Epoch 17/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0124 - val_loss: 0.0168 Epoch 18/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0129 - val_loss: 0.0166 Epoch 19/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0128 - val_loss: 0.0159 Epoch 20/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0119 - val_loss: 0.0159 Epoch 21/1000 15/15 [==============================] - 1s 40ms/step - loss: 0.0115 - val_loss: 0.0159 Epoch 22/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0112 - val_loss: 0.0158 Epoch 23/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0113 - val_loss: 0.0161 Epoch 24/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0113 - val_loss: 0.0173 Epoch 25/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0105 - val_loss: 0.0160 Epoch 26/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0104 - val_loss: 0.0172 Epoch 27/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0107 - val_loss: 0.0160 Epoch 28/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0099 - val_loss: 0.0159 Epoch 29/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0096 - val_loss: 0.0153 Epoch 30/1000 15/15 [==============================] - 1s 51ms/step - loss: 0.0097 - val_loss: 0.0176 Epoch 31/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0103 - val_loss: 0.0180 Epoch 32/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0102 - val_loss: 0.0161 Epoch 33/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0095 - val_loss: 0.0173 Epoch 34/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0101 - val_loss: 0.0171 Epoch 35/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0102 - val_loss: 0.0176 Epoch 36/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0092 - val_loss: 0.0160 Epoch 37/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0090 - val_loss: 0.0158 Epoch 38/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0088 - val_loss: 0.0200 Epoch 39/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0110 - val_loss: 0.0189 Epoch 40/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0096 - val_loss: 0.0172 Epoch 41/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0093 - val_loss: 0.0151 Epoch 42/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0082 - val_loss: 0.0153 Epoch 43/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0075 - val_loss: 0.0167 Epoch 44/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0080 - val_loss: 0.0157 Epoch 45/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0079 - val_loss: 0.0152 Epoch 46/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0077 - val_loss: 0.0173 Epoch 47/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0088 - val_loss: 0.0161 Epoch 48/1000 15/15 [==============================] - 1s 41ms/step - loss: 0.0081 - val_loss: 0.0166 Epoch 49/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0080 - val_loss: 0.0195 # Show the learning curves history_df = pd . DataFrame ( history . history ) fig , ax = plt . subplots ( 1 , 1 ) history_df . plot ( ax = ax ) ax . set_ylim ( 0 , .1 ) (0.0, 0.1) y_test_pred = np . exp ( model . predict ( X_test_std )) plt . plot ( y_test_pred , np . exp ( y_test_std ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f4c409a6210>]","title":"\ud83c\udfcb\ufe0f Exercise 1: Try Early Stopping"},{"location":"S2_Feed_Forward_Neural_Networks/#22-regularizing-layers-dropout-and-batch-normalization","text":"back to top There are dozens of different layer types for accomplishing different tasks (and more are being generated by researchers all the time). While we're on the topic of general model performance, there are two layer types we'll want to introduce here: dropout and batch normalization or just batchnorm. What we'll see with both of these types is that while they don't contain any neurons, they're useful to think of as layers, because they are an extra processing step between neural layers.","title":"2.2 Regularizing Layers: Dropout and Batch Normalization"},{"location":"S2_Feed_Forward_Neural_Networks/#221-dropout","text":"back to top Dropout is the Neural Network response to the wide success of ensemble learning. In a dropout layer, random neurons are dropped in each batch of training, i.e. their weighted updates are not sent to the next neural layer. Just as with random forests, the end result is that the neural network can be thought of as many independent models that vote on the final output. Put another way, when a network does not contain dropout layers, and has a capacity that exceeds that which would be suited for the true, underlying complexity level of the data, it can begin to fit to noise. This ability to fit to noise is based on very specific relationships between neurons, which fire uniquely given the particular training example. Adding dropout breaks these specific neural connections, and so the neural network as a whole is forced to find weights that apply generally, as there is no guarantee they will be turned on when their specific training example they would usually overfit for comes around again. Network with 50% dropout A last thing to note, is that after adding dropout, we will typically need to add additional layers to our network to maintain the overall capacity of the network. keras . Sequential ([ # ... layers . Dropout ( rate = 0.3 ), # apply 30% dropout to the next layer layers . Dense ( 16 ), # ... ]) <tensorflow.python.keras.engine.sequential.Sequential at 0x7f2ae011ba50> When adding dropout to a model we will usually want to increase the depth: # Dropout model model = keras . Sequential ([ layers . Dense ( 512 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . Dropout ( 0.3 ), layers . Dense ( 512 * 2 , activation = 'relu' ), layers . Dropout ( 0.3 ), layers . Dense ( 512 , activation = 'relu' ), layers . Dense ( 1 ), ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 512) 7168 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense_11 (Dense) (None, 1024) 525312 _________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________ dense_12 (Dense) (None, 512) 524800 _________________________________________________________________ dense_13 (Dense) (None, 1) 513 ================================================================= Total params: 1,057,793 Trainable params: 1,057,793 Non-trainable params: 0 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 1s 51ms/step - loss: 0.6459 - val_loss: 0.1513 Epoch 2/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1613 - val_loss: 0.1027 Epoch 3/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.1056 - val_loss: 0.0918 Epoch 4/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0730 - val_loss: 0.0689 Epoch 5/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0566 - val_loss: 0.0742 Epoch 6/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0466 - val_loss: 0.0360 Epoch 7/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0409 - val_loss: 0.0673 Epoch 8/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0396 - val_loss: 0.0652 Epoch 9/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0339 - val_loss: 0.0702 Epoch 10/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0337 - val_loss: 0.0623 Epoch 11/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0329 - val_loss: 0.0561 Epoch 12/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0307 - val_loss: 0.0721 Epoch 13/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0294 - val_loss: 0.0828 Epoch 14/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0273 - val_loss: 0.0763 Epoch 15/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0267 - val_loss: 0.0672 Epoch 16/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0258 - val_loss: 0.0740 Epoch 17/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0249 - val_loss: 0.0462 Epoch 18/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0263 - val_loss: 0.0770 Epoch 19/1000 15/15 [==============================] - 1s 42ms/step - loss: 0.0246 - val_loss: 0.0679 Epoch 20/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0238 - val_loss: 0.0984 Epoch 21/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0246 - val_loss: 0.0536 Epoch 22/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0237 - val_loss: 0.0837 Epoch 23/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0214 - val_loss: 0.0754 Epoch 24/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0216 - val_loss: 0.0664 Epoch 25/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0222 - val_loss: 0.0631 Epoch 26/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0215 - val_loss: 0.0858","title":"2.2.1 Dropout"},{"location":"S2_Feed_Forward_Neural_Networks/#222-batch-normalization","text":"back to top Batch normalization ( batchnorm ) accomplishes a task very similar to sklearn's StandardScaler , and in fact, it can be added to the front of a neural network in place of sklearn's StandardScaler to scale and center the input data. But batchnorm is also used between layers, why would we want to rescale the output from one layer before feeding into the next? For the same reason we would want to scale and center our data in the first place: it levels the playing field for the following layer to find the important relationships. The optimizer algorithm, SGD, will shift weights in proportion to how large the activation is out of each neuron, and so large differences can lead to spurious behavior. In practice, batchnorm is often implemented to make the overall training time faster, rather than to improve the final loss (although in some cases it does do this as well). It is good to envoke batchnorm in particular, if you are dealing with neural networks that take a long time to train. Batchnorm can be added between layers, as well as between a layer and its activation function. # BatchNorm model model = keras . Sequential ([ layers . Dense ( 512 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . BatchNormalization (), layers . Dense ( 512 * 2 , activation = 'relu' ), layers . BatchNormalization (), layers . Dense ( 512 , activation = 'relu' ), layers . Dense ( 1 ), ]) model . compile ( optimizer = 'adam' , loss = 'mse' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) print ( model . summary ()) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 1000 , verbose = 1 , callbacks = [ early_stopping ], ) Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_14 (Dense) (None, 512) 7168 _________________________________________________________________ batch_normalization (BatchNo (None, 512) 2048 _________________________________________________________________ dense_15 (Dense) (None, 1024) 525312 _________________________________________________________________ batch_normalization_1 (Batch (None, 1024) 4096 _________________________________________________________________ dense_16 (Dense) (None, 512) 524800 _________________________________________________________________ dense_17 (Dense) (None, 1) 513 ================================================================= Total params: 1,063,937 Trainable params: 1,060,865 Non-trainable params: 3,072 _________________________________________________________________ None Epoch 1/1000 15/15 [==============================] - 2s 54ms/step - loss: 2.7890 - val_loss: 2.5754 Epoch 2/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.7404 - val_loss: 2.2122 Epoch 3/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.2845 - val_loss: 1.5998 Epoch 4/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1671 - val_loss: 1.4054 Epoch 5/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1195 - val_loss: 1.0034 Epoch 6/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.1000 - val_loss: 0.9543 Epoch 7/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1353 - val_loss: 0.6496 Epoch 8/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1688 - val_loss: 0.4768 Epoch 9/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.1562 - val_loss: 0.3414 Epoch 10/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.1296 - val_loss: 0.3393 Epoch 11/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0750 - val_loss: 0.2155 Epoch 12/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0512 - val_loss: 0.1708 Epoch 13/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0332 - val_loss: 0.1139 Epoch 14/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0329 - val_loss: 0.0851 Epoch 15/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0411 - val_loss: 0.0597 Epoch 16/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0455 - val_loss: 0.0591 Epoch 17/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0510 - val_loss: 0.0424 Epoch 18/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0381 - val_loss: 0.0549 Epoch 19/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0403 - val_loss: 0.0466 Epoch 20/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0290 - val_loss: 0.0468 Epoch 21/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0379 - val_loss: 0.0504 Epoch 22/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0444 - val_loss: 0.0553 Epoch 23/1000 15/15 [==============================] - 1s 43ms/step - loss: 0.0479 - val_loss: 0.0480 Epoch 24/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0331 - val_loss: 0.0400 Epoch 25/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0416 - val_loss: 0.0333 Epoch 26/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0396 - val_loss: 0.0276 Epoch 27/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0426 - val_loss: 0.0386 Epoch 28/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0308 - val_loss: 0.0427 Epoch 29/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0365 - val_loss: 0.0369 Epoch 30/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0337 - val_loss: 0.0393 Epoch 31/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0383 - val_loss: 0.0314 Epoch 32/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0358 - val_loss: 0.0533 Epoch 33/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0354 - val_loss: 0.0336 Epoch 34/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0332 - val_loss: 0.0363 Epoch 35/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0326 - val_loss: 0.0638 Epoch 36/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0459 - val_loss: 0.0427 Epoch 37/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0382 - val_loss: 0.0293 Epoch 38/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0440 - val_loss: 0.0446 Epoch 39/1000 15/15 [==============================] - 1s 47ms/step - loss: 0.0436 - val_loss: 0.0673 Epoch 40/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0535 - val_loss: 0.0354 Epoch 41/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0245 - val_loss: 0.0373 Epoch 42/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0225 - val_loss: 0.0313 Epoch 43/1000 15/15 [==============================] - 1s 47ms/step - loss: 0.0179 - val_loss: 0.0267 Epoch 44/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0174 - val_loss: 0.0427 Epoch 45/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0211 - val_loss: 0.0240 Epoch 46/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0139 - val_loss: 0.0275 Epoch 47/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0138 - val_loss: 0.0319 Epoch 48/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0205 - val_loss: 0.0237 Epoch 49/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0163 - val_loss: 0.0310 Epoch 50/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0169 - val_loss: 0.0259 Epoch 51/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0210 - val_loss: 0.0527 Epoch 52/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0261 - val_loss: 0.0432 Epoch 53/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0255 - val_loss: 0.0282 Epoch 54/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0246 - val_loss: 0.0268 Epoch 55/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0183 - val_loss: 0.0577 Epoch 56/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0236 - val_loss: 0.0276 Epoch 57/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0235 - val_loss: 0.0524 Epoch 58/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0308 - val_loss: 0.0517 Epoch 59/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0243 - val_loss: 0.0433 Epoch 60/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0207 - val_loss: 0.0293 Epoch 61/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0191 - val_loss: 0.0307 Epoch 62/1000 15/15 [==============================] - 1s 44ms/step - loss: 0.0183 - val_loss: 0.0350 Epoch 63/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0252 - val_loss: 0.0460 Epoch 64/1000 15/15 [==============================] - 1s 45ms/step - loss: 0.0281 - val_loss: 0.0365 Epoch 65/1000 15/15 [==============================] - 1s 46ms/step - loss: 0.0459 - val_loss: 0.0656 # Show the learning curves history_df = pd . DataFrame ( history . history ) fig , ax = plt . subplots ( 1 , 1 ) history_df . plot ( ax = ax ) ax . set_ylim ( 0 , .2 ) (0.0, 0.2) y_test_pred = np . exp ( model . predict ( X_test_std )) plt . plot ( y_test_pred , np . exp ( y_test_std ), ls = '' , marker = 'o' ) [<matplotlib.lines.Line2D at 0x7f4c3ceb5150>] Model with BatchNorm and Dropout: from tensorflow import keras from tensorflow.keras import layers # Dropout and BatchNorm Model model = keras . Sequential ([ layers . Dense ( 512 , input_shape = [ X_train_std . shape [ 1 ]]), layers . BatchNormalization (), layers . Activation ( 'relu' ), # separate activation layers . Dropout ( 0.3 ), layers . Dense ( 1024 , activation = 'relu' ), # built-in activation layers . Dropout ( 0.3 ), layers . BatchNormalization (), layers . Dense ( 512 , activation = 'relu' ), layers . Dropout ( 0.3 ), layers . BatchNormalization (), layers . Dense ( 1 ), ]) print ( model . summary ()) Model: \"sequential_4\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 512) 7168 _________________________________________________________________ batch_normalization_2 (Batch (None, 512) 2048 _________________________________________________________________ activation (Activation) (None, 512) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________ dense_19 (Dense) (None, 1024) 525312 _________________________________________________________________ dropout_3 (Dropout) (None, 1024) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 1024) 4096 _________________________________________________________________ dense_20 (Dense) (None, 512) 524800 _________________________________________________________________ dropout_4 (Dropout) (None, 512) 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 512) 2048 _________________________________________________________________ dense_21 (Dense) (None, 1) 513 ================================================================= Total params: 1,065,985 Trainable params: 1,061,889 Non-trainable params: 4,096 _________________________________________________________________ None","title":"2.2.2 Batch Normalization"},{"location":"S2_Feed_Forward_Neural_Networks/#exercise-2-try-batch-normalization-and-dropout","text":"Create 3 models and train for 100 epochs with early stopping: compile and train the model previously defined above recreate the architecture but remove the batchnorm layers recreate the architecture but remove the dropout layers and one hidden, dense layer Compare the number of epochs required to converge, and the overall loss # Dropout and BatchNorm Model model = keras . Sequential ([ ### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'mae' , ) early_stopping = EarlyStopping ( min_delta = 0.001 , # minimium amount of change to count as an improvement patience = 20 , # how many epochs to wait before stopping restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 256 , epochs = 100 , verbose = 1 , callbacks = [ early_stopping ], ) Epoch 1/100 15/15 [==============================] - 1s 16ms/step - loss: 1.7653 - val_loss: 1.3092 Epoch 2/100 15/15 [==============================] - 0s 6ms/step - loss: 1.3614 - val_loss: 0.9521 Epoch 3/100 15/15 [==============================] - 0s 7ms/step - loss: 1.0655 - val_loss: 0.6253 Epoch 4/100 15/15 [==============================] - 0s 7ms/step - loss: 0.8148 - val_loss: 0.3236 Epoch 5/100 15/15 [==============================] - 0s 6ms/step - loss: 0.6566 - val_loss: 0.2231 Epoch 6/100 15/15 [==============================] - 0s 6ms/step - loss: 0.5569 - val_loss: 0.1752 Epoch 7/100 15/15 [==============================] - 0s 6ms/step - loss: 0.5185 - val_loss: 0.1731 Epoch 8/100 15/15 [==============================] - 0s 6ms/step - loss: 0.4466 - val_loss: 0.1596 Epoch 9/100 15/15 [==============================] - 0s 6ms/step - loss: 0.4092 - val_loss: 0.1712 Epoch 10/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3836 - val_loss: 0.1516 Epoch 11/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3530 - val_loss: 0.1603 Epoch 12/100 15/15 [==============================] - 0s 6ms/step - loss: 0.3323 - val_loss: 0.1423 Epoch 13/100 15/15 [==============================] - 0s 6ms/step - loss: 0.3423 - val_loss: 0.1385 Epoch 14/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3116 - val_loss: 0.1289 Epoch 15/100 15/15 [==============================] - 0s 7ms/step - loss: 0.3055 - val_loss: 0.1269 Epoch 16/100 15/15 [==============================] - 0s 7ms/step - loss: 0.2849 - val_loss: 0.1067 Epoch 17/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2813 - val_loss: 0.1116 Epoch 18/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2737 - val_loss: 0.1163 Epoch 19/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2520 - val_loss: 0.1149 Epoch 20/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2454 - val_loss: 0.1366 Epoch 21/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2406 - val_loss: 0.1094 Epoch 22/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2334 - val_loss: 0.1069 Epoch 23/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2283 - val_loss: 0.1109 Epoch 24/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2116 - val_loss: 0.1092 Epoch 25/100 15/15 [==============================] - 0s 6ms/step - loss: 0.2048 - val_loss: 0.1024 Epoch 26/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1885 - val_loss: 0.1004 Epoch 27/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1979 - val_loss: 0.1115 Epoch 28/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1863 - val_loss: 0.1077 Epoch 29/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1713 - val_loss: 0.1106 Epoch 30/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1756 - val_loss: 0.1057 Epoch 31/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1598 - val_loss: 0.1120 Epoch 32/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1706 - val_loss: 0.1153 Epoch 33/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1553 - val_loss: 0.1175 Epoch 34/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1628 - val_loss: 0.1051 Epoch 35/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1538 - val_loss: 0.0985 Epoch 36/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1472 - val_loss: 0.1005 Epoch 37/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1447 - val_loss: 0.0950 Epoch 38/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1439 - val_loss: 0.1088 Epoch 39/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1530 - val_loss: 0.1176 Epoch 40/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1400 - val_loss: 0.1240 Epoch 41/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1438 - val_loss: 0.0981 Epoch 42/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1410 - val_loss: 0.0963 Epoch 43/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1420 - val_loss: 0.0972 Epoch 44/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1468 - val_loss: 0.1168 Epoch 45/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1419 - val_loss: 0.0949 Epoch 46/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1401 - val_loss: 0.0940 Epoch 47/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1394 - val_loss: 0.0970 Epoch 48/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1368 - val_loss: 0.1291 Epoch 49/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1445 - val_loss: 0.1008 Epoch 50/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1467 - val_loss: 0.1030 Epoch 51/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1404 - val_loss: 0.1066 Epoch 52/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1420 - val_loss: 0.0946 Epoch 53/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1370 - val_loss: 0.0946 Epoch 54/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1367 - val_loss: 0.1009 Epoch 55/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1403 - val_loss: 0.1037 Epoch 56/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1377 - val_loss: 0.1040 Epoch 57/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1377 - val_loss: 0.0935 Epoch 58/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1420 - val_loss: 0.0921 Epoch 59/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1333 - val_loss: 0.0990 Epoch 60/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1305 - val_loss: 0.1067 Epoch 61/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1362 - val_loss: 0.1123 Epoch 62/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1344 - val_loss: 0.0942 Epoch 63/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1267 - val_loss: 0.0981 Epoch 64/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1247 - val_loss: 0.0904 Epoch 65/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1307 - val_loss: 0.0979 Epoch 66/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1321 - val_loss: 0.0976 Epoch 67/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1325 - val_loss: 0.1214 Epoch 68/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1340 - val_loss: 0.1112 Epoch 69/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1290 - val_loss: 0.1022 Epoch 70/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1330 - val_loss: 0.1014 Epoch 71/100 15/15 [==============================] - 0s 8ms/step - loss: 0.1274 - val_loss: 0.0989 Epoch 72/100 15/15 [==============================] - 0s 8ms/step - loss: 0.1179 - val_loss: 0.0913 Epoch 73/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1157 - val_loss: 0.0970 Epoch 74/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1280 - val_loss: 0.0909 Epoch 75/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1203 - val_loss: 0.0940 Epoch 76/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1293 - val_loss: 0.0934 Epoch 77/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1300 - val_loss: 0.0975 Epoch 78/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1257 - val_loss: 0.0915 Epoch 79/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1265 - val_loss: 0.0975 Epoch 80/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1192 - val_loss: 0.0906 Epoch 81/100 15/15 [==============================] - 0s 7ms/step - loss: 0.1214 - val_loss: 0.0902 Epoch 82/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1209 - val_loss: 0.0994 Epoch 83/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1200 - val_loss: 0.0971 Epoch 84/100 15/15 [==============================] - 0s 6ms/step - loss: 0.1238 - val_loss: 0.0942","title":"\ud83c\udfcb\ufe0f Exercise 2: Try Batch Normalization and Dropout"},{"location":"S2_Feed_Forward_Neural_Networks/#23-binary-classification-with-neural-networks","text":"When dealing with classification tasks, everything we've covered up till now with regression still applies. The main difference is the format of the last layer's outputs and the choice of loss function.","title":"2.3 Binary Classification with Neural Networks"},{"location":"S2_Feed_Forward_Neural_Networks/#231-accuracy-and-cross-entropy","text":"Formerly, we've encountered accuracy in classification tasks. It is the ratio of correct predictions over total predictions: accuracy = number_corect / total (and when classes were heavily imballanced we used a weighted accuracy). The problem with using accuracy as a loss function, is that it does not change smoothly (there are jumps in the numerator since it is just a count in correct predictions), which the SGD algorithm requires in order to work properly. We need another metric. the further a model is from predicting the correct class, the higher the loss. Instead we use cross-entropy , we won't go into detail here, other than that it is a distance measure between two probabilities (the probability of predicting the class or the incorrect class). We want the probability for predicting the correct class to be 1 (100%) and cross-entropy will measure the distance the current probability of the model is from 1. We set cross-entropy as our loss when we compile the model. In this case we have two classes so we use binary_crossentropy model.compile( # optimizer... loss='binary_crossentropy', metrics=['binary_accuracy'], )","title":"2.3.1 Accuracy and Cross Entropy"},{"location":"S2_Feed_Forward_Neural_Networks/#232-0-or-1-the-sigmoid-function","text":"Finally, we need to introduce a special activation function that will map our last layer outputs from 0 to 1, to feed into our loss function. The traditional function we use for this is the sigmoid function . The sigmoid function maps values on the interval [0, 1] To get the final class prediction, we need a threshold probability, typically 0.5 where we will round up to the class label. Keras will set to 0.5 by default. We set the sigmoid activation function in the last layer: model = keras.Sequential([ # ... previous layers ... layers.Dense(1, activation='sigmoid'), ])","title":"2.3.2 0 or 1: The Sigmoid Function"},{"location":"S2_Feed_Forward_Neural_Networks/#233-classification-example","text":"wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'type' ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = y_train . map ({ 'white' : 0 , 'red' : 1 }) # convert to int y_val_std = y_val . map ({ 'white' : 0 , 'red' : 1 }) # convert to int y_test_std = y_test . map ({ 'white' : 0 , 'red' : 1 }) # convert to int preprocessor #sk-540523ed-2679-4d85-a24d-3d67318116c3 {color: black;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 pre{padding: 0;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable {background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-item {z-index: 1;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-parallel-item:only-child::after {width: 0;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-540523ed-2679-4d85-a24d-3d67318116c3 div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Dense ( 4 , activation = 'relu' , input_shape = [ X_train_std . shape [ 1 ]]), layers . Dense ( 4 , activation = 'relu' ), layers . Dense ( 1 , activation = 'sigmoid' ), ]) print ( model . summary ()) Model: \"sequential_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_22 (Dense) (None, 4) 52 _________________________________________________________________ dense_23 (Dense) (None, 4) 20 _________________________________________________________________ dense_24 (Dense) (None, 1) 5 ================================================================= Total params: 77 Trainable params: 77 Non-trainable params: 0 _________________________________________________________________ None tf . keras . utils . plot_model ( model , show_layer_names = True , # show_dtype=True, show_shapes = True ) We set cross-entropy as our loss when we compile the model. In this case we have two classes so we use binary_crossentropy Introducing metrics : we can track other forms of performance during training with the metrics parameter in model.compile . We will look at the results when we finish training. model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], # new metrics flag here ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 512 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 1 , ) Epoch 1/1000 8/8 [==============================] - 1s 21ms/step - loss: 0.6466 - binary_accuracy: 0.6292 - val_loss: 0.6328 - val_binary_accuracy: 0.6371 Epoch 2/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6336 - binary_accuracy: 0.6623 - val_loss: 0.6196 - val_binary_accuracy: 0.6658 Epoch 3/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6207 - binary_accuracy: 0.6891 - val_loss: 0.6066 - val_binary_accuracy: 0.6954 Epoch 4/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.6079 - binary_accuracy: 0.7154 - val_loss: 0.5938 - val_binary_accuracy: 0.7192 Epoch 5/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5951 - binary_accuracy: 0.7365 - val_loss: 0.5812 - val_binary_accuracy: 0.7381 Epoch 6/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5826 - binary_accuracy: 0.7597 - val_loss: 0.5687 - val_binary_accuracy: 0.7594 Epoch 7/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5700 - binary_accuracy: 0.7756 - val_loss: 0.5559 - val_binary_accuracy: 0.7750 Epoch 8/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5573 - binary_accuracy: 0.7909 - val_loss: 0.5428 - val_binary_accuracy: 0.7939 Epoch 9/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5444 - binary_accuracy: 0.8041 - val_loss: 0.5295 - val_binary_accuracy: 0.8095 Epoch 10/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.8158 - val_loss: 0.5162 - val_binary_accuracy: 0.8194 Epoch 11/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.5184 - binary_accuracy: 0.8281 - val_loss: 0.5028 - val_binary_accuracy: 0.8300 Epoch 12/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.5051 - binary_accuracy: 0.8388 - val_loss: 0.4890 - val_binary_accuracy: 0.8415 Epoch 13/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4913 - binary_accuracy: 0.8498 - val_loss: 0.4748 - val_binary_accuracy: 0.8539 Epoch 14/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4771 - binary_accuracy: 0.8580 - val_loss: 0.4601 - val_binary_accuracy: 0.8629 Epoch 15/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4627 - binary_accuracy: 0.8675 - val_loss: 0.4454 - val_binary_accuracy: 0.8695 Epoch 16/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4478 - binary_accuracy: 0.8725 - val_loss: 0.4309 - val_binary_accuracy: 0.8727 Epoch 17/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4327 - binary_accuracy: 0.8788 - val_loss: 0.4163 - val_binary_accuracy: 0.8785 Epoch 18/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4172 - binary_accuracy: 0.8831 - val_loss: 0.4014 - val_binary_accuracy: 0.8826 Epoch 19/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.4015 - binary_accuracy: 0.8886 - val_loss: 0.3863 - val_binary_accuracy: 0.8892 Epoch 20/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3857 - binary_accuracy: 0.8944 - val_loss: 0.3710 - val_binary_accuracy: 0.8957 Epoch 21/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3695 - binary_accuracy: 0.9015 - val_loss: 0.3554 - val_binary_accuracy: 0.9064 Epoch 22/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3536 - binary_accuracy: 0.9078 - val_loss: 0.3400 - val_binary_accuracy: 0.9130 Epoch 23/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.3380 - binary_accuracy: 0.9168 - val_loss: 0.3247 - val_binary_accuracy: 0.9195 Epoch 24/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.3226 - binary_accuracy: 0.9228 - val_loss: 0.3098 - val_binary_accuracy: 0.9286 Epoch 25/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.3074 - binary_accuracy: 0.9297 - val_loss: 0.2953 - val_binary_accuracy: 0.9392 Epoch 26/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.2926 - binary_accuracy: 0.9360 - val_loss: 0.2811 - val_binary_accuracy: 0.9417 Epoch 27/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2780 - binary_accuracy: 0.9409 - val_loss: 0.2671 - val_binary_accuracy: 0.9499 Epoch 28/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2639 - binary_accuracy: 0.9480 - val_loss: 0.2537 - val_binary_accuracy: 0.9548 Epoch 29/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2502 - binary_accuracy: 0.9540 - val_loss: 0.2408 - val_binary_accuracy: 0.9581 Epoch 30/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.2373 - binary_accuracy: 0.9589 - val_loss: 0.2285 - val_binary_accuracy: 0.9606 Epoch 31/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2248 - binary_accuracy: 0.9625 - val_loss: 0.2168 - val_binary_accuracy: 0.9663 Epoch 32/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2129 - binary_accuracy: 0.9658 - val_loss: 0.2056 - val_binary_accuracy: 0.9672 Epoch 33/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.2017 - binary_accuracy: 0.9699 - val_loss: 0.1948 - val_binary_accuracy: 0.9688 Epoch 34/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1910 - binary_accuracy: 0.9729 - val_loss: 0.1849 - val_binary_accuracy: 0.9713 Epoch 35/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1811 - binary_accuracy: 0.9762 - val_loss: 0.1755 - val_binary_accuracy: 0.9745 Epoch 36/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1718 - binary_accuracy: 0.9787 - val_loss: 0.1665 - val_binary_accuracy: 0.9770 Epoch 37/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1630 - binary_accuracy: 0.9795 - val_loss: 0.1582 - val_binary_accuracy: 0.9778 Epoch 38/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.1546 - binary_accuracy: 0.9806 - val_loss: 0.1504 - val_binary_accuracy: 0.9778 Epoch 39/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1468 - binary_accuracy: 0.9806 - val_loss: 0.1431 - val_binary_accuracy: 0.9778 Epoch 40/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1394 - binary_accuracy: 0.9817 - val_loss: 0.1362 - val_binary_accuracy: 0.9803 Epoch 41/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1326 - binary_accuracy: 0.9819 - val_loss: 0.1297 - val_binary_accuracy: 0.9819 Epoch 42/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1262 - binary_accuracy: 0.9839 - val_loss: 0.1237 - val_binary_accuracy: 0.9819 Epoch 43/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1202 - binary_accuracy: 0.9849 - val_loss: 0.1183 - val_binary_accuracy: 0.9828 Epoch 44/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1147 - binary_accuracy: 0.9849 - val_loss: 0.1132 - val_binary_accuracy: 0.9828 Epoch 45/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.1096 - binary_accuracy: 0.9852 - val_loss: 0.1085 - val_binary_accuracy: 0.9844 Epoch 46/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1048 - binary_accuracy: 0.9858 - val_loss: 0.1042 - val_binary_accuracy: 0.9844 Epoch 47/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.1003 - binary_accuracy: 0.9866 - val_loss: 0.1001 - val_binary_accuracy: 0.9852 Epoch 48/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0962 - binary_accuracy: 0.9866 - val_loss: 0.0963 - val_binary_accuracy: 0.9860 Epoch 49/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - binary_accuracy: 0.9869 - val_loss: 0.0929 - val_binary_accuracy: 0.9869 Epoch 50/1000 8/8 [==============================] - 0s 4ms/step - loss: 0.0889 - binary_accuracy: 0.9869 - val_loss: 0.0898 - val_binary_accuracy: 0.9877 Epoch 51/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0856 - binary_accuracy: 0.9871 - val_loss: 0.0868 - val_binary_accuracy: 0.9877 Epoch 52/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0826 - binary_accuracy: 0.9874 - val_loss: 0.0840 - val_binary_accuracy: 0.9877 Epoch 53/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0798 - binary_accuracy: 0.9871 - val_loss: 0.0813 - val_binary_accuracy: 0.9877 Epoch 54/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0771 - binary_accuracy: 0.9877 - val_loss: 0.0788 - val_binary_accuracy: 0.9877 Epoch 55/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0747 - binary_accuracy: 0.9880 - val_loss: 0.0765 - val_binary_accuracy: 0.9877 Epoch 56/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0725 - binary_accuracy: 0.9885 - val_loss: 0.0744 - val_binary_accuracy: 0.9877 Epoch 57/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0704 - binary_accuracy: 0.9882 - val_loss: 0.0726 - val_binary_accuracy: 0.9877 Epoch 58/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0685 - binary_accuracy: 0.9882 - val_loss: 0.0708 - val_binary_accuracy: 0.9877 Epoch 59/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0667 - binary_accuracy: 0.9880 - val_loss: 0.0691 - val_binary_accuracy: 0.9869 Epoch 60/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0651 - binary_accuracy: 0.9882 - val_loss: 0.0675 - val_binary_accuracy: 0.9869 Epoch 61/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0635 - binary_accuracy: 0.9888 - val_loss: 0.0661 - val_binary_accuracy: 0.9869 Epoch 62/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0620 - binary_accuracy: 0.9888 - val_loss: 0.0647 - val_binary_accuracy: 0.9869 Epoch 63/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0607 - binary_accuracy: 0.9893 - val_loss: 0.0635 - val_binary_accuracy: 0.9869 Epoch 64/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0594 - binary_accuracy: 0.9896 - val_loss: 0.0623 - val_binary_accuracy: 0.9877 Epoch 65/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0582 - binary_accuracy: 0.9893 - val_loss: 0.0612 - val_binary_accuracy: 0.9877 Epoch 66/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0571 - binary_accuracy: 0.9896 - val_loss: 0.0601 - val_binary_accuracy: 0.9877 Epoch 67/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0559 - binary_accuracy: 0.9896 - val_loss: 0.0591 - val_binary_accuracy: 0.9877 Epoch 68/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0549 - binary_accuracy: 0.9896 - val_loss: 0.0581 - val_binary_accuracy: 0.9877 Epoch 69/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0539 - binary_accuracy: 0.9896 - val_loss: 0.0572 - val_binary_accuracy: 0.9877 Epoch 70/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - binary_accuracy: 0.9896 - val_loss: 0.0563 - val_binary_accuracy: 0.9885 Epoch 71/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0521 - binary_accuracy: 0.9899 - val_loss: 0.0554 - val_binary_accuracy: 0.9885 Epoch 72/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0512 - binary_accuracy: 0.9901 - val_loss: 0.0546 - val_binary_accuracy: 0.9885 Epoch 73/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0505 - binary_accuracy: 0.9901 - val_loss: 0.0539 - val_binary_accuracy: 0.9885 Epoch 74/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0498 - binary_accuracy: 0.9901 - val_loss: 0.0532 - val_binary_accuracy: 0.9885 Epoch 75/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0492 - binary_accuracy: 0.9901 - val_loss: 0.0525 - val_binary_accuracy: 0.9885 Epoch 76/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0485 - binary_accuracy: 0.9901 - val_loss: 0.0519 - val_binary_accuracy: 0.9885 Epoch 77/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0479 - binary_accuracy: 0.9901 - val_loss: 0.0514 - val_binary_accuracy: 0.9885 Epoch 78/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0473 - binary_accuracy: 0.9901 - val_loss: 0.0509 - val_binary_accuracy: 0.9885 Epoch 79/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0468 - binary_accuracy: 0.9901 - val_loss: 0.0504 - val_binary_accuracy: 0.9885 Epoch 80/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - binary_accuracy: 0.9901 - val_loss: 0.0499 - val_binary_accuracy: 0.9885 Epoch 81/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0458 - binary_accuracy: 0.9901 - val_loss: 0.0494 - val_binary_accuracy: 0.9893 Epoch 82/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0454 - binary_accuracy: 0.9901 - val_loss: 0.0490 - val_binary_accuracy: 0.9893 Epoch 83/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0450 - binary_accuracy: 0.9901 - val_loss: 0.0486 - val_binary_accuracy: 0.9901 Epoch 84/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0446 - binary_accuracy: 0.9901 - val_loss: 0.0481 - val_binary_accuracy: 0.9901 Epoch 85/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0442 - binary_accuracy: 0.9904 - val_loss: 0.0478 - val_binary_accuracy: 0.9901 Epoch 86/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0438 - binary_accuracy: 0.9904 - val_loss: 0.0475 - val_binary_accuracy: 0.9901 Epoch 87/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0434 - binary_accuracy: 0.9904 - val_loss: 0.0472 - val_binary_accuracy: 0.9901 Epoch 88/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0431 - binary_accuracy: 0.9904 - val_loss: 0.0469 - val_binary_accuracy: 0.9901 Epoch 89/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0428 - binary_accuracy: 0.9904 - val_loss: 0.0465 - val_binary_accuracy: 0.9910 Epoch 90/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0425 - binary_accuracy: 0.9904 - val_loss: 0.0463 - val_binary_accuracy: 0.9910 Epoch 91/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0422 - binary_accuracy: 0.9904 - val_loss: 0.0460 - val_binary_accuracy: 0.9910 Epoch 92/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0419 - binary_accuracy: 0.9907 - val_loss: 0.0457 - val_binary_accuracy: 0.9910 Epoch 93/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0416 - binary_accuracy: 0.9907 - val_loss: 0.0455 - val_binary_accuracy: 0.9910 Epoch 94/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0413 - binary_accuracy: 0.9907 - val_loss: 0.0452 - val_binary_accuracy: 0.9910 Epoch 95/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0410 - binary_accuracy: 0.9907 - val_loss: 0.0450 - val_binary_accuracy: 0.9910 Epoch 96/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0408 - binary_accuracy: 0.9907 - val_loss: 0.0448 - val_binary_accuracy: 0.9910 Epoch 97/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0405 - binary_accuracy: 0.9907 - val_loss: 0.0446 - val_binary_accuracy: 0.9910 Epoch 98/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0403 - binary_accuracy: 0.9907 - val_loss: 0.0444 - val_binary_accuracy: 0.9910 Epoch 99/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0400 - binary_accuracy: 0.9912 - val_loss: 0.0442 - val_binary_accuracy: 0.9910 Epoch 100/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0398 - binary_accuracy: 0.9915 - val_loss: 0.0439 - val_binary_accuracy: 0.9910 Epoch 101/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0396 - binary_accuracy: 0.9915 - val_loss: 0.0437 - val_binary_accuracy: 0.9910 Epoch 102/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0394 - binary_accuracy: 0.9915 - val_loss: 0.0436 - val_binary_accuracy: 0.9910 Epoch 103/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0391 - binary_accuracy: 0.9915 - val_loss: 0.0434 - val_binary_accuracy: 0.9910 Epoch 104/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0390 - binary_accuracy: 0.9915 - val_loss: 0.0432 - val_binary_accuracy: 0.9910 Epoch 105/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0388 - binary_accuracy: 0.9915 - val_loss: 0.0430 - val_binary_accuracy: 0.9910 Epoch 106/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0385 - binary_accuracy: 0.9918 - val_loss: 0.0428 - val_binary_accuracy: 0.9910 Epoch 107/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0383 - binary_accuracy: 0.9918 - val_loss: 0.0427 - val_binary_accuracy: 0.9918 Epoch 108/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0381 - binary_accuracy: 0.9918 - val_loss: 0.0425 - val_binary_accuracy: 0.9918 Epoch 109/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0380 - binary_accuracy: 0.9918 - val_loss: 0.0423 - val_binary_accuracy: 0.9918 Epoch 110/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0378 - binary_accuracy: 0.9921 - val_loss: 0.0421 - val_binary_accuracy: 0.9918 Epoch 111/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0376 - binary_accuracy: 0.9921 - val_loss: 0.0420 - val_binary_accuracy: 0.9918 Epoch 112/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0374 - binary_accuracy: 0.9921 - val_loss: 0.0419 - val_binary_accuracy: 0.9918 Epoch 113/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0372 - binary_accuracy: 0.9918 - val_loss: 0.0417 - val_binary_accuracy: 0.9918 Epoch 114/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0371 - binary_accuracy: 0.9918 - val_loss: 0.0415 - val_binary_accuracy: 0.9918 Epoch 115/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0369 - binary_accuracy: 0.9921 - val_loss: 0.0414 - val_binary_accuracy: 0.9918 Epoch 116/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0368 - binary_accuracy: 0.9921 - val_loss: 0.0413 - val_binary_accuracy: 0.9918 Epoch 117/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0366 - binary_accuracy: 0.9921 - val_loss: 0.0411 - val_binary_accuracy: 0.9926 Epoch 118/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0365 - binary_accuracy: 0.9921 - val_loss: 0.0410 - val_binary_accuracy: 0.9918 Epoch 119/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0364 - binary_accuracy: 0.9923 - val_loss: 0.0409 - val_binary_accuracy: 0.9918 Epoch 120/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - binary_accuracy: 0.9923 - val_loss: 0.0408 - val_binary_accuracy: 0.9918 Epoch 121/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0361 - binary_accuracy: 0.9923 - val_loss: 0.0406 - val_binary_accuracy: 0.9918 Epoch 122/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0360 - binary_accuracy: 0.9923 - val_loss: 0.0405 - val_binary_accuracy: 0.9918 Epoch 123/1000 8/8 [==============================] - 0s 8ms/step - loss: 0.0359 - binary_accuracy: 0.9923 - val_loss: 0.0403 - val_binary_accuracy: 0.9918 Epoch 124/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0357 - binary_accuracy: 0.9923 - val_loss: 0.0402 - val_binary_accuracy: 0.9918 Epoch 125/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0356 - binary_accuracy: 0.9923 - val_loss: 0.0401 - val_binary_accuracy: 0.9918 Epoch 126/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0355 - binary_accuracy: 0.9923 - val_loss: 0.0399 - val_binary_accuracy: 0.9926 Epoch 127/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0354 - binary_accuracy: 0.9926 - val_loss: 0.0398 - val_binary_accuracy: 0.9926 Epoch 128/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0353 - binary_accuracy: 0.9926 - val_loss: 0.0397 - val_binary_accuracy: 0.9926 Epoch 129/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0352 - binary_accuracy: 0.9926 - val_loss: 0.0395 - val_binary_accuracy: 0.9926 Epoch 130/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0351 - binary_accuracy: 0.9926 - val_loss: 0.0394 - val_binary_accuracy: 0.9926 Epoch 131/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0350 - binary_accuracy: 0.9926 - val_loss: 0.0393 - val_binary_accuracy: 0.9918 Epoch 132/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0349 - binary_accuracy: 0.9926 - val_loss: 0.0392 - val_binary_accuracy: 0.9918 Epoch 133/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0348 - binary_accuracy: 0.9926 - val_loss: 0.0392 - val_binary_accuracy: 0.9918 Epoch 134/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0347 - binary_accuracy: 0.9929 - val_loss: 0.0391 - val_binary_accuracy: 0.9918 Epoch 135/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0345 - binary_accuracy: 0.9926 - val_loss: 0.0390 - val_binary_accuracy: 0.9918 Epoch 136/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0344 - binary_accuracy: 0.9929 - val_loss: 0.0389 - val_binary_accuracy: 0.9918 Epoch 137/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0344 - binary_accuracy: 0.9929 - val_loss: 0.0388 - val_binary_accuracy: 0.9918 Epoch 138/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0343 - binary_accuracy: 0.9932 - val_loss: 0.0387 - val_binary_accuracy: 0.9926 Epoch 139/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0342 - binary_accuracy: 0.9932 - val_loss: 0.0386 - val_binary_accuracy: 0.9918 Epoch 140/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0341 - binary_accuracy: 0.9932 - val_loss: 0.0385 - val_binary_accuracy: 0.9918 Epoch 141/1000 8/8 [==============================] - 0s 6ms/step - loss: 0.0339 - binary_accuracy: 0.9929 - val_loss: 0.0384 - val_binary_accuracy: 0.9918 Epoch 142/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0338 - binary_accuracy: 0.9929 - val_loss: 0.0383 - val_binary_accuracy: 0.9918 Epoch 143/1000 8/8 [==============================] - 0s 5ms/step - loss: 0.0337 - binary_accuracy: 0.9929 - val_loss: 0.0382 - val_binary_accuracy: 0.9918 Epoch 144/1000 8/8 [==============================] - 0s 7ms/step - loss: 0.0336 - binary_accuracy: 0.9929 - val_loss: 0.0382 - val_binary_accuracy: 0.9918 history_df = pd . DataFrame ( history . history ) history_df . loc [:, [ 'loss' , 'val_loss' ]] . plot () history_df . loc [:, [ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot () print (( \"Best Validation Loss: {:0.4f} \" + \\ \" \\n Best Validation Accuracy: {:0.4f} \" ) \\ . format ( history_df [ 'val_loss' ] . min (), history_df [ 'val_binary_accuracy' ] . max ())) Best Validation Loss: 0.0382 Best Validation Accuracy: 0.9926 # predict test set pred_probability = model . predict ( X_test_std ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score print ( classification_report ( y_test_std , predictions )) precision recall f1-score support 0 1.00 1.00 1.00 1231 1 0.99 0.99 0.99 394 accuracy 1.00 1625 macro avg 0.99 1.00 0.99 1625 weighted avg 1.00 1.00 1.00 1625 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( y_test_std , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f4c3c439c90>","title":"2.3.3 Classification Example"},{"location":"S2_Feed_Forward_Neural_Networks/#24-multi-class-classification","text":"back to top It is good practice, and often necessary to one-hot encode the target values for a multi-class problem. We will need to do that with our wine data here.","title":"2.4 Multi-Class Classification"},{"location":"S2_Feed_Forward_Neural_Networks/#exercise-3-multi-class-classification","text":"1) Define Model Define a model with both batch normalization and dropout layers. 2) Add Optimizer, Loss, and Metric 3) Train and Evaluate from keras.utils import np_utils wine = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"ds_for_engineers/main/data/wine_quality/winequalityN.csv\" ) # create X and y X = wine . copy () y = X . pop ( 'quality' ) # preprocess y for quality and/or type encoder = LabelEncoder () encoder . fit ( y ) encoded_y = encoder . transform ( y ) y = np_utils . to_categorical ( encoded_y ) # split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y ) X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train ) # the numerical values pipe num_proc = make_pipeline ( SimpleImputer ( strategy = 'median' ), StandardScaler ()) # the categorical values pipe cat_proc = make_pipeline ( SimpleImputer ( strategy = 'constant' , fill_value = 'missing' ), OneHotEncoder ( handle_unknown = 'ignore' )) # parallelize the two pipes preprocessor = make_column_transformer (( num_proc , make_column_selector ( dtype_include = np . number )), ( cat_proc , make_column_selector ( dtype_include = object ))) X_train_std = preprocessor . fit_transform ( X_train ) # fit_transform on train X_test_std = preprocessor . transform ( X_test ) # transform test X_val_std = preprocessor . transform ( X_val ) y_train_std = y_train y_val_std = y_val y_test_std = y_test preprocessor #sk-babe4437-2322-43ce-8b3c-05d68e06278f {color: black;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f pre{padding: 0;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable {background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-estimator:hover {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-item {z-index: 1;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-parallel-item:only-child::after {width: 0;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-babe4437-2322-43ce-8b3c-05d68e06278f div.sk-container {display: inline-block;position: relative;} ColumnTransformer ColumnTransformer(transformers=[('pipeline-1', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')), ('standardscaler', StandardScaler())]), ), ('pipeline-2', Pipeline(steps=[('simpleimputer', SimpleImputer(fill_value='missing', strategy='constant')), ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))]), )]) pipeline-1 SimpleImputer SimpleImputer(strategy='median') StandardScaler StandardScaler() pipeline-2 SimpleImputer SimpleImputer(fill_value='missing', strategy='constant') OneHotEncoder OneHotEncoder(handle_unknown='ignore') from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ ### YOUR MODEL HERE ### ]) print ( model . summary ()) Model: \"sequential_8\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_33 (Dense) (None, 4) 56 _________________________________________________________________ dense_34 (Dense) (None, 4) 20 _________________________________________________________________ dense_35 (Dense) (None, 7) 35 ================================================================= Total params: 111 Trainable params: 111 Non-trainable params: 0 _________________________________________________________________ None model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ], ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , ) history = model . fit ( X_train_std , y_train_std , validation_data = ( X_val_std , y_val_std ), batch_size = 512 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 1 , # hide the output because we have so many epochs ) Epoch 1/1000 8/8 [==============================] - 1s 25ms/step - loss: 1.9526 - accuracy: 0.1089 - val_loss: 1.9443 - val_accuracy: 0.1667 Epoch 2/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.9330 - accuracy: 0.1656 - val_loss: 1.9251 - val_accuracy: 0.2192 Epoch 3/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.9144 - accuracy: 0.2288 - val_loss: 1.9066 - val_accuracy: 0.3005 Epoch 4/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.8962 - accuracy: 0.3049 - val_loss: 1.8879 - val_accuracy: 0.3259 Epoch 5/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8778 - accuracy: 0.3309 - val_loss: 1.8686 - val_accuracy: 0.3563 Epoch 6/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8586 - accuracy: 0.3629 - val_loss: 1.8485 - val_accuracy: 0.3834 Epoch 7/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.8387 - accuracy: 0.3829 - val_loss: 1.8270 - val_accuracy: 0.4015 Epoch 8/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.8174 - accuracy: 0.3960 - val_loss: 1.8045 - val_accuracy: 0.4097 Epoch 9/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7950 - accuracy: 0.4056 - val_loss: 1.7808 - val_accuracy: 0.4146 Epoch 10/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7713 - accuracy: 0.4146 - val_loss: 1.7556 - val_accuracy: 0.4220 Epoch 11/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7465 - accuracy: 0.4187 - val_loss: 1.7291 - val_accuracy: 0.4220 Epoch 12/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.7204 - accuracy: 0.4220 - val_loss: 1.7017 - val_accuracy: 0.4245 Epoch 13/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6936 - accuracy: 0.4245 - val_loss: 1.6735 - val_accuracy: 0.4245 Epoch 14/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6663 - accuracy: 0.4261 - val_loss: 1.6452 - val_accuracy: 0.4294 Epoch 15/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6390 - accuracy: 0.4302 - val_loss: 1.6171 - val_accuracy: 0.4261 Epoch 16/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.6114 - accuracy: 0.4335 - val_loss: 1.5890 - val_accuracy: 0.4310 Epoch 17/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5836 - accuracy: 0.4387 - val_loss: 1.5613 - val_accuracy: 0.4286 Epoch 18/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5562 - accuracy: 0.4417 - val_loss: 1.5347 - val_accuracy: 0.4319 Epoch 19/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5300 - accuracy: 0.4469 - val_loss: 1.5096 - val_accuracy: 0.4376 Epoch 20/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.5058 - accuracy: 0.4518 - val_loss: 1.4865 - val_accuracy: 0.4433 Epoch 21/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.4831 - accuracy: 0.4559 - val_loss: 1.4651 - val_accuracy: 0.4442 Epoch 22/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.4626 - accuracy: 0.4570 - val_loss: 1.4460 - val_accuracy: 0.4466 Epoch 23/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4440 - accuracy: 0.4576 - val_loss: 1.4287 - val_accuracy: 0.4507 Epoch 24/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4274 - accuracy: 0.4592 - val_loss: 1.4126 - val_accuracy: 0.4507 Epoch 25/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.4120 - accuracy: 0.4587 - val_loss: 1.3978 - val_accuracy: 0.4548 Epoch 26/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3984 - accuracy: 0.4589 - val_loss: 1.3841 - val_accuracy: 0.4548 Epoch 27/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3859 - accuracy: 0.4617 - val_loss: 1.3713 - val_accuracy: 0.4548 Epoch 28/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3744 - accuracy: 0.4603 - val_loss: 1.3598 - val_accuracy: 0.4532 Epoch 29/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3640 - accuracy: 0.4625 - val_loss: 1.3494 - val_accuracy: 0.4565 Epoch 30/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3545 - accuracy: 0.4636 - val_loss: 1.3401 - val_accuracy: 0.4581 Epoch 31/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3459 - accuracy: 0.4641 - val_loss: 1.3317 - val_accuracy: 0.4581 Epoch 32/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3380 - accuracy: 0.4641 - val_loss: 1.3236 - val_accuracy: 0.4573 Epoch 33/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3310 - accuracy: 0.4636 - val_loss: 1.3165 - val_accuracy: 0.4565 Epoch 34/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3244 - accuracy: 0.4644 - val_loss: 1.3100 - val_accuracy: 0.4589 Epoch 35/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3182 - accuracy: 0.4652 - val_loss: 1.3039 - val_accuracy: 0.4573 Epoch 36/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3126 - accuracy: 0.4631 - val_loss: 1.2982 - val_accuracy: 0.4565 Epoch 37/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3071 - accuracy: 0.4628 - val_loss: 1.2929 - val_accuracy: 0.4622 Epoch 38/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.3022 - accuracy: 0.4628 - val_loss: 1.2881 - val_accuracy: 0.4639 Epoch 39/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2976 - accuracy: 0.4639 - val_loss: 1.2838 - val_accuracy: 0.4647 Epoch 40/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2937 - accuracy: 0.4636 - val_loss: 1.2797 - val_accuracy: 0.4639 Epoch 41/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2897 - accuracy: 0.4639 - val_loss: 1.2758 - val_accuracy: 0.4639 Epoch 42/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2862 - accuracy: 0.4631 - val_loss: 1.2722 - val_accuracy: 0.4631 Epoch 43/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2830 - accuracy: 0.4639 - val_loss: 1.2686 - val_accuracy: 0.4614 Epoch 44/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2798 - accuracy: 0.4636 - val_loss: 1.2658 - val_accuracy: 0.4614 Epoch 45/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.2768 - accuracy: 0.4644 - val_loss: 1.2631 - val_accuracy: 0.4639 Epoch 46/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2740 - accuracy: 0.4661 - val_loss: 1.2608 - val_accuracy: 0.4647 Epoch 47/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2712 - accuracy: 0.4658 - val_loss: 1.2582 - val_accuracy: 0.4647 Epoch 48/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2686 - accuracy: 0.4661 - val_loss: 1.2559 - val_accuracy: 0.4672 Epoch 49/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2657 - accuracy: 0.4680 - val_loss: 1.2529 - val_accuracy: 0.4713 Epoch 50/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2631 - accuracy: 0.4669 - val_loss: 1.2500 - val_accuracy: 0.4713 Epoch 51/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2606 - accuracy: 0.4688 - val_loss: 1.2470 - val_accuracy: 0.4778 Epoch 52/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2580 - accuracy: 0.4735 - val_loss: 1.2438 - val_accuracy: 0.4811 Epoch 53/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2556 - accuracy: 0.4726 - val_loss: 1.2408 - val_accuracy: 0.4860 Epoch 54/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2530 - accuracy: 0.4751 - val_loss: 1.2377 - val_accuracy: 0.4852 Epoch 55/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2502 - accuracy: 0.4770 - val_loss: 1.2358 - val_accuracy: 0.4901 Epoch 56/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2477 - accuracy: 0.4781 - val_loss: 1.2343 - val_accuracy: 0.4885 Epoch 57/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2451 - accuracy: 0.4787 - val_loss: 1.2317 - val_accuracy: 0.4926 Epoch 58/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2422 - accuracy: 0.4825 - val_loss: 1.2286 - val_accuracy: 0.4959 Epoch 59/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2396 - accuracy: 0.4828 - val_loss: 1.2265 - val_accuracy: 0.4992 Epoch 60/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2371 - accuracy: 0.4839 - val_loss: 1.2242 - val_accuracy: 0.5000 Epoch 61/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2342 - accuracy: 0.4839 - val_loss: 1.2221 - val_accuracy: 0.4967 Epoch 62/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2313 - accuracy: 0.4877 - val_loss: 1.2200 - val_accuracy: 0.4967 Epoch 63/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2287 - accuracy: 0.4893 - val_loss: 1.2178 - val_accuracy: 0.5025 Epoch 64/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2262 - accuracy: 0.4891 - val_loss: 1.2158 - val_accuracy: 0.5016 Epoch 65/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2237 - accuracy: 0.4915 - val_loss: 1.2135 - val_accuracy: 0.5016 Epoch 66/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.2213 - accuracy: 0.4918 - val_loss: 1.2119 - val_accuracy: 0.5025 Epoch 67/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2189 - accuracy: 0.4926 - val_loss: 1.2100 - val_accuracy: 0.5000 Epoch 68/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2166 - accuracy: 0.4951 - val_loss: 1.2072 - val_accuracy: 0.5074 Epoch 69/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2137 - accuracy: 0.4973 - val_loss: 1.2052 - val_accuracy: 0.5025 Epoch 70/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.2114 - accuracy: 0.4995 - val_loss: 1.2028 - val_accuracy: 0.5025 Epoch 71/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2088 - accuracy: 0.5003 - val_loss: 1.1996 - val_accuracy: 0.5041 Epoch 72/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2062 - accuracy: 0.5019 - val_loss: 1.1964 - val_accuracy: 0.5066 Epoch 73/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.2036 - accuracy: 0.5027 - val_loss: 1.1936 - val_accuracy: 0.5049 Epoch 74/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.2013 - accuracy: 0.5055 - val_loss: 1.1917 - val_accuracy: 0.5082 Epoch 75/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1988 - accuracy: 0.5041 - val_loss: 1.1900 - val_accuracy: 0.5057 Epoch 76/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1959 - accuracy: 0.5038 - val_loss: 1.1879 - val_accuracy: 0.5099 Epoch 77/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1934 - accuracy: 0.5049 - val_loss: 1.1854 - val_accuracy: 0.5131 Epoch 78/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1908 - accuracy: 0.5077 - val_loss: 1.1830 - val_accuracy: 0.5107 Epoch 79/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1879 - accuracy: 0.5071 - val_loss: 1.1810 - val_accuracy: 0.5074 Epoch 80/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1852 - accuracy: 0.5060 - val_loss: 1.1786 - val_accuracy: 0.5074 Epoch 81/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1824 - accuracy: 0.5082 - val_loss: 1.1767 - val_accuracy: 0.5090 Epoch 82/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1798 - accuracy: 0.5093 - val_loss: 1.1743 - val_accuracy: 0.5066 Epoch 83/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1774 - accuracy: 0.5093 - val_loss: 1.1719 - val_accuracy: 0.5041 Epoch 84/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1749 - accuracy: 0.5088 - val_loss: 1.1693 - val_accuracy: 0.5066 Epoch 85/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1721 - accuracy: 0.5099 - val_loss: 1.1665 - val_accuracy: 0.5140 Epoch 86/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1696 - accuracy: 0.5112 - val_loss: 1.1640 - val_accuracy: 0.5189 Epoch 87/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1674 - accuracy: 0.5129 - val_loss: 1.1616 - val_accuracy: 0.5197 Epoch 88/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1649 - accuracy: 0.5126 - val_loss: 1.1597 - val_accuracy: 0.5172 Epoch 89/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1625 - accuracy: 0.5142 - val_loss: 1.1580 - val_accuracy: 0.5172 Epoch 90/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1601 - accuracy: 0.5142 - val_loss: 1.1562 - val_accuracy: 0.5148 Epoch 91/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1579 - accuracy: 0.5153 - val_loss: 1.1543 - val_accuracy: 0.5156 Epoch 92/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1557 - accuracy: 0.5142 - val_loss: 1.1526 - val_accuracy: 0.5181 Epoch 93/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1534 - accuracy: 0.5140 - val_loss: 1.1515 - val_accuracy: 0.5172 Epoch 94/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.1514 - accuracy: 0.5151 - val_loss: 1.1505 - val_accuracy: 0.5156 Epoch 95/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1494 - accuracy: 0.5151 - val_loss: 1.1489 - val_accuracy: 0.5115 Epoch 96/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1475 - accuracy: 0.5170 - val_loss: 1.1475 - val_accuracy: 0.5172 Epoch 97/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1453 - accuracy: 0.5189 - val_loss: 1.1452 - val_accuracy: 0.5148 Epoch 98/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1430 - accuracy: 0.5192 - val_loss: 1.1430 - val_accuracy: 0.5148 Epoch 99/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.1410 - accuracy: 0.5186 - val_loss: 1.1409 - val_accuracy: 0.5148 Epoch 100/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1391 - accuracy: 0.5208 - val_loss: 1.1392 - val_accuracy: 0.5164 Epoch 101/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1374 - accuracy: 0.5205 - val_loss: 1.1380 - val_accuracy: 0.5197 Epoch 102/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1357 - accuracy: 0.5235 - val_loss: 1.1369 - val_accuracy: 0.5156 Epoch 103/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1338 - accuracy: 0.5244 - val_loss: 1.1354 - val_accuracy: 0.5156 Epoch 104/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1322 - accuracy: 0.5241 - val_loss: 1.1349 - val_accuracy: 0.5172 Epoch 105/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1303 - accuracy: 0.5227 - val_loss: 1.1331 - val_accuracy: 0.5181 Epoch 106/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1285 - accuracy: 0.5257 - val_loss: 1.1311 - val_accuracy: 0.5189 Epoch 107/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1268 - accuracy: 0.5276 - val_loss: 1.1298 - val_accuracy: 0.5181 Epoch 108/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1249 - accuracy: 0.5315 - val_loss: 1.1287 - val_accuracy: 0.5238 Epoch 109/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1232 - accuracy: 0.5353 - val_loss: 1.1271 - val_accuracy: 0.5271 Epoch 110/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1217 - accuracy: 0.5372 - val_loss: 1.1254 - val_accuracy: 0.5255 Epoch 111/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1201 - accuracy: 0.5397 - val_loss: 1.1235 - val_accuracy: 0.5312 Epoch 112/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1183 - accuracy: 0.5397 - val_loss: 1.1224 - val_accuracy: 0.5312 Epoch 113/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1167 - accuracy: 0.5416 - val_loss: 1.1213 - val_accuracy: 0.5304 Epoch 114/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1152 - accuracy: 0.5449 - val_loss: 1.1202 - val_accuracy: 0.5312 Epoch 115/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1136 - accuracy: 0.5457 - val_loss: 1.1189 - val_accuracy: 0.5328 Epoch 116/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1117 - accuracy: 0.5468 - val_loss: 1.1179 - val_accuracy: 0.5353 Epoch 117/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1103 - accuracy: 0.5471 - val_loss: 1.1175 - val_accuracy: 0.5353 Epoch 118/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1089 - accuracy: 0.5465 - val_loss: 1.1171 - val_accuracy: 0.5345 Epoch 119/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1075 - accuracy: 0.5471 - val_loss: 1.1165 - val_accuracy: 0.5378 Epoch 120/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1062 - accuracy: 0.5487 - val_loss: 1.1153 - val_accuracy: 0.5411 Epoch 121/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1049 - accuracy: 0.5498 - val_loss: 1.1142 - val_accuracy: 0.5378 Epoch 122/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.1036 - accuracy: 0.5501 - val_loss: 1.1134 - val_accuracy: 0.5312 Epoch 123/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1024 - accuracy: 0.5484 - val_loss: 1.1121 - val_accuracy: 0.5353 Epoch 124/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1011 - accuracy: 0.5501 - val_loss: 1.1115 - val_accuracy: 0.5402 Epoch 125/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.1000 - accuracy: 0.5523 - val_loss: 1.1109 - val_accuracy: 0.5411 Epoch 126/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0987 - accuracy: 0.5506 - val_loss: 1.1092 - val_accuracy: 0.5378 Epoch 127/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0972 - accuracy: 0.5561 - val_loss: 1.1080 - val_accuracy: 0.5361 Epoch 128/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0958 - accuracy: 0.5534 - val_loss: 1.1080 - val_accuracy: 0.5337 Epoch 129/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0947 - accuracy: 0.5558 - val_loss: 1.1077 - val_accuracy: 0.5353 Epoch 130/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0933 - accuracy: 0.5531 - val_loss: 1.1074 - val_accuracy: 0.5369 Epoch 131/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0922 - accuracy: 0.5536 - val_loss: 1.1070 - val_accuracy: 0.5394 Epoch 132/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0911 - accuracy: 0.5523 - val_loss: 1.1059 - val_accuracy: 0.5394 Epoch 133/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0900 - accuracy: 0.5536 - val_loss: 1.1047 - val_accuracy: 0.5378 Epoch 134/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0890 - accuracy: 0.5545 - val_loss: 1.1034 - val_accuracy: 0.5411 Epoch 135/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0879 - accuracy: 0.5564 - val_loss: 1.1017 - val_accuracy: 0.5427 Epoch 136/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0869 - accuracy: 0.5545 - val_loss: 1.1010 - val_accuracy: 0.5435 Epoch 137/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0860 - accuracy: 0.5564 - val_loss: 1.1006 - val_accuracy: 0.5435 Epoch 138/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0849 - accuracy: 0.5567 - val_loss: 1.1004 - val_accuracy: 0.5443 Epoch 139/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0842 - accuracy: 0.5553 - val_loss: 1.0999 - val_accuracy: 0.5452 Epoch 140/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0831 - accuracy: 0.5558 - val_loss: 1.0988 - val_accuracy: 0.5484 Epoch 141/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0822 - accuracy: 0.5550 - val_loss: 1.0979 - val_accuracy: 0.5525 Epoch 142/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0816 - accuracy: 0.5577 - val_loss: 1.0975 - val_accuracy: 0.5493 Epoch 143/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0809 - accuracy: 0.5556 - val_loss: 1.0971 - val_accuracy: 0.5517 Epoch 144/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0800 - accuracy: 0.5545 - val_loss: 1.0970 - val_accuracy: 0.5484 Epoch 145/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0792 - accuracy: 0.5545 - val_loss: 1.0964 - val_accuracy: 0.5493 Epoch 146/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0785 - accuracy: 0.5572 - val_loss: 1.0956 - val_accuracy: 0.5517 Epoch 147/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0777 - accuracy: 0.5569 - val_loss: 1.0943 - val_accuracy: 0.5517 Epoch 148/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0771 - accuracy: 0.5586 - val_loss: 1.0937 - val_accuracy: 0.5542 Epoch 149/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0764 - accuracy: 0.5588 - val_loss: 1.0930 - val_accuracy: 0.5542 Epoch 150/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0759 - accuracy: 0.5586 - val_loss: 1.0926 - val_accuracy: 0.5599 Epoch 151/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0756 - accuracy: 0.5580 - val_loss: 1.0926 - val_accuracy: 0.5575 Epoch 152/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0750 - accuracy: 0.5577 - val_loss: 1.0926 - val_accuracy: 0.5567 Epoch 153/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0743 - accuracy: 0.5564 - val_loss: 1.0926 - val_accuracy: 0.5534 Epoch 154/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0736 - accuracy: 0.5569 - val_loss: 1.0918 - val_accuracy: 0.5525 Epoch 155/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0732 - accuracy: 0.5577 - val_loss: 1.0912 - val_accuracy: 0.5558 Epoch 156/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0727 - accuracy: 0.5567 - val_loss: 1.0906 - val_accuracy: 0.5567 Epoch 157/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0724 - accuracy: 0.5577 - val_loss: 1.0908 - val_accuracy: 0.5525 Epoch 158/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0720 - accuracy: 0.5553 - val_loss: 1.0906 - val_accuracy: 0.5534 Epoch 159/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0715 - accuracy: 0.5561 - val_loss: 1.0903 - val_accuracy: 0.5525 Epoch 160/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0710 - accuracy: 0.5556 - val_loss: 1.0897 - val_accuracy: 0.5534 Epoch 161/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0706 - accuracy: 0.5558 - val_loss: 1.0887 - val_accuracy: 0.5525 Epoch 162/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0704 - accuracy: 0.5577 - val_loss: 1.0883 - val_accuracy: 0.5542 Epoch 163/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0700 - accuracy: 0.5564 - val_loss: 1.0882 - val_accuracy: 0.5534 Epoch 164/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0697 - accuracy: 0.5553 - val_loss: 1.0880 - val_accuracy: 0.5558 Epoch 165/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0694 - accuracy: 0.5553 - val_loss: 1.0879 - val_accuracy: 0.5534 Epoch 166/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0693 - accuracy: 0.5558 - val_loss: 1.0880 - val_accuracy: 0.5550 Epoch 167/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0690 - accuracy: 0.5569 - val_loss: 1.0876 - val_accuracy: 0.5567 Epoch 168/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0685 - accuracy: 0.5550 - val_loss: 1.0871 - val_accuracy: 0.5575 Epoch 169/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0684 - accuracy: 0.5564 - val_loss: 1.0873 - val_accuracy: 0.5599 Epoch 170/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0679 - accuracy: 0.5558 - val_loss: 1.0867 - val_accuracy: 0.5567 Epoch 171/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0679 - accuracy: 0.5577 - val_loss: 1.0871 - val_accuracy: 0.5575 Epoch 172/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0673 - accuracy: 0.5575 - val_loss: 1.0865 - val_accuracy: 0.5542 Epoch 173/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0670 - accuracy: 0.5556 - val_loss: 1.0866 - val_accuracy: 0.5591 Epoch 174/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0665 - accuracy: 0.5539 - val_loss: 1.0862 - val_accuracy: 0.5583 Epoch 175/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0662 - accuracy: 0.5547 - val_loss: 1.0862 - val_accuracy: 0.5599 Epoch 176/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0659 - accuracy: 0.5556 - val_loss: 1.0857 - val_accuracy: 0.5583 Epoch 177/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0654 - accuracy: 0.5539 - val_loss: 1.0851 - val_accuracy: 0.5558 Epoch 178/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0652 - accuracy: 0.5542 - val_loss: 1.0841 - val_accuracy: 0.5575 Epoch 179/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0650 - accuracy: 0.5523 - val_loss: 1.0840 - val_accuracy: 0.5575 Epoch 180/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0647 - accuracy: 0.5534 - val_loss: 1.0834 - val_accuracy: 0.5534 Epoch 181/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0644 - accuracy: 0.5553 - val_loss: 1.0833 - val_accuracy: 0.5542 Epoch 182/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0641 - accuracy: 0.5534 - val_loss: 1.0832 - val_accuracy: 0.5542 Epoch 183/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0637 - accuracy: 0.5539 - val_loss: 1.0835 - val_accuracy: 0.5575 Epoch 184/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0636 - accuracy: 0.5550 - val_loss: 1.0839 - val_accuracy: 0.5599 Epoch 185/1000 8/8 [==============================] - 0s 8ms/step - loss: 1.0634 - accuracy: 0.5506 - val_loss: 1.0837 - val_accuracy: 0.5575 Epoch 186/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0634 - accuracy: 0.5501 - val_loss: 1.0826 - val_accuracy: 0.5517 Epoch 187/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0634 - accuracy: 0.5534 - val_loss: 1.0823 - val_accuracy: 0.5542 Epoch 188/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0631 - accuracy: 0.5517 - val_loss: 1.0820 - val_accuracy: 0.5534 Epoch 189/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0626 - accuracy: 0.5528 - val_loss: 1.0822 - val_accuracy: 0.5558 Epoch 190/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0626 - accuracy: 0.5495 - val_loss: 1.0824 - val_accuracy: 0.5575 Epoch 191/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0623 - accuracy: 0.5523 - val_loss: 1.0818 - val_accuracy: 0.5599 Epoch 192/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0621 - accuracy: 0.5542 - val_loss: 1.0817 - val_accuracy: 0.5575 Epoch 193/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0618 - accuracy: 0.5558 - val_loss: 1.0812 - val_accuracy: 0.5558 Epoch 194/1000 8/8 [==============================] - 0s 7ms/step - loss: 1.0619 - accuracy: 0.5553 - val_loss: 1.0812 - val_accuracy: 0.5575 Epoch 195/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0617 - accuracy: 0.5564 - val_loss: 1.0813 - val_accuracy: 0.5567 Epoch 196/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0614 - accuracy: 0.5547 - val_loss: 1.0810 - val_accuracy: 0.5624 Epoch 197/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0613 - accuracy: 0.5531 - val_loss: 1.0812 - val_accuracy: 0.5640 Epoch 198/1000 8/8 [==============================] - 0s 9ms/step - loss: 1.0613 - accuracy: 0.5536 - val_loss: 1.0817 - val_accuracy: 0.5649 Epoch 199/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0611 - accuracy: 0.5520 - val_loss: 1.0817 - val_accuracy: 0.5640 Epoch 200/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0610 - accuracy: 0.5509 - val_loss: 1.0815 - val_accuracy: 0.5632 Epoch 201/1000 8/8 [==============================] - 0s 5ms/step - loss: 1.0606 - accuracy: 0.5506 - val_loss: 1.0812 - val_accuracy: 0.5640 Epoch 202/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0606 - accuracy: 0.5487 - val_loss: 1.0814 - val_accuracy: 0.5599 Epoch 203/1000 8/8 [==============================] - 0s 6ms/step - loss: 1.0606 - accuracy: 0.5515 - val_loss: 1.0807 - val_accuracy: 0.5599 history_df = pd . DataFrame ( history . history ) history_df . loc [:, [ 'loss' , 'val_loss' ]] . plot () history_df . loc [:, [ 'accuracy' , 'val_accuracy' ]] . plot () print (( \"Best Validation Loss: {:0.4f} \" + \\ \" \\n Best Validation Accuracy: {:0.4f} \" ) \\ . format ( history_df [ 'val_loss' ] . min (), history_df [ 'val_accuracy' ] . max ())) Best Validation Loss: 1.0807 Best Validation Accuracy: 0.5649 # predict test set pred_probability = model . predict ( X_test_std ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score print ( classification_report ( y_test_std , predictions )) precision recall f1-score support 0 0.06 0.22 0.09 9 1 0.06 0.78 0.11 55 2 0.36 0.99 0.53 535 3 0.43 1.00 0.60 698 4 0.19 1.00 0.31 286 5 0.05 0.85 0.09 41 6 0.00 0.00 0.00 1 micro avg 0.26 0.98 0.41 1625 macro avg 0.16 0.69 0.25 1625 weighted avg 0.34 0.98 0.49 1625 samples avg 0.26 0.98 0.41 1625 /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result))","title":"\ud83c\udfcb\ufe0f Exercise 3: Multi-Class Classification"},{"location":"S2_Feed_Forward_Neural_Networks/#references","text":"Difference between a batch and an epoch Cross Entropy","title":"References"},{"location":"S3_Computer_Vision_I/","text":"General Applications of Neural Networks Session 3: Computer Vision Part 1 \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will turn our magnifying glass over to images. There are many application-centric subdomains of machine learning and this is one of them (another would be natural language processing). We can use the power of image related machine learning to replace, augment, or optimize what would typically be reserved for a humans. In this session we will: Use modern deep-learning networks using keras Design CNNs Learn about feature extraction in convolutional layers Learn about transfer learning Utilize data augmentation in the context of images images in this notebook borrowed from Ryan Holbrook 3.0 Preparing Environment and Importing Data \u00b6 back to top 3.0.1 Import Packages \u00b6 back to top import matplotlib.pyplot as plt from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Activation , Dropout , Flatten , Dense , Conv2D , MaxPooling2D from tensorflow.keras.callbacks import EarlyStopping import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing import image_dataset_from_directory import numpy as np from itertools import product def show_kernel ( kernel , label = True , digits = None , text_size = 28 ): # helper function borrowed from Ryan Holbrock # Format kernel kernel = np . array ( kernel ) if digits is not None : kernel = kernel . round ( digits ) # Plot kernel cmap = plt . get_cmap ( 'Blues_r' ) plt . imshow ( kernel , cmap = cmap ) rows , cols = kernel . shape thresh = ( kernel . max () + kernel . min ()) / 2 # Optionally, add value labels if label : for i , j in product ( range ( rows ), range ( cols )): val = kernel [ i , j ] color = cmap ( 0 ) if val > thresh else cmap ( 255 ) plt . text ( j , i , val , color = color , size = text_size , horizontalalignment = 'center' , verticalalignment = 'center' ) plt . xticks ([]) plt . yticks ([]) 3.0.2 Load Dataset + Segway Into Images \u00b6 back to top We're going to be working with a new kind of data structure today, an image . There are many ways to load images into python: matplotlib ( plt.imread() ), OpenCV ( cv2.imread() ), Pillow ( Image.open() ), scikit-image ( io.imread() ), tensorflow ( tf.io.read_file() and tf.io.decode_jpeg() ) . Let's give these a shot! # Sync your google drive folder from google.colab import drive drive . mount ( \"/content/drive\" ) Mounted at /content/drive # image read libraries from PIL import Image import cv2 import matplotlib.pyplot as plt import skimage from skimage import io ### YOU WILL CHANGE TO THE PATH WHERE YOU HAVE TECH_FUNDAMENTALS ### path_to_casting_data = '/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' image_path = technocast_train_path + '/ok_front/cast_ok_0_1.jpeg' # pyplot import matplotlib.pyplot as plt img1 = plt . imread ( image_path ) print ( type ( img1 )) print ( img1 . shape ) plt . imshow ( img1 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # cv2 import cv2 img2 = cv2 . imread ( image_path ) print ( type ( img2 )) print ( img2 . shape ) plt . imshow ( img2 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # PIL from PIL import Image img3 = Image . open ( image_path ) print ( type ( img3 )) print ( np . array ( img3 ) . shape ) #conv to array plt . imshow ( img3 ) plt . axis ( 'off' ) <class 'PIL.JpegImagePlugin.JpegImageFile'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # skimage import skimage img4 = skimage . io . imread ( image_path ) print ( type ( img4 )) print ( img4 . shape ) plt . imshow ( img4 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # tensorflow img5 = tf . io . read_file ( image_path ) img5 = tf . io . decode_jpeg ( img5 ) # conver to bw img5 = tf . image . rgb_to_grayscale ( img5 ) # optionally could convert to an array # img5 = np.asarray(img5) print ( type ( img5 )) print ( img5 . shape ) # drop the extra channel (3d squeezed into 2d, rgb to intensity/gray) print ( tf . squeeze ( img5 ) . shape ) plt . imshow ( tf . squeeze ( img5 ), cmap = 'gray' , vmin = 0 , vmax = 255 ) plt . axis ( 'off' ) <class 'tensorflow.python.framework.ops.EagerTensor'> (300, 300, 1) (300, 300) (-0.5, 299.5, 299.5, -0.5) \ud83c\udfcb\ufe0f Exercise 1: Loading Images \u00b6 Find 2 different images on the internet (any 2 of: jpg, png, and svg format). Load them into python as colored, and then also convert to grayscale using tensorflow and pyplot. Convert to grayscale using tf.image.rgb_to_grayscale for one of the images and np.dot() for the other. img_path2 = \"/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/squirtle.png\" # tensorflow ### YOUR CODE HERE ### #<my_var> = tf.io.read_file(<path_to_file>) #<my_var> = tf.io.decode_jpeg(<my_var) # extra channel (alpha channel for opacity, sometimes) #<my_var> = tf.image.rgb_to_grayscale(<my_var>[indexed_at_rgb_values]) # plt.imshow(tf.squeeze(my_var_so_that_it_is_reduced_to_one_chanel), cmap='some_scheme') img_path3 = \"/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/ghastly.jpg\" # pyplot ### YOUR CODE HERE ### # <my_var> = plt.imread(<path_to_file>) # extra channel (alpha channel for opacity, sometimes) # <my_var> = np.dot(<my_var>[indexed_at_rgb_values], rgb_weights) # plt.imshow(<my_var>, cmap='some_scheme') 3.1 The Classifier Structure \u00b6 Convolutional neural networks (CNNs, Convnets) take the gold for machine vision. We'll talk about what convolutional layers are in a moment. For now lets talk about the general structure of these types of neural networks. A CNN consists of a base and a head . The base is used to extract the relevant features from the image. It consits mainly of convolutional layers. The head is used to map those features to the classification task and mostly consits of dense layers. What is a visual feature? It is a relevant abstraction from the image data, often edges and shapes, that then form more abstract objects like eyes, ears or wheels and windows: note: this is an oversimplification but it gives the general idea. A classification NN will always have this phenomenon where, early in the network layers are learning features, and later in the network layers are appropriating those features to different classes. In a CNN this phenomenon is accentuated by the base-head dynamic. Given that the feature generation task can be very similar across images, often we use the base of a pretrained model. This strategy is called transfer learning and is extraordinarily powerful when dealing with small datasets! \"When doing transfer learning, it's generally not a good idea to retrain the entire base -- at least not without some care. The reason is that the random weights in the head will initially create large gradient updates, which propogate back into the base layers and destroy much of the pretraining. Using techniques known as fine tuning it's possible to further train the base on new data, but this requires some care to do well.\" -kaggle gurus 3.2 Convolutions, ReLU and Maximum Pooling \u00b6 back to top We'll now discuss the three heavy-weights for convolutional networks: convolutions, rectified linear units, and maximum pooling. You can think of these as the agents of three primary steps in a convolutional network: Filter an image down to features (convolution) Detect that the feature is present (ReLU) Condense the image to enhance the features (maximum pooling) These three steps are demonstrated in the following image: 3.2.1 The Convolution \u00b6 back to top The convolutional layer carries out the filtering step. We can create a convolution in keras like so: from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Conv2D ( filters = 64 , kernel_size = 3 ), # activation is None # More layers follow ]) Let's talk about the hyperparameters in Conv2D . The weights a CNN learns are primarily in its kernels . The kernels are a collection of arrays that are passed over the image to produce weighted sums. An example of a 3x3 kernel: As the kernel is swept over the image, it acts to accentuate certain features. For instance, some kernels will bring out vertical edges, some horizontal edges, others will accentuate gradients in a general sense. As we train our CNN, it learns what kernels are best for learning relevant features for the classification task. We set the number of kernels with the filters hyperparameter and the shape with kernel_size . The shape will usually be an odd number so that the kernel is oriented around one center pixel, but this is not a requirement. 3.2.2 Activations \u00b6 back to top The activations also called feature maps are the output from the kernels. We can see an example: Notice that the left and middle kernels are augmenting horizontal boundaries. 3.2.3 ReLU \u00b6 back to top The ReLU should be familiar by now. It appears here in CNNs to further isolate the presense of features in the feature maps. Remember that it sets anything below 0 to simply 0. In a way it is saying, anything that is unimportant, is equally unimportant. Let's see it in action: And, just as with dense layers, the ReLU allows us to create non-linear relationships within our network, something that we definitely want. \ud83c\udfcb\ufe0f Exercise 2: Experiment with Kernels \u00b6 back to top image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) plt . figure ( figsize = ( 6 , 6 )) plt . imshow ( tf . squeeze ( image ), cmap = 'gray' ) plt . axis ( 'off' ) plt . show (); import tensorflow as tf kernel = tf . constant ([ [ - 1 , - 1 , - 1 ], [ - 1 , 8 , - 1 ], [ - 1 , - 1 , - 1 ], ]) plt . figure ( figsize = ( 3 , 3 )) show_kernel ( kernel ) # Reformat for batch compatibility. image = tf . image . rgb_to_grayscale ( image ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) image = tf . expand_dims ( image , axis = 0 ) # Prep the kernel kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) kernel = tf . cast ( kernel , dtype = tf . float32 ) image_filter = tf . nn . conv2d ( input = image , filters = kernel , # we'll talk about strides and padding in the next session strides = 1 , padding = 'SAME' , ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_filter )) plt . axis ( 'off' ) plt . show (); And now the ReLU step: image_detect = tf . nn . relu ( image_filter ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_detect )) plt . axis ( 'off' ) plt . show (); We're going to wrap these steps into a function and simply play around with the kernel to see how it changes the feature mapping for this impellar. def apply_the_KERN ( kernel ): fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) image_path = technocast_train_path + '/ok_front/cast_ok_0_1.jpeg' image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) # Reformat for batch compatibility. image = tf . image . rgb_to_grayscale ( image ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) image = tf . expand_dims ( image , axis = 0 ) kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) kernel = tf . cast ( kernel , dtype = tf . float32 ) image_filter = tf . nn . conv2d ( input = image , filters = kernel , # we'll talk about strides and padding in the next session strides = 1 , padding = 'SAME' , ) ax [ 0 ] . imshow ( tf . squeeze ( image ), cmap = 'gray' ) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( tf . squeeze ( image_filter )) ax [ 1 ] . axis ( 'off' ) image_detect = tf . nn . relu ( image_filter ) ax [ 2 ] . imshow ( tf . squeeze ( image_detect )) ax [ 2 ] . axis ( 'off' ) For this exercise create 2-3 kernels and show how they accentuate different features: kernel = tf . constant ([ ### YOUR KERNEL HERE ### [ - 1 , - 1 , - 1 ], [ - 1 , 8 , - 1 ], [ - 1 , - 1 , - 1 ], ]) show_kernel ( kernel ) apply_the_KERN ( kernel ) notice anything with different patterns? What happens when the sum of the kernel is more than 0? more than 1? 3.2.4 Condense with Maximum Pooling \u00b6 back to top In this last step that we'll cover, we're going to condense the image in a way that accentuates the identified features. This is done with MaxPool2D layer in keras A MaxPool2D layer isn't much different from a Conv2D layer other than that, instead of taking a sum, the kernel (not really a kernel) just returns the maximum value. kernel_size is replaced with pool_size and there are no trainable weights (hence not a kernel). Features are indeed intensified by the max pooling, but another way to think of this is that carrying all those empty \"black\" pixels through the network is computationally expensive without adding much information. Max pooling is a quick and dirty way of consolidating the network and retaining only the most salient information. image_condense = tf . nn . pool ( input = image_detect , # image in the Detect step above window_shape = ( 2 , 2 ), pooling_type = 'MAX' , # we'll talk about strides and padding in the next session strides = ( 2 , 2 ), padding = 'SAME' , ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_condense )) plt . axis ( 'off' ) plt . show (); 3.3 Enrichment: Practice with Global Average Pooling \u00b6 back to top Usually at the end of the convolutional blocks, we use a Flatten() layer to convert the 2D arrays of feature maps to 1D to feed into the Dense layers of the network. Another method that has become popular is to use Global Average Pooling . With this schematic, each feature map is turned into an average. By this method, the head of the CNN now only has to make a classificaiton based on how turned on the feature maps are, indicated by the result of the global average pooling. We would implement this with keras like so: model = keras.Sequential([ pretrained_base, layers.GlobalAvgPool2D(), layers.Dense(1, activation='sigmoid'), ]) Resources \u00b6 CNN Explainer","title":"Computer Vision I"},{"location":"S3_Computer_Vision_I/#general-applications-of-neural-networks-session-3-computer-vision-part-1","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will turn our magnifying glass over to images. There are many application-centric subdomains of machine learning and this is one of them (another would be natural language processing). We can use the power of image related machine learning to replace, augment, or optimize what would typically be reserved for a humans. In this session we will: Use modern deep-learning networks using keras Design CNNs Learn about feature extraction in convolutional layers Learn about transfer learning Utilize data augmentation in the context of images images in this notebook borrowed from Ryan Holbrook","title":"General Applications of Neural Networks  Session 3: Computer Vision Part 1"},{"location":"S3_Computer_Vision_I/#30-preparing-environment-and-importing-data","text":"back to top","title":"3.0 Preparing Environment and Importing Data"},{"location":"S3_Computer_Vision_I/#301-import-packages","text":"back to top import matplotlib.pyplot as plt from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Activation , Dropout , Flatten , Dense , Conv2D , MaxPooling2D from tensorflow.keras.callbacks import EarlyStopping import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing import image_dataset_from_directory import numpy as np from itertools import product def show_kernel ( kernel , label = True , digits = None , text_size = 28 ): # helper function borrowed from Ryan Holbrock # Format kernel kernel = np . array ( kernel ) if digits is not None : kernel = kernel . round ( digits ) # Plot kernel cmap = plt . get_cmap ( 'Blues_r' ) plt . imshow ( kernel , cmap = cmap ) rows , cols = kernel . shape thresh = ( kernel . max () + kernel . min ()) / 2 # Optionally, add value labels if label : for i , j in product ( range ( rows ), range ( cols )): val = kernel [ i , j ] color = cmap ( 0 ) if val > thresh else cmap ( 255 ) plt . text ( j , i , val , color = color , size = text_size , horizontalalignment = 'center' , verticalalignment = 'center' ) plt . xticks ([]) plt . yticks ([])","title":"3.0.1 Import Packages"},{"location":"S3_Computer_Vision_I/#302-load-dataset-segway-into-images","text":"back to top We're going to be working with a new kind of data structure today, an image . There are many ways to load images into python: matplotlib ( plt.imread() ), OpenCV ( cv2.imread() ), Pillow ( Image.open() ), scikit-image ( io.imread() ), tensorflow ( tf.io.read_file() and tf.io.decode_jpeg() ) . Let's give these a shot! # Sync your google drive folder from google.colab import drive drive . mount ( \"/content/drive\" ) Mounted at /content/drive # image read libraries from PIL import Image import cv2 import matplotlib.pyplot as plt import skimage from skimage import io ### YOU WILL CHANGE TO THE PATH WHERE YOU HAVE TECH_FUNDAMENTALS ### path_to_casting_data = '/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' image_path = technocast_train_path + '/ok_front/cast_ok_0_1.jpeg' # pyplot import matplotlib.pyplot as plt img1 = plt . imread ( image_path ) print ( type ( img1 )) print ( img1 . shape ) plt . imshow ( img1 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # cv2 import cv2 img2 = cv2 . imread ( image_path ) print ( type ( img2 )) print ( img2 . shape ) plt . imshow ( img2 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # PIL from PIL import Image img3 = Image . open ( image_path ) print ( type ( img3 )) print ( np . array ( img3 ) . shape ) #conv to array plt . imshow ( img3 ) plt . axis ( 'off' ) <class 'PIL.JpegImagePlugin.JpegImageFile'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # skimage import skimage img4 = skimage . io . imread ( image_path ) print ( type ( img4 )) print ( img4 . shape ) plt . imshow ( img4 ) plt . axis ( 'off' ) <class 'numpy.ndarray'> (300, 300, 3) (-0.5, 299.5, 299.5, -0.5) # tensorflow img5 = tf . io . read_file ( image_path ) img5 = tf . io . decode_jpeg ( img5 ) # conver to bw img5 = tf . image . rgb_to_grayscale ( img5 ) # optionally could convert to an array # img5 = np.asarray(img5) print ( type ( img5 )) print ( img5 . shape ) # drop the extra channel (3d squeezed into 2d, rgb to intensity/gray) print ( tf . squeeze ( img5 ) . shape ) plt . imshow ( tf . squeeze ( img5 ), cmap = 'gray' , vmin = 0 , vmax = 255 ) plt . axis ( 'off' ) <class 'tensorflow.python.framework.ops.EagerTensor'> (300, 300, 1) (300, 300) (-0.5, 299.5, 299.5, -0.5)","title":"3.0.2 Load Dataset + Segway Into Images"},{"location":"S3_Computer_Vision_I/#exercise-1-loading-images","text":"Find 2 different images on the internet (any 2 of: jpg, png, and svg format). Load them into python as colored, and then also convert to grayscale using tensorflow and pyplot. Convert to grayscale using tf.image.rgb_to_grayscale for one of the images and np.dot() for the other. img_path2 = \"/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/squirtle.png\" # tensorflow ### YOUR CODE HERE ### #<my_var> = tf.io.read_file(<path_to_file>) #<my_var> = tf.io.decode_jpeg(<my_var) # extra channel (alpha channel for opacity, sometimes) #<my_var> = tf.image.rgb_to_grayscale(<my_var>[indexed_at_rgb_values]) # plt.imshow(tf.squeeze(my_var_so_that_it_is_reduced_to_one_chanel), cmap='some_scheme') img_path3 = \"/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/ghastly.jpg\" # pyplot ### YOUR CODE HERE ### # <my_var> = plt.imread(<path_to_file>) # extra channel (alpha channel for opacity, sometimes) # <my_var> = np.dot(<my_var>[indexed_at_rgb_values], rgb_weights) # plt.imshow(<my_var>, cmap='some_scheme')","title":"\ud83c\udfcb\ufe0f Exercise 1: Loading Images"},{"location":"S3_Computer_Vision_I/#31-the-classifier-structure","text":"Convolutional neural networks (CNNs, Convnets) take the gold for machine vision. We'll talk about what convolutional layers are in a moment. For now lets talk about the general structure of these types of neural networks. A CNN consists of a base and a head . The base is used to extract the relevant features from the image. It consits mainly of convolutional layers. The head is used to map those features to the classification task and mostly consits of dense layers. What is a visual feature? It is a relevant abstraction from the image data, often edges and shapes, that then form more abstract objects like eyes, ears or wheels and windows: note: this is an oversimplification but it gives the general idea. A classification NN will always have this phenomenon where, early in the network layers are learning features, and later in the network layers are appropriating those features to different classes. In a CNN this phenomenon is accentuated by the base-head dynamic. Given that the feature generation task can be very similar across images, often we use the base of a pretrained model. This strategy is called transfer learning and is extraordinarily powerful when dealing with small datasets! \"When doing transfer learning, it's generally not a good idea to retrain the entire base -- at least not without some care. The reason is that the random weights in the head will initially create large gradient updates, which propogate back into the base layers and destroy much of the pretraining. Using techniques known as fine tuning it's possible to further train the base on new data, but this requires some care to do well.\" -kaggle gurus","title":"3.1 The Classifier Structure"},{"location":"S3_Computer_Vision_I/#32-convolutions-relu-and-maximum-pooling","text":"back to top We'll now discuss the three heavy-weights for convolutional networks: convolutions, rectified linear units, and maximum pooling. You can think of these as the agents of three primary steps in a convolutional network: Filter an image down to features (convolution) Detect that the feature is present (ReLU) Condense the image to enhance the features (maximum pooling) These three steps are demonstrated in the following image:","title":"3.2 Convolutions, ReLU and Maximum Pooling"},{"location":"S3_Computer_Vision_I/#321-the-convolution","text":"back to top The convolutional layer carries out the filtering step. We can create a convolution in keras like so: from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Conv2D ( filters = 64 , kernel_size = 3 ), # activation is None # More layers follow ]) Let's talk about the hyperparameters in Conv2D . The weights a CNN learns are primarily in its kernels . The kernels are a collection of arrays that are passed over the image to produce weighted sums. An example of a 3x3 kernel: As the kernel is swept over the image, it acts to accentuate certain features. For instance, some kernels will bring out vertical edges, some horizontal edges, others will accentuate gradients in a general sense. As we train our CNN, it learns what kernels are best for learning relevant features for the classification task. We set the number of kernels with the filters hyperparameter and the shape with kernel_size . The shape will usually be an odd number so that the kernel is oriented around one center pixel, but this is not a requirement.","title":"3.2.1 The Convolution"},{"location":"S3_Computer_Vision_I/#322-activations","text":"back to top The activations also called feature maps are the output from the kernels. We can see an example: Notice that the left and middle kernels are augmenting horizontal boundaries.","title":"3.2.2 Activations"},{"location":"S3_Computer_Vision_I/#323-relu","text":"back to top The ReLU should be familiar by now. It appears here in CNNs to further isolate the presense of features in the feature maps. Remember that it sets anything below 0 to simply 0. In a way it is saying, anything that is unimportant, is equally unimportant. Let's see it in action: And, just as with dense layers, the ReLU allows us to create non-linear relationships within our network, something that we definitely want.","title":"3.2.3 ReLU"},{"location":"S3_Computer_Vision_I/#exercise-2-experiment-with-kernels","text":"back to top image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) plt . figure ( figsize = ( 6 , 6 )) plt . imshow ( tf . squeeze ( image ), cmap = 'gray' ) plt . axis ( 'off' ) plt . show (); import tensorflow as tf kernel = tf . constant ([ [ - 1 , - 1 , - 1 ], [ - 1 , 8 , - 1 ], [ - 1 , - 1 , - 1 ], ]) plt . figure ( figsize = ( 3 , 3 )) show_kernel ( kernel ) # Reformat for batch compatibility. image = tf . image . rgb_to_grayscale ( image ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) image = tf . expand_dims ( image , axis = 0 ) # Prep the kernel kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) kernel = tf . cast ( kernel , dtype = tf . float32 ) image_filter = tf . nn . conv2d ( input = image , filters = kernel , # we'll talk about strides and padding in the next session strides = 1 , padding = 'SAME' , ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_filter )) plt . axis ( 'off' ) plt . show (); And now the ReLU step: image_detect = tf . nn . relu ( image_filter ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_detect )) plt . axis ( 'off' ) plt . show (); We're going to wrap these steps into a function and simply play around with the kernel to see how it changes the feature mapping for this impellar. def apply_the_KERN ( kernel ): fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) image_path = technocast_train_path + '/ok_front/cast_ok_0_1.jpeg' image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) # Reformat for batch compatibility. image = tf . image . rgb_to_grayscale ( image ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) image = tf . expand_dims ( image , axis = 0 ) kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) kernel = tf . cast ( kernel , dtype = tf . float32 ) image_filter = tf . nn . conv2d ( input = image , filters = kernel , # we'll talk about strides and padding in the next session strides = 1 , padding = 'SAME' , ) ax [ 0 ] . imshow ( tf . squeeze ( image ), cmap = 'gray' ) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( tf . squeeze ( image_filter )) ax [ 1 ] . axis ( 'off' ) image_detect = tf . nn . relu ( image_filter ) ax [ 2 ] . imshow ( tf . squeeze ( image_detect )) ax [ 2 ] . axis ( 'off' ) For this exercise create 2-3 kernels and show how they accentuate different features: kernel = tf . constant ([ ### YOUR KERNEL HERE ### [ - 1 , - 1 , - 1 ], [ - 1 , 8 , - 1 ], [ - 1 , - 1 , - 1 ], ]) show_kernel ( kernel ) apply_the_KERN ( kernel ) notice anything with different patterns? What happens when the sum of the kernel is more than 0? more than 1?","title":"\ud83c\udfcb\ufe0f Exercise 2: Experiment with Kernels"},{"location":"S3_Computer_Vision_I/#324-condense-with-maximum-pooling","text":"back to top In this last step that we'll cover, we're going to condense the image in a way that accentuates the identified features. This is done with MaxPool2D layer in keras A MaxPool2D layer isn't much different from a Conv2D layer other than that, instead of taking a sum, the kernel (not really a kernel) just returns the maximum value. kernel_size is replaced with pool_size and there are no trainable weights (hence not a kernel). Features are indeed intensified by the max pooling, but another way to think of this is that carrying all those empty \"black\" pixels through the network is computationally expensive without adding much information. Max pooling is a quick and dirty way of consolidating the network and retaining only the most salient information. image_condense = tf . nn . pool ( input = image_detect , # image in the Detect step above window_shape = ( 2 , 2 ), pooling_type = 'MAX' , # we'll talk about strides and padding in the next session strides = ( 2 , 2 ), padding = 'SAME' , ) plt . figure ( figsize = ( 4 , 4 )) plt . imshow ( tf . squeeze ( image_condense )) plt . axis ( 'off' ) plt . show ();","title":"3.2.4 Condense with Maximum Pooling"},{"location":"S3_Computer_Vision_I/#33-enrichment-practice-with-global-average-pooling","text":"back to top Usually at the end of the convolutional blocks, we use a Flatten() layer to convert the 2D arrays of feature maps to 1D to feed into the Dense layers of the network. Another method that has become popular is to use Global Average Pooling . With this schematic, each feature map is turned into an average. By this method, the head of the CNN now only has to make a classificaiton based on how turned on the feature maps are, indicated by the result of the global average pooling. We would implement this with keras like so: model = keras.Sequential([ pretrained_base, layers.GlobalAvgPool2D(), layers.Dense(1, activation='sigmoid'), ])","title":"3.3 Enrichment: Practice with Global Average Pooling"},{"location":"S3_Computer_Vision_I/#resources","text":"CNN Explainer","title":"Resources"},{"location":"S4_Computer_Vision_II/","text":"General Applications of Neural Networks Session 4: Computer Vision Part 2 (Defect Detection Case Study) \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will continue with our exploration of CNNs. In the previous session we discussed three flagship layers for the CNN: convolution ReLU and maximum pooling. Here we'll discuss the sliding window, how to build your custom CNN, and data augmentation for images. images in this notebook borrowed from Ryan Holbrook 4.0 Preparing Environment and Importing Data \u00b6 back to top 4.0.1 Enabling and testing the GPU \u00b6 back to top First, you'll need to enable GPUs for the notebook: Navigate to Edit\u2192Notebook Settings select GPU from the Hardware Accelerator drop-down Next, we'll confirm that we can connect to the GPU with tensorflow: % tensorflow_version 2. x import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) Found GPU at: /device:GPU:0 4.0.2 Observe TensorFlow speedup on GPU relative to CPU \u00b6 back to top This example constructs a typical convolutional neural network layer over a random image and manually places the resulting ops on either the CPU or the GPU to compare execution speed. % tensorflow_version 2. x import tensorflow as tf import timeit device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : print ( ' \\n\\n This error most likely means that this notebook is not ' 'configured to use a GPU. Change this in Notebook Settings via the ' 'command palette (cmd/ctrl-shift-P) or the Edit menu. \\n\\n ' ) raise SystemError ( 'GPU device not found' ) def cpu (): with tf . device ( '/cpu:0' ): random_image_cpu = tf . random . normal (( 100 , 100 , 100 , 3 )) net_cpu = tf . keras . layers . Conv2D ( 32 , 7 )( random_image_cpu ) return tf . math . reduce_sum ( net_cpu ) def gpu (): with tf . device ( '/device:GPU:0' ): random_image_gpu = tf . random . normal (( 100 , 100 , 100 , 3 )) net_gpu = tf . keras . layers . Conv2D ( 32 , 7 )( random_image_gpu ) return tf . math . reduce_sum ( net_gpu ) # We run each op once to warm up; see: https://stackoverflow.com/a/45067900 cpu () gpu () # Run the op several times. print ( 'Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images ' '(batch x height x width x channel). Sum of ten runs.' ) print ( 'CPU (s):' ) cpu_time = timeit . timeit ( 'cpu()' , number = 10 , setup = \"from __main__ import cpu\" ) print ( cpu_time ) print ( 'GPU (s):' ) gpu_time = timeit . timeit ( 'gpu()' , number = 10 , setup = \"from __main__ import gpu\" ) print ( gpu_time ) print ( 'GPU speedup over CPU: {} x' . format ( int ( cpu_time / gpu_time ))) Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs. CPU (s): 2.8009356639999936 GPU (s): 0.03463296600000376 GPU speedup over CPU: 80x 4.0.3 Import Packages \u00b6 back to top import os import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.preprocessing import image_dataset_from_directory #importing required libraries from tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img , img_to_array from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Activation , Dropout , Flatten , Dense , Conv2D , MaxPooling2D , InputLayer from tensorflow.keras.callbacks import EarlyStopping from sklearn.metrics import classification_report , confusion_matrix 4.0.4 Load Dataset \u00b6 back to top We will actually take a beat here today. When we started building our ML frameworks, we simply wanted our data in a numpy array to feed it into our pipeline. At some point, especially when working with images, the data becomes too large to fit into memory. For this reason we need an alternative way to import our data. With the merger of keras/tf two popular frameworks became available, ImageDataGenerator and image_dataset_from_directory both under tf.keras.preprocessing.image . image_dataset_from_directory can sometimes be faster (tf origin) but ImageDataGenerator is a lot simpler to use and has on-the-fly data augmentation capability (keras). For a full comparison of methods visit this link # Sync your google drive folder from google.colab import drive drive . mount ( \"/content/drive\" ) Mounted at /content/drive 4.0.4.1 Loading Data with ImageDataGenerator \u00b6 back to top # full dataset can be attained from kaggle if you are interested # https://www.kaggle.com/ravirajsinh45/real-life-industrial-dataset-of-casting-product?select=casting_data path_to_casting_data = '/content/drive/MyDrive/courses/tech_fundamentals/TECH_FUNDAMENTALS/data/casting_data_class_practice' image_shape = ( 300 , 300 , 1 ) batch_size = 32 technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' image_gen = ImageDataGenerator ( rescale = 1 / 255 ) # normalize pixels to 0-1 #we're using keras inbuilt function to ImageDataGenerator so we # dont need to label all images into 0 and 1 print ( \"loading training set...\" ) train_set = image_gen . flow_from_directory ( technocast_train_path , target_size = image_shape [: 2 ], color_mode = \"grayscale\" , batch_size = batch_size , class_mode = 'binary' , shuffle = True ) print ( \"loading testing set...\" ) test_set = image_gen . flow_from_directory ( technocast_test_path , target_size = image_shape [: 2 ], color_mode = \"grayscale\" , batch_size = batch_size , class_mode = 'binary' , shuffle = False ) loading training set... Found 840 images belonging to 2 classes. loading testing set... Found 715 images belonging to 2 classes. 4.0.4.2 loading data with image_dataset_from_directory \u00b6 back to top This method should be approx 2x faster than ImageDataGenerator from tensorflow.keras.preprocessing import image_dataset_from_directory from tensorflow.data.experimental import AUTOTUNE path_to_casting_data = '/content/drive/MyDrive/courses/tech_fundamentals/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' # Load training and validation sets image_shape = ( 300 , 300 , 1 ) batch_size = 32 ds_train_ = image_dataset_from_directory ( technocast_train_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = True , ) ds_valid_ = image_dataset_from_directory ( technocast_test_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = False , ) train_set = ds_train_ . prefetch ( buffer_size = AUTOTUNE ) test_set = ds_valid_ . prefetch ( buffer_size = AUTOTUNE ) Found 840 files belonging to 2 classes. Found 715 files belonging to 2 classes. # view some images def_path = '/def_front/cast_def_0_1001.jpeg' ok_path = '/ok_front/cast_ok_0_1.jpeg' image_path = technocast_train_path + ok_path image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) plt . figure ( figsize = ( 6 , 6 )) plt . imshow ( tf . squeeze ( image ), cmap = 'gray' ) plt . axis ( 'off' ) plt . show (); 4.1 Sliding Window \u00b6 back to top The kernels we just reviewed, need to be swept or slid along the preceding layer. We call this a sliding window , the window being the kernel. What do you notice about the gif? One perhaps obvious observation is that you can't scoot all the way up to the border of the input layer, this is because the kernel defines operations around the centered pixel and so you bang up against the margin of the input array. We can change the behavior at the boundary with a padding hyperparameter. A second observation, is that the distance we move the kernel along in each step could be variable, we call this the stride . We will explore the affects of each of these. from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Conv2D ( filters = 64 , kernel_size = 3 , strides = 1 , padding = 'same' , activation = 'relu' ), layers . MaxPool2D ( pool_size = 2 , strides = 1 , padding = 'same' ) # More layers follow ]) 4.1.1 Stride \u00b6 back to top Stride defines the the step size we take with each kernel as it passes along the input array. The stride needs to be defined in both the horizontal and vertical dimensions. This animation shows a 2x2 stride The stride will often be 1 for CNNs, where we don't want to lose any important information. Maximum pooling layers will often have strides greater than 1, to better summarize/accentuate the relevant features/activations. If the stride is the same in both the horizontal and vertical directions, it can be set with a single number like strides=2 within keras. 4.1.2 Padding \u00b6 back to top Padding attempts to resolve our issue at the border: our kernel requires information surrounding the centered pixel, and at the border of the input array we don't have that information. What to do? We have a couple popular options within the keras framework. We can set padding='valid' and only slide the kernel to the edge of the input array. This has the drawback of feature maps shrinking in size as we pass through the NN. Another option is to set padding='same' what this will do is pad the input array with 0's, just enough of them to allow the feature map to be the same size as the input array. This is shown in the gif below: The downside of setting the padding to same will be that features at the edges of the image will be diluted. \ud83c\udfcb\ufe0f Exercise 1: Exploring Sliding Windows \u00b6 back to top from skimage import draw , transform from itertools import product # helper functions borrowed from Ryan Holbrook # https://mathformachines.com/ def circle ( size , val = None , r_shrink = 0 ): circle = np . zeros ([ size [ 0 ] + 1 , size [ 1 ] + 1 ]) rr , cc = draw . circle_perimeter ( size [ 0 ] // 2 , size [ 1 ] // 2 , radius = size [ 0 ] // 2 - r_shrink , shape = [ size [ 0 ] + 1 , size [ 1 ] + 1 ], ) if val is None : circle [ rr , cc ] = np . random . uniform ( size = circle . shape )[ rr , cc ] else : circle [ rr , cc ] = val circle = transform . resize ( circle , size , order = 0 ) return circle def show_kernel ( kernel , label = True , digits = None , text_size = 28 ): # Format kernel kernel = np . array ( kernel ) if digits is not None : kernel = kernel . round ( digits ) # Plot kernel cmap = plt . get_cmap ( 'Blues_r' ) plt . imshow ( kernel , cmap = cmap ) rows , cols = kernel . shape thresh = ( kernel . max () + kernel . min ()) / 2 # Optionally, add value labels if label : for i , j in product ( range ( rows ), range ( cols )): val = kernel [ i , j ] color = cmap ( 0 ) if val > thresh else cmap ( 255 ) plt . text ( j , i , val , color = color , size = text_size , horizontalalignment = 'center' , verticalalignment = 'center' ) plt . xticks ([]) plt . yticks ([]) def show_extraction ( image , kernel , conv_stride = 1 , conv_padding = 'valid' , activation = 'relu' , pool_size = 2 , pool_stride = 2 , pool_padding = 'same' , figsize = ( 10 , 10 ), subplot_shape = ( 2 , 2 ), ops = [ 'Input' , 'Filter' , 'Detect' , 'Condense' ], gamma = 1.0 ): # Create Layers model = tf . keras . Sequential ([ tf . keras . layers . Conv2D ( filters = 1 , kernel_size = kernel . shape , strides = conv_stride , padding = conv_padding , use_bias = False , input_shape = image . shape , ), tf . keras . layers . Activation ( activation ), tf . keras . layers . MaxPool2D ( pool_size = pool_size , strides = pool_stride , padding = pool_padding , ), ]) layer_filter , layer_detect , layer_condense = model . layers kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) layer_filter . set_weights ([ kernel ]) # Format for TF image = tf . expand_dims ( image , axis = 0 ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) # Extract Feature image_filter = layer_filter ( image ) image_detect = layer_detect ( image_filter ) image_condense = layer_condense ( image_detect ) images = {} if 'Input' in ops : images . update ({ 'Input' : ( image , 1.0 )}) if 'Filter' in ops : images . update ({ 'Filter' : ( image_filter , 1.0 )}) if 'Detect' in ops : images . update ({ 'Detect' : ( image_detect , gamma )}) if 'Condense' in ops : images . update ({ 'Condense' : ( image_condense , gamma )}) # Plot plt . figure ( figsize = figsize ) for i , title in enumerate ( ops ): image , gamma = images [ title ] plt . subplot ( * subplot_shape , i + 1 ) plt . imshow ( tf . image . adjust_gamma ( tf . squeeze ( image ), gamma )) plt . axis ( 'off' ) plt . title ( title ) Create an image and kernel: import tensorflow as tf import matplotlib.pyplot as plt plt . rc ( 'figure' , autolayout = True ) plt . rc ( 'axes' , labelweight = 'bold' , labelsize = 'large' , titleweight = 'bold' , titlesize = 18 , titlepad = 10 ) plt . rc ( 'image' , cmap = 'magma' ) image = circle ([ 64 , 64 ], val = 1.0 , r_shrink = 3 ) image = tf . reshape ( image , [ * image . shape , 1 ]) # Bottom sobel kernel = tf . constant ( [[ - 1 , - 2 , - 1 ], [ 0 , 0 , 0 ], [ 1 , 2 , 1 ]], ) show_kernel ( kernel ) What do we think this kernel is meant to detect for? We will apply our kernel with a 1x1 stride and our max pooling with a 2x2 stride and pool size of 2. show_extraction ( image , kernel , # Window parameters conv_stride = 1 , pool_size = 2 , pool_stride = 2 , subplot_shape = ( 1 , 4 ), figsize = ( 14 , 6 ), ) Works ok! what about a higher conv stride? show_extraction ( image , kernel , # Window parameters conv_stride = 3 , pool_size = 2 , pool_stride = 2 , subplot_shape = ( 1 , 4 ), figsize = ( 14 , 6 ), ) Looks like we lost a bit of information! Sometimes published models will use a larger kernel and stride in the initial layer to produce large-scale features early on in the network without losing too much information (ResNet50 uses 7x7 kernels with a stride of 2). For now, without having much experience it's safe to set conv strides to 1. Take a moment here with the given kernel and explore different settings for applying both the kernel and the max_pool conv_stride=YOUR_VALUE, # condenses pixels pool_size=YOUR_VALUE, pool_stride=YOUR_VALUE, # condenses pixels Given a total condensation of 8 (I'm taking condensation to mean conv_stride x pool_stride ). what do you think is the best combination of values for conv_stride, pool_size, and pool_stride ? 4.2 Custom CNN \u00b6 back to top As we move through the network, small-scale features (lines, edges, etc.) turn to large-scale features (shapes, eyes, ears, etc). We call these blocks of convolution, ReLU, and max pool convolutional blocks and they are the low level modular framework we work with. By this means, the CNN is able to design it's own features, ones suited for the classification or regression task at hand. We will design a custom CNN for the Casting Defect Detection Dataset. In the following I'm going to double the filter size after the first block. This is a common pattern as the max pooling layers forces us in the opposite direction. #Creating model model = Sequential () model . add ( InputLayer ( input_shape = ( image_shape ))) model . add ( Conv2D ( filters = 8 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 224 )) model . add ( Activation ( 'relu' )) # Last layer model . add ( Dense ( 1 )) model . add ( Activation ( 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'binary_accuracy' ]) early_stop = EarlyStopping ( monitor = 'val_loss' , patience = 5 , restore_best_weights = True ,) # with CPU + ImageDataGenerator runs for about 40 minutes (5 epochs) # with GPU + image_dataset_from_directory runs for about 4 minutes (16 epochs) with tf . device ( '/device:GPU:0' ): results = model . fit ( train_set , epochs = 20 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/20 27/27 [==============================] - 224s 7s/step - loss: 68.2100 - binary_accuracy: 0.5714 - val_loss: 0.8023 - val_binary_accuracy: 0.5343 Epoch 2/20 27/27 [==============================] - 3s 86ms/step - loss: 0.5720 - binary_accuracy: 0.7048 - val_loss: 0.5140 - val_binary_accuracy: 0.7399 Epoch 3/20 27/27 [==============================] - 3s 82ms/step - loss: 0.4475 - binary_accuracy: 0.7881 - val_loss: 0.4868 - val_binary_accuracy: 0.7524 Epoch 4/20 27/27 [==============================] - 3s 84ms/step - loss: 0.4018 - binary_accuracy: 0.8202 - val_loss: 0.5074 - val_binary_accuracy: 0.7469 Epoch 5/20 27/27 [==============================] - 3s 87ms/step - loss: 0.3237 - binary_accuracy: 0.8631 - val_loss: 0.4168 - val_binary_accuracy: 0.8112 Epoch 6/20 27/27 [==============================] - 3s 82ms/step - loss: 0.1998 - binary_accuracy: 0.9333 - val_loss: 0.3857 - val_binary_accuracy: 0.8322 Epoch 7/20 27/27 [==============================] - 3s 83ms/step - loss: 0.1086 - binary_accuracy: 0.9738 - val_loss: 0.2786 - val_binary_accuracy: 0.8853 Epoch 8/20 27/27 [==============================] - 3s 83ms/step - loss: 0.0560 - binary_accuracy: 0.9929 - val_loss: 0.3389 - val_binary_accuracy: 0.8615 Epoch 9/20 27/27 [==============================] - 3s 85ms/step - loss: 0.0345 - binary_accuracy: 0.9952 - val_loss: 0.2035 - val_binary_accuracy: 0.9175 Epoch 10/20 27/27 [==============================] - 3s 85ms/step - loss: 0.0772 - binary_accuracy: 0.9762 - val_loss: 0.2724 - val_binary_accuracy: 0.8979 Epoch 11/20 27/27 [==============================] - 3s 87ms/step - loss: 0.0336 - binary_accuracy: 0.9976 - val_loss: 0.4880 - val_binary_accuracy: 0.8420 Epoch 12/20 27/27 [==============================] - 3s 84ms/step - loss: 0.0387 - binary_accuracy: 0.9881 - val_loss: 0.2376 - val_binary_accuracy: 0.9105 Epoch 13/20 27/27 [==============================] - 3s 86ms/step - loss: 0.0172 - binary_accuracy: 0.9988 - val_loss: 0.2523 - val_binary_accuracy: 0.9133 Epoch 14/20 27/27 [==============================] - 3s 88ms/step - loss: 0.0045 - binary_accuracy: 1.0000 - val_loss: 0.3099 - val_binary_accuracy: 0.9021 # model.save('inspection_of_casting_products.h5') 4.2.1 Evaluate Model \u00b6 back to top # model.load_weights('inspection_of_casting_products.h5') losses = pd . DataFrame ( results . history ) # losses.to_csv('history_simple_model.csv', index=False) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) losses [[ 'loss' , 'val_loss' ]] . plot ( ax = ax [ 0 ]) losses [[ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot ( ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f20b425fa50> # predict test set pred_probability = model . predict ( test_set ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score # test_set.classes to get images from ImageDataGenerator # for image_dataset_from_directory we have to do a little gymnastics # to get the labels labels = np . array ([]) for x , y in ds_valid_ : labels = np . concatenate ([ labels , tf . squeeze ( y . numpy ()) . numpy ()]) print ( classification_report ( labels , predictions )) precision recall f1-score support 0.0 0.97 0.90 0.93 453 1.0 0.84 0.95 0.89 262 accuracy 0.92 715 macro avg 0.91 0.93 0.91 715 weighted avg 0.92 0.92 0.92 715 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( labels , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bc8bd150> 4.3 Data Augmentation \u00b6 back to top Alright, alright, alright. We've done pretty good making our CNN model. But let's see if we can make it even better. There's a last trick we'll cover here in regard to image classifiers. We're going to perturb the input images in such a way as to create a pseudo-larger dataset. With any machine learning model, the more relevant training data we give the model, the better. The key here is relevant training data. We can easily do this with images so long as we do not change the class of the image. For example, in the small plot below, we are changing contrast, hue, rotation, and doing other things to the image of a car; and this is okay because it does not change the classification from a car to, say, a truck. Typically when we do data augmentation for images, we do them online , i.e. during training. Recall that we train in batches (or minibatches) with CNNs. An example of a minibatch then, might be the small multiples plot below. by varying the images in this way, the model always sees slightly new data, and becomes a more robust model. Remember that the caveat is that we can't muddle the relevant classification of the image. Sometimes the best way to see if data augmentation will be helpful is to just try it and see! from tensorflow.keras.layers.experimental import preprocessing #Creating model model = Sequential () model . add ( preprocessing . RandomFlip ( 'horizontal' )), # flip left-to-right model . add ( preprocessing . RandomFlip ( 'vertical' )), # flip upside-down model . add ( preprocessing . RandomContrast ( 0.5 )), # contrast change by up to 50% model . add ( Conv2D ( filters = 8 , kernel_size = ( 3 , 3 ), input_shape = image_shape , activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 224 )) model . add ( Activation ( 'relu' )) # Last layer model . add ( Dense ( 1 )) model . add ( Activation ( 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'binary_accuracy' ]) early_stop = EarlyStopping ( monitor = 'val_loss' , patience = 5 , restore_best_weights = True ,) results = model . fit ( train_set , epochs = 30 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/30 27/27 [==============================] - 3s 89ms/step - loss: 63.6867 - binary_accuracy: 0.5500 - val_loss: 1.4445 - val_binary_accuracy: 0.5888 Epoch 2/30 27/27 [==============================] - 3s 87ms/step - loss: 1.5593 - binary_accuracy: 0.6036 - val_loss: 1.2496 - val_binary_accuracy: 0.6573 Epoch 3/30 27/27 [==============================] - 3s 86ms/step - loss: 1.0772 - binary_accuracy: 0.6869 - val_loss: 0.4536 - val_binary_accuracy: 0.8014 Epoch 4/30 27/27 [==============================] - 3s 85ms/step - loss: 0.8275 - binary_accuracy: 0.7095 - val_loss: 0.3635 - val_binary_accuracy: 0.8336 Epoch 5/30 27/27 [==============================] - 3s 86ms/step - loss: 0.5000 - binary_accuracy: 0.7821 - val_loss: 0.3140 - val_binary_accuracy: 0.8643 Epoch 6/30 27/27 [==============================] - 3s 86ms/step - loss: 0.4402 - binary_accuracy: 0.8214 - val_loss: 0.2672 - val_binary_accuracy: 0.8783 Epoch 7/30 27/27 [==============================] - 3s 85ms/step - loss: 0.3306 - binary_accuracy: 0.8524 - val_loss: 0.9402 - val_binary_accuracy: 0.6825 Epoch 8/30 27/27 [==============================] - 3s 86ms/step - loss: 1.1711 - binary_accuracy: 0.7143 - val_loss: 0.4291 - val_binary_accuracy: 0.8252 Epoch 9/30 27/27 [==============================] - 3s 86ms/step - loss: 0.3464 - binary_accuracy: 0.8583 - val_loss: 0.5895 - val_binary_accuracy: 0.7832 Epoch 10/30 27/27 [==============================] - 3s 85ms/step - loss: 0.3010 - binary_accuracy: 0.8738 - val_loss: 0.4319 - val_binary_accuracy: 0.8196 Epoch 11/30 27/27 [==============================] - 3s 89ms/step - loss: 0.2554 - binary_accuracy: 0.8893 - val_loss: 0.2068 - val_binary_accuracy: 0.9091 Epoch 12/30 27/27 [==============================] - 3s 86ms/step - loss: 0.1639 - binary_accuracy: 0.9512 - val_loss: 0.1869 - val_binary_accuracy: 0.9189 Epoch 13/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1728 - binary_accuracy: 0.9298 - val_loss: 0.1441 - val_binary_accuracy: 0.9441 Epoch 14/30 27/27 [==============================] - 3s 85ms/step - loss: 0.1314 - binary_accuracy: 0.9536 - val_loss: 0.1434 - val_binary_accuracy: 0.9455 Epoch 15/30 27/27 [==============================] - 3s 85ms/step - loss: 0.1620 - binary_accuracy: 0.9512 - val_loss: 0.2196 - val_binary_accuracy: 0.9091 Epoch 16/30 27/27 [==============================] - 3s 87ms/step - loss: 0.1345 - binary_accuracy: 0.9512 - val_loss: 0.1243 - val_binary_accuracy: 0.9538 Epoch 17/30 27/27 [==============================] - 3s 86ms/step - loss: 0.3751 - binary_accuracy: 0.8500 - val_loss: 0.6976 - val_binary_accuracy: 0.7580 Epoch 18/30 27/27 [==============================] - 3s 86ms/step - loss: 0.2022 - binary_accuracy: 0.9214 - val_loss: 0.1656 - val_binary_accuracy: 0.9231 Epoch 19/30 27/27 [==============================] - 3s 84ms/step - loss: 0.1082 - binary_accuracy: 0.9571 - val_loss: 0.1395 - val_binary_accuracy: 0.9371 Epoch 20/30 27/27 [==============================] - 3s 84ms/step - loss: 0.0978 - binary_accuracy: 0.9643 - val_loss: 0.1182 - val_binary_accuracy: 0.9441 Epoch 21/30 27/27 [==============================] - 3s 88ms/step - loss: 0.1174 - binary_accuracy: 0.9583 - val_loss: 0.1207 - val_binary_accuracy: 0.9469 Epoch 22/30 27/27 [==============================] - 3s 87ms/step - loss: 0.0898 - binary_accuracy: 0.9774 - val_loss: 0.1390 - val_binary_accuracy: 0.9413 Epoch 23/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1028 - binary_accuracy: 0.9607 - val_loss: 0.1150 - val_binary_accuracy: 0.9552 Epoch 24/30 27/27 [==============================] - 3s 86ms/step - loss: 0.1146 - binary_accuracy: 0.9583 - val_loss: 0.1078 - val_binary_accuracy: 0.9650 Epoch 25/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1007 - binary_accuracy: 0.9643 - val_loss: 0.0802 - val_binary_accuracy: 0.9692 Epoch 26/30 27/27 [==============================] - 3s 88ms/step - loss: 0.0882 - binary_accuracy: 0.9750 - val_loss: 0.0755 - val_binary_accuracy: 0.9692 Epoch 27/30 27/27 [==============================] - 3s 89ms/step - loss: 0.0884 - binary_accuracy: 0.9750 - val_loss: 0.1813 - val_binary_accuracy: 0.9385 Epoch 28/30 27/27 [==============================] - 3s 85ms/step - loss: 0.0908 - binary_accuracy: 0.9631 - val_loss: 0.1431 - val_binary_accuracy: 0.9385 Epoch 29/30 27/27 [==============================] - 3s 87ms/step - loss: 0.0576 - binary_accuracy: 0.9833 - val_loss: 0.0920 - val_binary_accuracy: 0.9664 Epoch 30/30 27/27 [==============================] - 3s 87ms/step - loss: 0.1199 - binary_accuracy: 0.9500 - val_loss: 0.3270 - val_binary_accuracy: 0.8769 4.3.1 Evaluate Model \u00b6 back to top losses = pd . DataFrame ( results . history ) # losses.to_csv('history_augment_model.csv', index=False) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) losses [[ 'loss' , 'val_loss' ]] . plot ( ax = ax [ 0 ]) losses [[ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot ( ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bbbc59d0> # predict test set pred_probability = model . predict ( test_set ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score # test_set.classes to get images from ImageDataGenerator # for image_dataset_from_directory we have to do a little gymnastics # to get the labels labels = np . array ([]) for x , y in ds_valid_ : labels = np . concatenate ([ labels , tf . squeeze ( y . numpy ()) . numpy ()]) print ( classification_report ( labels , predictions )) precision recall f1-score support 0.0 1.00 0.81 0.89 453 1.0 0.75 1.00 0.86 262 accuracy 0.88 715 macro avg 0.87 0.90 0.87 715 weighted avg 0.91 0.88 0.88 715 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( labels , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bda35890> \ud83c\udfcb\ufe0f Exercise 2: Image Preprocessing Layers \u00b6 back to top These layers apply random augmentation transforms to a batch of images. They are only active during training. You can visit the documentation here RandomCrop layer RandomFlip layer RandomTranslation layer RandomRotation layer RandomZoom layer RandomHeight layer RandomWidth layer Use any combination of random augmentation transforms and retrain your model. Can you get a higher val performance? you may need to increase your epochs. # code cell for exercise 4.3.2 4.4 Transfer Learning \u00b6 back to top Transfer learning with EfficientNet from tensorflow.keras.preprocessing import image_dataset_from_directory from tensorflow.data.experimental import AUTOTUNE path_to_casting_data = '/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' # Load training and validation sets image_shape = ( 300 , 300 , 3 ) batch_size = 32 ds_train_ = image_dataset_from_directory ( technocast_train_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = True , ) ds_valid_ = image_dataset_from_directory ( technocast_test_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = False , ) train_set = ds_train_ . prefetch ( buffer_size = AUTOTUNE ) test_set = ds_valid_ . prefetch ( buffer_size = AUTOTUNE ) Found 840 files belonging to 2 classes. Found 715 files belonging to 2 classes. def build_model ( image_shape ): input = tf . keras . layers . Input ( shape = ( image_shape )) # include_top = False will take of the last dense layer used for classification model = tf . keras . applications . EfficientNetB3 ( include_top = False , input_tensor = input , weights = \"imagenet\" ) # Freeze the pretrained weights model . trainable = False # now we have to rebuild the top x = tf . keras . layers . GlobalAveragePooling2D ( name = \"avg_pool\" )( model . output ) x = tf . keras . layers . BatchNormalization ()( x ) top_dropout_rate = 0.2 x = tf . keras . layers . Dropout ( top_dropout_rate , name = \"top_dropout\" )( x ) # use num-nodes = 1 for binary, class # for multiclass output = tf . keras . layers . Dense ( 1 , activation = \"softmax\" , name = \"pred\" )( x ) # Compile model = tf . keras . Model ( input , output , name = \"EfficientNet\" ) model . compile ( optimizer = 'adam' , loss = \"binary_crossentropy\" , metrics = [ \"binary_accuracy\" ]) return model model = build_model ( image_shape ) with tf . device ( '/device:GPU:0' ): results = model . fit ( train_set , epochs = 20 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/20 WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). 27/27 [==============================] - ETA: 0s - loss: 0.4457 - binary_accuracy: 0.4905WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). 27/27 [==============================] - 20s 442ms/step - loss: 0.4457 - binary_accuracy: 0.4905 - val_loss: 0.4851 - val_binary_accuracy: 0.3664 Epoch 2/20 27/27 [==============================] - 11s 381ms/step - loss: 0.1878 - binary_accuracy: 0.4905 - val_loss: 0.3930 - val_binary_accuracy: 0.3664 Epoch 3/20 27/27 [==============================] - 11s 384ms/step - loss: 0.1816 - binary_accuracy: 0.4905 - val_loss: 0.3407 - val_binary_accuracy: 0.3664 Epoch 4/20 27/27 [==============================] - 11s 378ms/step - loss: 0.1394 - binary_accuracy: 0.4905 - val_loss: 0.2971 - val_binary_accuracy: 0.3664 Epoch 5/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0982 - binary_accuracy: 0.4905 - val_loss: 0.2490 - val_binary_accuracy: 0.3664 Epoch 6/20 27/27 [==============================] - 11s 376ms/step - loss: 0.1032 - binary_accuracy: 0.4905 - val_loss: 0.2130 - val_binary_accuracy: 0.3664 Epoch 7/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0801 - binary_accuracy: 0.4905 - val_loss: 0.1846 - val_binary_accuracy: 0.3664 Epoch 8/20 27/27 [==============================] - 11s 383ms/step - loss: 0.0806 - binary_accuracy: 0.4905 - val_loss: 0.1509 - val_binary_accuracy: 0.3664 Epoch 9/20 27/27 [==============================] - 11s 379ms/step - loss: 0.0736 - binary_accuracy: 0.4905 - val_loss: 0.1263 - val_binary_accuracy: 0.3664 Epoch 10/20 27/27 [==============================] - 11s 379ms/step - loss: 0.0916 - binary_accuracy: 0.4905 - val_loss: 0.1008 - val_binary_accuracy: 0.3664 Epoch 11/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0618 - binary_accuracy: 0.4905 - val_loss: 0.0886 - val_binary_accuracy: 0.3664 Epoch 12/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0843 - binary_accuracy: 0.4905 - val_loss: 0.0728 - val_binary_accuracy: 0.3664 Epoch 13/20 27/27 [==============================] - 11s 383ms/step - loss: 0.0741 - binary_accuracy: 0.4905 - val_loss: 0.0593 - val_binary_accuracy: 0.3664 Epoch 14/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0637 - binary_accuracy: 0.4905 - val_loss: 0.0535 - val_binary_accuracy: 0.3664 Epoch 15/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0657 - binary_accuracy: 0.4905 - val_loss: 0.0491 - val_binary_accuracy: 0.3664 Epoch 16/20 27/27 [==============================] - 11s 375ms/step - loss: 0.0653 - binary_accuracy: 0.4905 - val_loss: 0.0424 - val_binary_accuracy: 0.3664 Epoch 17/20 27/27 [==============================] - 11s 378ms/step - loss: 0.0451 - binary_accuracy: 0.4905 - val_loss: 0.0394 - val_binary_accuracy: 0.3664 Epoch 18/20 27/27 [==============================] - 11s 382ms/step - loss: 0.0667 - binary_accuracy: 0.4905 - val_loss: 0.0345 - val_binary_accuracy: 0.3664 Epoch 19/20 27/27 [==============================] - 11s 382ms/step - loss: 0.0541 - binary_accuracy: 0.4905 - val_loss: 0.0351 - val_binary_accuracy: 0.3664 Epoch 20/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0353 - binary_accuracy: 0.4905 - val_loss: 0.0365 - val_binary_accuracy: 0.3664","title":"Computer Vision II"},{"location":"S4_Computer_Vision_II/#general-applications-of-neural-networks-session-4-computer-vision-part-2-defect-detection-case-study","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session we will continue with our exploration of CNNs. In the previous session we discussed three flagship layers for the CNN: convolution ReLU and maximum pooling. Here we'll discuss the sliding window, how to build your custom CNN, and data augmentation for images. images in this notebook borrowed from Ryan Holbrook","title":"General Applications of Neural Networks  Session 4: Computer Vision Part 2 (Defect Detection Case Study)"},{"location":"S4_Computer_Vision_II/#40-preparing-environment-and-importing-data","text":"back to top","title":"4.0 Preparing Environment and Importing Data"},{"location":"S4_Computer_Vision_II/#401-enabling-and-testing-the-gpu","text":"back to top First, you'll need to enable GPUs for the notebook: Navigate to Edit\u2192Notebook Settings select GPU from the Hardware Accelerator drop-down Next, we'll confirm that we can connect to the GPU with tensorflow: % tensorflow_version 2. x import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) Found GPU at: /device:GPU:0","title":"4.0.1 Enabling and testing the GPU"},{"location":"S4_Computer_Vision_II/#402-observe-tensorflow-speedup-on-gpu-relative-to-cpu","text":"back to top This example constructs a typical convolutional neural network layer over a random image and manually places the resulting ops on either the CPU or the GPU to compare execution speed. % tensorflow_version 2. x import tensorflow as tf import timeit device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : print ( ' \\n\\n This error most likely means that this notebook is not ' 'configured to use a GPU. Change this in Notebook Settings via the ' 'command palette (cmd/ctrl-shift-P) or the Edit menu. \\n\\n ' ) raise SystemError ( 'GPU device not found' ) def cpu (): with tf . device ( '/cpu:0' ): random_image_cpu = tf . random . normal (( 100 , 100 , 100 , 3 )) net_cpu = tf . keras . layers . Conv2D ( 32 , 7 )( random_image_cpu ) return tf . math . reduce_sum ( net_cpu ) def gpu (): with tf . device ( '/device:GPU:0' ): random_image_gpu = tf . random . normal (( 100 , 100 , 100 , 3 )) net_gpu = tf . keras . layers . Conv2D ( 32 , 7 )( random_image_gpu ) return tf . math . reduce_sum ( net_gpu ) # We run each op once to warm up; see: https://stackoverflow.com/a/45067900 cpu () gpu () # Run the op several times. print ( 'Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images ' '(batch x height x width x channel). Sum of ten runs.' ) print ( 'CPU (s):' ) cpu_time = timeit . timeit ( 'cpu()' , number = 10 , setup = \"from __main__ import cpu\" ) print ( cpu_time ) print ( 'GPU (s):' ) gpu_time = timeit . timeit ( 'gpu()' , number = 10 , setup = \"from __main__ import gpu\" ) print ( gpu_time ) print ( 'GPU speedup over CPU: {} x' . format ( int ( cpu_time / gpu_time ))) Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs. CPU (s): 2.8009356639999936 GPU (s): 0.03463296600000376 GPU speedup over CPU: 80x","title":"4.0.2 Observe TensorFlow speedup on GPU relative to CPU"},{"location":"S4_Computer_Vision_II/#403-import-packages","text":"back to top import os import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.preprocessing import image_dataset_from_directory #importing required libraries from tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img , img_to_array from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Activation , Dropout , Flatten , Dense , Conv2D , MaxPooling2D , InputLayer from tensorflow.keras.callbacks import EarlyStopping from sklearn.metrics import classification_report , confusion_matrix","title":"4.0.3 Import Packages"},{"location":"S4_Computer_Vision_II/#404-load-dataset","text":"back to top We will actually take a beat here today. When we started building our ML frameworks, we simply wanted our data in a numpy array to feed it into our pipeline. At some point, especially when working with images, the data becomes too large to fit into memory. For this reason we need an alternative way to import our data. With the merger of keras/tf two popular frameworks became available, ImageDataGenerator and image_dataset_from_directory both under tf.keras.preprocessing.image . image_dataset_from_directory can sometimes be faster (tf origin) but ImageDataGenerator is a lot simpler to use and has on-the-fly data augmentation capability (keras). For a full comparison of methods visit this link # Sync your google drive folder from google.colab import drive drive . mount ( \"/content/drive\" ) Mounted at /content/drive","title":"4.0.4 Load Dataset"},{"location":"S4_Computer_Vision_II/#4041-loading-data-with-imagedatagenerator","text":"back to top # full dataset can be attained from kaggle if you are interested # https://www.kaggle.com/ravirajsinh45/real-life-industrial-dataset-of-casting-product?select=casting_data path_to_casting_data = '/content/drive/MyDrive/courses/tech_fundamentals/TECH_FUNDAMENTALS/data/casting_data_class_practice' image_shape = ( 300 , 300 , 1 ) batch_size = 32 technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' image_gen = ImageDataGenerator ( rescale = 1 / 255 ) # normalize pixels to 0-1 #we're using keras inbuilt function to ImageDataGenerator so we # dont need to label all images into 0 and 1 print ( \"loading training set...\" ) train_set = image_gen . flow_from_directory ( technocast_train_path , target_size = image_shape [: 2 ], color_mode = \"grayscale\" , batch_size = batch_size , class_mode = 'binary' , shuffle = True ) print ( \"loading testing set...\" ) test_set = image_gen . flow_from_directory ( technocast_test_path , target_size = image_shape [: 2 ], color_mode = \"grayscale\" , batch_size = batch_size , class_mode = 'binary' , shuffle = False ) loading training set... Found 840 images belonging to 2 classes. loading testing set... Found 715 images belonging to 2 classes.","title":"4.0.4.1 Loading Data with ImageDataGenerator"},{"location":"S4_Computer_Vision_II/#4042-loading-data-with-image_dataset_from_directory","text":"back to top This method should be approx 2x faster than ImageDataGenerator from tensorflow.keras.preprocessing import image_dataset_from_directory from tensorflow.data.experimental import AUTOTUNE path_to_casting_data = '/content/drive/MyDrive/courses/tech_fundamentals/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' # Load training and validation sets image_shape = ( 300 , 300 , 1 ) batch_size = 32 ds_train_ = image_dataset_from_directory ( technocast_train_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = True , ) ds_valid_ = image_dataset_from_directory ( technocast_test_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = False , ) train_set = ds_train_ . prefetch ( buffer_size = AUTOTUNE ) test_set = ds_valid_ . prefetch ( buffer_size = AUTOTUNE ) Found 840 files belonging to 2 classes. Found 715 files belonging to 2 classes. # view some images def_path = '/def_front/cast_def_0_1001.jpeg' ok_path = '/ok_front/cast_ok_0_1.jpeg' image_path = technocast_train_path + ok_path image = tf . io . read_file ( image_path ) image = tf . io . decode_jpeg ( image ) plt . figure ( figsize = ( 6 , 6 )) plt . imshow ( tf . squeeze ( image ), cmap = 'gray' ) plt . axis ( 'off' ) plt . show ();","title":"4.0.4.2 loading data with image_dataset_from_directory"},{"location":"S4_Computer_Vision_II/#41-sliding-window","text":"back to top The kernels we just reviewed, need to be swept or slid along the preceding layer. We call this a sliding window , the window being the kernel. What do you notice about the gif? One perhaps obvious observation is that you can't scoot all the way up to the border of the input layer, this is because the kernel defines operations around the centered pixel and so you bang up against the margin of the input array. We can change the behavior at the boundary with a padding hyperparameter. A second observation, is that the distance we move the kernel along in each step could be variable, we call this the stride . We will explore the affects of each of these. from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ layers . Conv2D ( filters = 64 , kernel_size = 3 , strides = 1 , padding = 'same' , activation = 'relu' ), layers . MaxPool2D ( pool_size = 2 , strides = 1 , padding = 'same' ) # More layers follow ])","title":"4.1 Sliding Window"},{"location":"S4_Computer_Vision_II/#411-stride","text":"back to top Stride defines the the step size we take with each kernel as it passes along the input array. The stride needs to be defined in both the horizontal and vertical dimensions. This animation shows a 2x2 stride The stride will often be 1 for CNNs, where we don't want to lose any important information. Maximum pooling layers will often have strides greater than 1, to better summarize/accentuate the relevant features/activations. If the stride is the same in both the horizontal and vertical directions, it can be set with a single number like strides=2 within keras.","title":"4.1.1 Stride"},{"location":"S4_Computer_Vision_II/#412-padding","text":"back to top Padding attempts to resolve our issue at the border: our kernel requires information surrounding the centered pixel, and at the border of the input array we don't have that information. What to do? We have a couple popular options within the keras framework. We can set padding='valid' and only slide the kernel to the edge of the input array. This has the drawback of feature maps shrinking in size as we pass through the NN. Another option is to set padding='same' what this will do is pad the input array with 0's, just enough of them to allow the feature map to be the same size as the input array. This is shown in the gif below: The downside of setting the padding to same will be that features at the edges of the image will be diluted.","title":"4.1.2 Padding"},{"location":"S4_Computer_Vision_II/#exercise-1-exploring-sliding-windows","text":"back to top from skimage import draw , transform from itertools import product # helper functions borrowed from Ryan Holbrook # https://mathformachines.com/ def circle ( size , val = None , r_shrink = 0 ): circle = np . zeros ([ size [ 0 ] + 1 , size [ 1 ] + 1 ]) rr , cc = draw . circle_perimeter ( size [ 0 ] // 2 , size [ 1 ] // 2 , radius = size [ 0 ] // 2 - r_shrink , shape = [ size [ 0 ] + 1 , size [ 1 ] + 1 ], ) if val is None : circle [ rr , cc ] = np . random . uniform ( size = circle . shape )[ rr , cc ] else : circle [ rr , cc ] = val circle = transform . resize ( circle , size , order = 0 ) return circle def show_kernel ( kernel , label = True , digits = None , text_size = 28 ): # Format kernel kernel = np . array ( kernel ) if digits is not None : kernel = kernel . round ( digits ) # Plot kernel cmap = plt . get_cmap ( 'Blues_r' ) plt . imshow ( kernel , cmap = cmap ) rows , cols = kernel . shape thresh = ( kernel . max () + kernel . min ()) / 2 # Optionally, add value labels if label : for i , j in product ( range ( rows ), range ( cols )): val = kernel [ i , j ] color = cmap ( 0 ) if val > thresh else cmap ( 255 ) plt . text ( j , i , val , color = color , size = text_size , horizontalalignment = 'center' , verticalalignment = 'center' ) plt . xticks ([]) plt . yticks ([]) def show_extraction ( image , kernel , conv_stride = 1 , conv_padding = 'valid' , activation = 'relu' , pool_size = 2 , pool_stride = 2 , pool_padding = 'same' , figsize = ( 10 , 10 ), subplot_shape = ( 2 , 2 ), ops = [ 'Input' , 'Filter' , 'Detect' , 'Condense' ], gamma = 1.0 ): # Create Layers model = tf . keras . Sequential ([ tf . keras . layers . Conv2D ( filters = 1 , kernel_size = kernel . shape , strides = conv_stride , padding = conv_padding , use_bias = False , input_shape = image . shape , ), tf . keras . layers . Activation ( activation ), tf . keras . layers . MaxPool2D ( pool_size = pool_size , strides = pool_stride , padding = pool_padding , ), ]) layer_filter , layer_detect , layer_condense = model . layers kernel = tf . reshape ( kernel , [ * kernel . shape , 1 , 1 ]) layer_filter . set_weights ([ kernel ]) # Format for TF image = tf . expand_dims ( image , axis = 0 ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) # Extract Feature image_filter = layer_filter ( image ) image_detect = layer_detect ( image_filter ) image_condense = layer_condense ( image_detect ) images = {} if 'Input' in ops : images . update ({ 'Input' : ( image , 1.0 )}) if 'Filter' in ops : images . update ({ 'Filter' : ( image_filter , 1.0 )}) if 'Detect' in ops : images . update ({ 'Detect' : ( image_detect , gamma )}) if 'Condense' in ops : images . update ({ 'Condense' : ( image_condense , gamma )}) # Plot plt . figure ( figsize = figsize ) for i , title in enumerate ( ops ): image , gamma = images [ title ] plt . subplot ( * subplot_shape , i + 1 ) plt . imshow ( tf . image . adjust_gamma ( tf . squeeze ( image ), gamma )) plt . axis ( 'off' ) plt . title ( title ) Create an image and kernel: import tensorflow as tf import matplotlib.pyplot as plt plt . rc ( 'figure' , autolayout = True ) plt . rc ( 'axes' , labelweight = 'bold' , labelsize = 'large' , titleweight = 'bold' , titlesize = 18 , titlepad = 10 ) plt . rc ( 'image' , cmap = 'magma' ) image = circle ([ 64 , 64 ], val = 1.0 , r_shrink = 3 ) image = tf . reshape ( image , [ * image . shape , 1 ]) # Bottom sobel kernel = tf . constant ( [[ - 1 , - 2 , - 1 ], [ 0 , 0 , 0 ], [ 1 , 2 , 1 ]], ) show_kernel ( kernel ) What do we think this kernel is meant to detect for? We will apply our kernel with a 1x1 stride and our max pooling with a 2x2 stride and pool size of 2. show_extraction ( image , kernel , # Window parameters conv_stride = 1 , pool_size = 2 , pool_stride = 2 , subplot_shape = ( 1 , 4 ), figsize = ( 14 , 6 ), ) Works ok! what about a higher conv stride? show_extraction ( image , kernel , # Window parameters conv_stride = 3 , pool_size = 2 , pool_stride = 2 , subplot_shape = ( 1 , 4 ), figsize = ( 14 , 6 ), ) Looks like we lost a bit of information! Sometimes published models will use a larger kernel and stride in the initial layer to produce large-scale features early on in the network without losing too much information (ResNet50 uses 7x7 kernels with a stride of 2). For now, without having much experience it's safe to set conv strides to 1. Take a moment here with the given kernel and explore different settings for applying both the kernel and the max_pool conv_stride=YOUR_VALUE, # condenses pixels pool_size=YOUR_VALUE, pool_stride=YOUR_VALUE, # condenses pixels Given a total condensation of 8 (I'm taking condensation to mean conv_stride x pool_stride ). what do you think is the best combination of values for conv_stride, pool_size, and pool_stride ?","title":"\ud83c\udfcb\ufe0f Exercise 1: Exploring Sliding Windows"},{"location":"S4_Computer_Vision_II/#42-custom-cnn","text":"back to top As we move through the network, small-scale features (lines, edges, etc.) turn to large-scale features (shapes, eyes, ears, etc). We call these blocks of convolution, ReLU, and max pool convolutional blocks and they are the low level modular framework we work with. By this means, the CNN is able to design it's own features, ones suited for the classification or regression task at hand. We will design a custom CNN for the Casting Defect Detection Dataset. In the following I'm going to double the filter size after the first block. This is a common pattern as the max pooling layers forces us in the opposite direction. #Creating model model = Sequential () model . add ( InputLayer ( input_shape = ( image_shape ))) model . add ( Conv2D ( filters = 8 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 224 )) model . add ( Activation ( 'relu' )) # Last layer model . add ( Dense ( 1 )) model . add ( Activation ( 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'binary_accuracy' ]) early_stop = EarlyStopping ( monitor = 'val_loss' , patience = 5 , restore_best_weights = True ,) # with CPU + ImageDataGenerator runs for about 40 minutes (5 epochs) # with GPU + image_dataset_from_directory runs for about 4 minutes (16 epochs) with tf . device ( '/device:GPU:0' ): results = model . fit ( train_set , epochs = 20 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/20 27/27 [==============================] - 224s 7s/step - loss: 68.2100 - binary_accuracy: 0.5714 - val_loss: 0.8023 - val_binary_accuracy: 0.5343 Epoch 2/20 27/27 [==============================] - 3s 86ms/step - loss: 0.5720 - binary_accuracy: 0.7048 - val_loss: 0.5140 - val_binary_accuracy: 0.7399 Epoch 3/20 27/27 [==============================] - 3s 82ms/step - loss: 0.4475 - binary_accuracy: 0.7881 - val_loss: 0.4868 - val_binary_accuracy: 0.7524 Epoch 4/20 27/27 [==============================] - 3s 84ms/step - loss: 0.4018 - binary_accuracy: 0.8202 - val_loss: 0.5074 - val_binary_accuracy: 0.7469 Epoch 5/20 27/27 [==============================] - 3s 87ms/step - loss: 0.3237 - binary_accuracy: 0.8631 - val_loss: 0.4168 - val_binary_accuracy: 0.8112 Epoch 6/20 27/27 [==============================] - 3s 82ms/step - loss: 0.1998 - binary_accuracy: 0.9333 - val_loss: 0.3857 - val_binary_accuracy: 0.8322 Epoch 7/20 27/27 [==============================] - 3s 83ms/step - loss: 0.1086 - binary_accuracy: 0.9738 - val_loss: 0.2786 - val_binary_accuracy: 0.8853 Epoch 8/20 27/27 [==============================] - 3s 83ms/step - loss: 0.0560 - binary_accuracy: 0.9929 - val_loss: 0.3389 - val_binary_accuracy: 0.8615 Epoch 9/20 27/27 [==============================] - 3s 85ms/step - loss: 0.0345 - binary_accuracy: 0.9952 - val_loss: 0.2035 - val_binary_accuracy: 0.9175 Epoch 10/20 27/27 [==============================] - 3s 85ms/step - loss: 0.0772 - binary_accuracy: 0.9762 - val_loss: 0.2724 - val_binary_accuracy: 0.8979 Epoch 11/20 27/27 [==============================] - 3s 87ms/step - loss: 0.0336 - binary_accuracy: 0.9976 - val_loss: 0.4880 - val_binary_accuracy: 0.8420 Epoch 12/20 27/27 [==============================] - 3s 84ms/step - loss: 0.0387 - binary_accuracy: 0.9881 - val_loss: 0.2376 - val_binary_accuracy: 0.9105 Epoch 13/20 27/27 [==============================] - 3s 86ms/step - loss: 0.0172 - binary_accuracy: 0.9988 - val_loss: 0.2523 - val_binary_accuracy: 0.9133 Epoch 14/20 27/27 [==============================] - 3s 88ms/step - loss: 0.0045 - binary_accuracy: 1.0000 - val_loss: 0.3099 - val_binary_accuracy: 0.9021 # model.save('inspection_of_casting_products.h5')","title":"4.2 Custom CNN"},{"location":"S4_Computer_Vision_II/#421-evaluate-model","text":"back to top # model.load_weights('inspection_of_casting_products.h5') losses = pd . DataFrame ( results . history ) # losses.to_csv('history_simple_model.csv', index=False) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) losses [[ 'loss' , 'val_loss' ]] . plot ( ax = ax [ 0 ]) losses [[ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot ( ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f20b425fa50> # predict test set pred_probability = model . predict ( test_set ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score # test_set.classes to get images from ImageDataGenerator # for image_dataset_from_directory we have to do a little gymnastics # to get the labels labels = np . array ([]) for x , y in ds_valid_ : labels = np . concatenate ([ labels , tf . squeeze ( y . numpy ()) . numpy ()]) print ( classification_report ( labels , predictions )) precision recall f1-score support 0.0 0.97 0.90 0.93 453 1.0 0.84 0.95 0.89 262 accuracy 0.92 715 macro avg 0.91 0.93 0.91 715 weighted avg 0.92 0.92 0.92 715 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( labels , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bc8bd150>","title":"4.2.1 Evaluate Model"},{"location":"S4_Computer_Vision_II/#43-data-augmentation","text":"back to top Alright, alright, alright. We've done pretty good making our CNN model. But let's see if we can make it even better. There's a last trick we'll cover here in regard to image classifiers. We're going to perturb the input images in such a way as to create a pseudo-larger dataset. With any machine learning model, the more relevant training data we give the model, the better. The key here is relevant training data. We can easily do this with images so long as we do not change the class of the image. For example, in the small plot below, we are changing contrast, hue, rotation, and doing other things to the image of a car; and this is okay because it does not change the classification from a car to, say, a truck. Typically when we do data augmentation for images, we do them online , i.e. during training. Recall that we train in batches (or minibatches) with CNNs. An example of a minibatch then, might be the small multiples plot below. by varying the images in this way, the model always sees slightly new data, and becomes a more robust model. Remember that the caveat is that we can't muddle the relevant classification of the image. Sometimes the best way to see if data augmentation will be helpful is to just try it and see! from tensorflow.keras.layers.experimental import preprocessing #Creating model model = Sequential () model . add ( preprocessing . RandomFlip ( 'horizontal' )), # flip left-to-right model . add ( preprocessing . RandomFlip ( 'vertical' )), # flip upside-down model . add ( preprocessing . RandomContrast ( 0.5 )), # contrast change by up to 50% model . add ( Conv2D ( filters = 8 , kernel_size = ( 3 , 3 ), input_shape = image_shape , activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), activation = 'relu' ,)) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 224 )) model . add ( Activation ( 'relu' )) # Last layer model . add ( Dense ( 1 )) model . add ( Activation ( 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'binary_accuracy' ]) early_stop = EarlyStopping ( monitor = 'val_loss' , patience = 5 , restore_best_weights = True ,) results = model . fit ( train_set , epochs = 30 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/30 27/27 [==============================] - 3s 89ms/step - loss: 63.6867 - binary_accuracy: 0.5500 - val_loss: 1.4445 - val_binary_accuracy: 0.5888 Epoch 2/30 27/27 [==============================] - 3s 87ms/step - loss: 1.5593 - binary_accuracy: 0.6036 - val_loss: 1.2496 - val_binary_accuracy: 0.6573 Epoch 3/30 27/27 [==============================] - 3s 86ms/step - loss: 1.0772 - binary_accuracy: 0.6869 - val_loss: 0.4536 - val_binary_accuracy: 0.8014 Epoch 4/30 27/27 [==============================] - 3s 85ms/step - loss: 0.8275 - binary_accuracy: 0.7095 - val_loss: 0.3635 - val_binary_accuracy: 0.8336 Epoch 5/30 27/27 [==============================] - 3s 86ms/step - loss: 0.5000 - binary_accuracy: 0.7821 - val_loss: 0.3140 - val_binary_accuracy: 0.8643 Epoch 6/30 27/27 [==============================] - 3s 86ms/step - loss: 0.4402 - binary_accuracy: 0.8214 - val_loss: 0.2672 - val_binary_accuracy: 0.8783 Epoch 7/30 27/27 [==============================] - 3s 85ms/step - loss: 0.3306 - binary_accuracy: 0.8524 - val_loss: 0.9402 - val_binary_accuracy: 0.6825 Epoch 8/30 27/27 [==============================] - 3s 86ms/step - loss: 1.1711 - binary_accuracy: 0.7143 - val_loss: 0.4291 - val_binary_accuracy: 0.8252 Epoch 9/30 27/27 [==============================] - 3s 86ms/step - loss: 0.3464 - binary_accuracy: 0.8583 - val_loss: 0.5895 - val_binary_accuracy: 0.7832 Epoch 10/30 27/27 [==============================] - 3s 85ms/step - loss: 0.3010 - binary_accuracy: 0.8738 - val_loss: 0.4319 - val_binary_accuracy: 0.8196 Epoch 11/30 27/27 [==============================] - 3s 89ms/step - loss: 0.2554 - binary_accuracy: 0.8893 - val_loss: 0.2068 - val_binary_accuracy: 0.9091 Epoch 12/30 27/27 [==============================] - 3s 86ms/step - loss: 0.1639 - binary_accuracy: 0.9512 - val_loss: 0.1869 - val_binary_accuracy: 0.9189 Epoch 13/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1728 - binary_accuracy: 0.9298 - val_loss: 0.1441 - val_binary_accuracy: 0.9441 Epoch 14/30 27/27 [==============================] - 3s 85ms/step - loss: 0.1314 - binary_accuracy: 0.9536 - val_loss: 0.1434 - val_binary_accuracy: 0.9455 Epoch 15/30 27/27 [==============================] - 3s 85ms/step - loss: 0.1620 - binary_accuracy: 0.9512 - val_loss: 0.2196 - val_binary_accuracy: 0.9091 Epoch 16/30 27/27 [==============================] - 3s 87ms/step - loss: 0.1345 - binary_accuracy: 0.9512 - val_loss: 0.1243 - val_binary_accuracy: 0.9538 Epoch 17/30 27/27 [==============================] - 3s 86ms/step - loss: 0.3751 - binary_accuracy: 0.8500 - val_loss: 0.6976 - val_binary_accuracy: 0.7580 Epoch 18/30 27/27 [==============================] - 3s 86ms/step - loss: 0.2022 - binary_accuracy: 0.9214 - val_loss: 0.1656 - val_binary_accuracy: 0.9231 Epoch 19/30 27/27 [==============================] - 3s 84ms/step - loss: 0.1082 - binary_accuracy: 0.9571 - val_loss: 0.1395 - val_binary_accuracy: 0.9371 Epoch 20/30 27/27 [==============================] - 3s 84ms/step - loss: 0.0978 - binary_accuracy: 0.9643 - val_loss: 0.1182 - val_binary_accuracy: 0.9441 Epoch 21/30 27/27 [==============================] - 3s 88ms/step - loss: 0.1174 - binary_accuracy: 0.9583 - val_loss: 0.1207 - val_binary_accuracy: 0.9469 Epoch 22/30 27/27 [==============================] - 3s 87ms/step - loss: 0.0898 - binary_accuracy: 0.9774 - val_loss: 0.1390 - val_binary_accuracy: 0.9413 Epoch 23/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1028 - binary_accuracy: 0.9607 - val_loss: 0.1150 - val_binary_accuracy: 0.9552 Epoch 24/30 27/27 [==============================] - 3s 86ms/step - loss: 0.1146 - binary_accuracy: 0.9583 - val_loss: 0.1078 - val_binary_accuracy: 0.9650 Epoch 25/30 27/27 [==============================] - 3s 89ms/step - loss: 0.1007 - binary_accuracy: 0.9643 - val_loss: 0.0802 - val_binary_accuracy: 0.9692 Epoch 26/30 27/27 [==============================] - 3s 88ms/step - loss: 0.0882 - binary_accuracy: 0.9750 - val_loss: 0.0755 - val_binary_accuracy: 0.9692 Epoch 27/30 27/27 [==============================] - 3s 89ms/step - loss: 0.0884 - binary_accuracy: 0.9750 - val_loss: 0.1813 - val_binary_accuracy: 0.9385 Epoch 28/30 27/27 [==============================] - 3s 85ms/step - loss: 0.0908 - binary_accuracy: 0.9631 - val_loss: 0.1431 - val_binary_accuracy: 0.9385 Epoch 29/30 27/27 [==============================] - 3s 87ms/step - loss: 0.0576 - binary_accuracy: 0.9833 - val_loss: 0.0920 - val_binary_accuracy: 0.9664 Epoch 30/30 27/27 [==============================] - 3s 87ms/step - loss: 0.1199 - binary_accuracy: 0.9500 - val_loss: 0.3270 - val_binary_accuracy: 0.8769","title":"4.3 Data Augmentation"},{"location":"S4_Computer_Vision_II/#431-evaluate-model","text":"back to top losses = pd . DataFrame ( results . history ) # losses.to_csv('history_augment_model.csv', index=False) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 5 )) losses [[ 'loss' , 'val_loss' ]] . plot ( ax = ax [ 0 ]) losses [[ 'binary_accuracy' , 'val_binary_accuracy' ]] . plot ( ax = ax [ 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bbbc59d0> # predict test set pred_probability = model . predict ( test_set ) # convert to bool predictions = pred_probability > 0.5 # precision / recall / f1-score # test_set.classes to get images from ImageDataGenerator # for image_dataset_from_directory we have to do a little gymnastics # to get the labels labels = np . array ([]) for x , y in ds_valid_ : labels = np . concatenate ([ labels , tf . squeeze ( y . numpy ()) . numpy ()]) print ( classification_report ( labels , predictions )) precision recall f1-score support 0.0 1.00 0.81 0.89 453 1.0 0.75 1.00 0.86 262 accuracy 0.88 715 macro avg 0.87 0.90 0.87 715 weighted avg 0.91 0.88 0.88 715 plt . figure ( figsize = ( 10 , 6 )) sns . heatmap ( confusion_matrix ( labels , predictions ), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x7f20bda35890>","title":"4.3.1 Evaluate Model"},{"location":"S4_Computer_Vision_II/#exercise-2-image-preprocessing-layers","text":"back to top These layers apply random augmentation transforms to a batch of images. They are only active during training. You can visit the documentation here RandomCrop layer RandomFlip layer RandomTranslation layer RandomRotation layer RandomZoom layer RandomHeight layer RandomWidth layer Use any combination of random augmentation transforms and retrain your model. Can you get a higher val performance? you may need to increase your epochs. # code cell for exercise 4.3.2","title":"\ud83c\udfcb\ufe0f Exercise 2: Image Preprocessing Layers"},{"location":"S4_Computer_Vision_II/#44-transfer-learning","text":"back to top Transfer learning with EfficientNet from tensorflow.keras.preprocessing import image_dataset_from_directory from tensorflow.data.experimental import AUTOTUNE path_to_casting_data = '/content/drive/MyDrive/courses/TECH_FUNDAMENTALS/data/casting_data_class_practice' technocast_train_path = path_to_casting_data + '/train/' technocast_test_path = path_to_casting_data + '/test/' # Load training and validation sets image_shape = ( 300 , 300 , 3 ) batch_size = 32 ds_train_ = image_dataset_from_directory ( technocast_train_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = True , ) ds_valid_ = image_dataset_from_directory ( technocast_test_path , labels = 'inferred' , label_mode = 'binary' , color_mode = \"grayscale\" , image_size = image_shape [: 2 ], batch_size = batch_size , shuffle = False , ) train_set = ds_train_ . prefetch ( buffer_size = AUTOTUNE ) test_set = ds_valid_ . prefetch ( buffer_size = AUTOTUNE ) Found 840 files belonging to 2 classes. Found 715 files belonging to 2 classes. def build_model ( image_shape ): input = tf . keras . layers . Input ( shape = ( image_shape )) # include_top = False will take of the last dense layer used for classification model = tf . keras . applications . EfficientNetB3 ( include_top = False , input_tensor = input , weights = \"imagenet\" ) # Freeze the pretrained weights model . trainable = False # now we have to rebuild the top x = tf . keras . layers . GlobalAveragePooling2D ( name = \"avg_pool\" )( model . output ) x = tf . keras . layers . BatchNormalization ()( x ) top_dropout_rate = 0.2 x = tf . keras . layers . Dropout ( top_dropout_rate , name = \"top_dropout\" )( x ) # use num-nodes = 1 for binary, class # for multiclass output = tf . keras . layers . Dense ( 1 , activation = \"softmax\" , name = \"pred\" )( x ) # Compile model = tf . keras . Model ( input , output , name = \"EfficientNet\" ) model . compile ( optimizer = 'adam' , loss = \"binary_crossentropy\" , metrics = [ \"binary_accuracy\" ]) return model model = build_model ( image_shape ) with tf . device ( '/device:GPU:0' ): results = model . fit ( train_set , epochs = 20 , validation_data = test_set , callbacks = [ early_stop ]) Epoch 1/20 WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). 27/27 [==============================] - ETA: 0s - loss: 0.4457 - binary_accuracy: 0.4905WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 300, 300, 1). 27/27 [==============================] - 20s 442ms/step - loss: 0.4457 - binary_accuracy: 0.4905 - val_loss: 0.4851 - val_binary_accuracy: 0.3664 Epoch 2/20 27/27 [==============================] - 11s 381ms/step - loss: 0.1878 - binary_accuracy: 0.4905 - val_loss: 0.3930 - val_binary_accuracy: 0.3664 Epoch 3/20 27/27 [==============================] - 11s 384ms/step - loss: 0.1816 - binary_accuracy: 0.4905 - val_loss: 0.3407 - val_binary_accuracy: 0.3664 Epoch 4/20 27/27 [==============================] - 11s 378ms/step - loss: 0.1394 - binary_accuracy: 0.4905 - val_loss: 0.2971 - val_binary_accuracy: 0.3664 Epoch 5/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0982 - binary_accuracy: 0.4905 - val_loss: 0.2490 - val_binary_accuracy: 0.3664 Epoch 6/20 27/27 [==============================] - 11s 376ms/step - loss: 0.1032 - binary_accuracy: 0.4905 - val_loss: 0.2130 - val_binary_accuracy: 0.3664 Epoch 7/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0801 - binary_accuracy: 0.4905 - val_loss: 0.1846 - val_binary_accuracy: 0.3664 Epoch 8/20 27/27 [==============================] - 11s 383ms/step - loss: 0.0806 - binary_accuracy: 0.4905 - val_loss: 0.1509 - val_binary_accuracy: 0.3664 Epoch 9/20 27/27 [==============================] - 11s 379ms/step - loss: 0.0736 - binary_accuracy: 0.4905 - val_loss: 0.1263 - val_binary_accuracy: 0.3664 Epoch 10/20 27/27 [==============================] - 11s 379ms/step - loss: 0.0916 - binary_accuracy: 0.4905 - val_loss: 0.1008 - val_binary_accuracy: 0.3664 Epoch 11/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0618 - binary_accuracy: 0.4905 - val_loss: 0.0886 - val_binary_accuracy: 0.3664 Epoch 12/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0843 - binary_accuracy: 0.4905 - val_loss: 0.0728 - val_binary_accuracy: 0.3664 Epoch 13/20 27/27 [==============================] - 11s 383ms/step - loss: 0.0741 - binary_accuracy: 0.4905 - val_loss: 0.0593 - val_binary_accuracy: 0.3664 Epoch 14/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0637 - binary_accuracy: 0.4905 - val_loss: 0.0535 - val_binary_accuracy: 0.3664 Epoch 15/20 27/27 [==============================] - 11s 380ms/step - loss: 0.0657 - binary_accuracy: 0.4905 - val_loss: 0.0491 - val_binary_accuracy: 0.3664 Epoch 16/20 27/27 [==============================] - 11s 375ms/step - loss: 0.0653 - binary_accuracy: 0.4905 - val_loss: 0.0424 - val_binary_accuracy: 0.3664 Epoch 17/20 27/27 [==============================] - 11s 378ms/step - loss: 0.0451 - binary_accuracy: 0.4905 - val_loss: 0.0394 - val_binary_accuracy: 0.3664 Epoch 18/20 27/27 [==============================] - 11s 382ms/step - loss: 0.0667 - binary_accuracy: 0.4905 - val_loss: 0.0345 - val_binary_accuracy: 0.3664 Epoch 19/20 27/27 [==============================] - 11s 382ms/step - loss: 0.0541 - binary_accuracy: 0.4905 - val_loss: 0.0351 - val_binary_accuracy: 0.3664 Epoch 20/20 27/27 [==============================] - 11s 381ms/step - loss: 0.0353 - binary_accuracy: 0.4905 - val_loss: 0.0365 - val_binary_accuracy: 0.3664","title":"4.4 Transfer Learning"},{"location":"S5_Time_Series_Analysis/","text":"General Applications of Neural Networks Session 5: Recurrent Neural Networks and Time Series Analysis \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we'll be exploring NN as they apply to sequenced data, specifically time series data. 5.0 Preparing Environment and Importing Data \u00b6 back to top 5.0.1 Import Packages \u00b6 back to top import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px import random from scipy.stats import gamma , norm , expon from ipywidgets import interact from statsmodels.tsa.stattools import pacf , acf from sklearn.metrics import mean_squared_error def melt_results ( model , X , y ): y_pred = model . predict ( X ) results = pd . DataFrame ( y_pred , y ) results = results . reset_index () results . index = data [ 'Date' ][ window_size :] results = results . reset_index () results . columns = [ 'Date' , 'real' , 'predicted' ] results = results . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) return results def process_data ( Xy , window = 3 , time_cols = 12 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels def train_test_process ( data , train_test_val_ratios = [ 0.6 , 0.8 ], window_size = 3 ): # get the indices at the associated ratios idx_split1 = int ( data . shape [ 1 ] * train_test_val_ratios [ 0 ]) idx_split2 = int ( data . shape [ 1 ] * train_test_val_ratios [ 1 ]) # index the data to build the sets data_train = data [:,: idx_split1 ] data_val = data [:, idx_split1 : idx_split2 ] data_test = data [:, idx_split2 :] # build out the training sets with the sweeping window method X_train , y_train , labels = process_data ( data_train , window = window_size , time_cols = 132 ) X_val , y_val , labels = process_data ( data_val , window = window_size , time_cols = 132 ) X_test , y_test , labels = process_data ( data_test , window = window_size , time_cols = 132 ) print ( \"train size: {} \" . format ( X_train . shape [ 0 ])) print ( \"val size: {} \" . format ( X_val . shape [ 0 ])) print ( \"test size: {} \" . format ( X_test . shape [ 0 ]), end = ' \\n\\n ' ) return X_train , y_train , X_val , y_val , X_test , y_test 5.0.2 Load Dataset \u00b6 back to top orders = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"truffletopia/main/truffletopia/data/12_year_orders.csv\" ) cat_cols = [ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' , 'customer' ] time_cols = [ i for i in orders . columns if i not in cat_cols ] # note that our data is 'untidy' if we wanted to tidy the data we would need to # unpivot or 'melt' our date columns like so: orders . melt ( id_vars = cat_cols , var_name = 'date' , value_name = 'kg' ) # however the data as it is, is useful for our purposes of timeseries prediction # today .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group customer date kg 0 Cheese Candy Outer Horchata Vanilla Amethyst Perk-a-Cola 1/2010 12570.335165 1 Tiramisu Chocolate Outer Irish Cream Egg Nog Slate Dandy's Candies 1/2010 7922.970436 2 Sponge Chocolate Outer Ginger Ale Apple Slate Dandy's Candies 1/2010 10521.306722 3 Cheese Chocolate Outer Coffee Pear Opal Dandy's Candies 1/2010 4739.122200 4 Chiffon Jelly Filled Butter Toffee Apricot Olive Slugworth 1/2010 2756.891961 ... ... ... ... ... ... ... ... ... 13195 Chiffon Chocolate Outer Acai Berry Tangerine Slate Fickelgruber 12/2020 25714.512372 13196 Butter Jelly Filled Plum Peppermint Olive Fickelgruber 12/2020 15043.303525 13197 Chiffon Chocolate Outer Wild Cherry Cream Peppermint Taupe Perk-a-Cola 12/2020 8769.613116 13198 Cheese Candy Outer Mango Mango Rose Dandy's Candies 12/2020 5065.975534 13199 Sponge Chocolate Outer Ginger Ale Passion Fruit Black Fickelgruber 12/2020 9466.712219 13200 rows \u00d7 8 columns 5.1 Why We Think in Sequences \u00b6 back to top There are some problems that are best framed as a sequence in either the input or the output. For example, in our image classification we are performing a mapping of many-to-one: sequence input (the pixels) to a single output (classification). Other examples include: One-to-many: sequence output, e.x. word (if treated as a single input) to generate a picture Many-to-many: sequence input and output, e.x. machine translation (like english to mandarin) Synchronized many-to-many: synced sequence input and output, e.x. video classification State of the art handling of sequences has occurred in a class of networks called recurrent neural networks 5.2 Recurrent Neural Networks \u00b6 back to top Recurrent Neural Networks (RNNs) can be thought of as a FFNN with loops added into the architecture. This allows the network to retain information, create \"memory\" that can be associated with signals later in the sequence. We didn't go into much detail about the actual training algorithm of neural networks: back propagation . But what we will say here, is that this algorithm breaks down with recurrent neural networks because of the looped connections. A trick was created to overcome this, where the looped connections are unrolled, using a copy of the \"unhooked\" neuron to represent where the loop was initally fed back. This algorithm is called back propagation through time . Another problem is introduced when training recurrent neural networks, in that the gradients calculated during back propagation can become very large, exploding gradients , or very small vanishing gradients . This problem is modulated in FNNNs by the ReLU, In RNNs, a more sophisticated gating mechanism is used in an architecture we call Long Short-Term Memory Networks LSTM shown in both typical and unfolded format 5.2.1 Long Short-Term Memory Networks \u00b6 back to top Long Short-Term Memory Networks (LSTMs) are a type of RNN that are trained using back propagation through time and overcome the vanishing/exploding gradient problem. Similar to CNNs, their architecture is composed of blocks, this time with memory blocks rather than convolutional blocks. A block is smarter than the classical neuron; it contains gates that manage the block's state and output. The gates are operated by a sigmoid function, determining whether they are open or closed (triggered or not trigerred). There are three types of gates within a memory block: Forget gate: decides what information is discarded Input gate: decides what information updates the memory state Output gate: decides what information to send forward depending on the input and memory state These weights that configure these gates are learned during training, and their coordination allow each memory block to learn sophisticated relationships in and among sequenced data. Big takeaway: memory blocks contain trainable parameters that allow the block to learn relationships between sequenced data 5.3 Exploratory Data Analysis with Plotly/Pandas \u00b6 back to top orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group customer 1/2010 2/2010 3/2010 4/2010 5/2010 6/2010 7/2010 8/2010 9/2010 10/2010 11/2010 12/2010 1/2011 2/2011 3/2011 4/2011 5/2011 6/2011 7/2011 8/2011 9/2011 10/2011 11/2011 12/2011 1/2012 2/2012 3/2012 4/2012 5/2012 6/2012 7/2012 8/2012 9/2012 10/2012 ... 9/2017 10/2017 11/2017 12/2017 1/2018 2/2018 3/2018 4/2018 5/2018 6/2018 7/2018 8/2018 9/2018 10/2018 11/2018 12/2018 1/2019 2/2019 3/2019 4/2019 5/2019 6/2019 7/2019 8/2019 9/2019 10/2019 11/2019 12/2019 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Cheese Candy Outer Horchata Vanilla Amethyst Perk-a-Cola 12570.335165 11569.168746 13616.812204 11884.370881 13950.332334 12781.156536 14256.210023 12887.711960 15038.574006 12626.489306 14611.291109 13194.814300 14921.016216 13477.391457 15409.211080 13999.215069 15597.436976 14098.124978 15596.818092 14941.694032 15715.347212 14181.212142 16282.098006 14650.929410 16433.209008 15400.579034 16756.981263 15128.148250 17523.979943 15413.044691 16366.264377 14568.470959 16901.111542 14659.021365 ... 20736.279239 18617.387585 20783.711234 17470.755865 20523.579840 18796.936906 20028.582493 18677.535295 20048.107422 18929.248617 20571.155902 18207.204656 20839.042892 18966.532984 20909.977545 18589.807152 21287.370123 17987.976867 21111.062685 18538.311321 21797.267132 18935.352772 21331.378420 18783.759611 22139.123373 18553.797271 21579.506284 19726.433111 21147.624131 19232.360491 21575.521051 18856.178110 20701.250676 19406.448560 22328.687163 19384.824042 21449.154890 19554.405590 21873.104938 19572.860127 1 Tiramisu Chocolate Outer Irish Cream Egg Nog Slate Dandy's Candies 7922.970436 6464.558625 6616.092291 8244.991928 6602.132649 7032.700478 8437.517865 6919.862786 7003.449554 8516.767749 7541.471510 7145.880001 8821.556334 7325.240199 7618.246523 9385.832260 7705.860411 7709.843383 9471.542415 7791.968645 8214.485181 9393.875864 7911.159820 8181.208339 9750.400651 8084.792749 8234.370603 9777.179731 8134.876166 8449.426300 9911.512891 8686.751188 8359.638696 10147.391908 ... 9709.721305 12367.917469 10109.247976 10597.517266 12723.546352 10535.166596 10043.352958 12434.413543 10594.590384 10473.165191 12323.923036 10232.771159 9973.322947 12450.632426 10247.199668 10310.557749 12527.881699 10288.118368 10792.495418 12538.911064 10564.257282 10672.337286 12442.348062 10975.342816 10504.218214 12700.925307 10853.622645 10917.981718 13005.963533 10610.202654 10145.394106 13132.925131 10821.805709 10829.961838 12995.340352 10504.814195 10617.199735 13377.165673 11065.835571 11135.386324 2 Sponge Chocolate Outer Ginger Ale Apple Slate Dandy's Candies 10521.306722 5543.335645 5294.892374 11010.452413 5267.190367 5546.045669 11394.362620 5712.245098 5798.349463 11781.306993 5918.299339 5892.693500 12298.538324 6260.141878 6244.742736 12336.799353 6201.024217 6638.056331 12661.775736 6545.099808 6536.149679 12757.183923 6717.449248 6473.324997 13467.205626 6806.690857 7052.340323 13488.253357 6613.263036 7017.902839 13676.458530 6925.928151 6931.425252 13418.564207 ... 8486.837094 16402.754127 8037.467789 8184.225799 16627.369839 8287.853805 8307.474776 16869.445360 8090.179106 8258.503638 16961.500038 8261.880924 8221.327715 16468.858014 8243.277819 8381.807681 16778.471019 8673.021737 8552.964237 16902.958241 8346.914162 8431.816880 17088.184916 8928.738286 8511.985934 17273.307554 8242.173578 8755.268429 16961.225344 8686.594959 8516.098910 17498.911792 8369.846849 8334.206937 17519.678690 8595.378915 8909.348040 17234.636475 9002.216839 8794.467252 3 Cheese Chocolate Outer Coffee Pear Opal Dandy's Candies 4739.122200 2733.281035 4984.394797 2750.709519 5274.473185 2737.736109 5236.191952 2807.504142 5581.285441 2500.882597 5635.195267 3035.782263 5492.449149 2987.850135 6021.641513 3141.406171 5884.424448 2898.492508 5925.633348 2990.291270 6055.228068 3587.204540 6138.483396 2997.648037 6370.293979 3442.123255 6187.807429 3140.255766 6243.439310 3568.780726 6393.113315 3246.908338 6540.176631 3383.552285 ... 7756.552767 4233.577541 8119.681843 4046.102552 7738.053080 4401.610595 7675.782975 4248.467389 7835.989038 3784.924538 7778.705078 4023.523503 7996.384938 4236.811930 7780.903630 4324.838890 8706.750222 4592.025934 7864.924599 3831.383360 8121.771691 4428.045665 8216.481440 4193.238269 8316.693448 4591.368108 8184.213024 4112.875085 8342.099081 4074.668047 8093.541144 4301.081977 8235.616589 4151.474242 8213.665500 4008.885583 7912.641813 4275.162782 8031.227879 4628.989194 4 Chiffon Jelly Filled Butter Toffee Apricot Olive Slugworth 2756.891961 1739.900797 1791.975108 1533.023665 1735.868123 1824.082183 2637.470462 1707.745100 1621.994676 1814.318681 1669.867919 1681.547977 2830.947230 1732.084718 1739.759848 1647.585142 1750.293269 1710.317714 2805.902600 1751.767883 2009.631273 1937.539054 1735.097831 1839.490125 2799.159990 1566.971234 1752.097294 1573.071123 1760.829823 1795.254784 2754.540709 1886.366523 1807.204970 1740.528223 ... 1842.142055 1500.057312 1731.064289 1675.758204 2904.893170 1789.992333 1810.125486 1795.379685 1907.771505 1973.079708 2865.869167 1574.850737 1783.459110 1787.164751 1752.689655 1734.478146 2804.497744 1956.641539 1909.752412 1693.251443 1748.211959 1842.843637 2757.454985 1674.184059 1698.962332 1631.735285 1769.115620 1663.851403 2919.917902 1830.186857 1864.015449 1800.566323 1625.130275 1908.316219 2696.631511 1859.017636 1690.042699 1764.410866 1909.608709 1711.780317 5 rows \u00d7 138 columns data = pd . DataFrame ( orders . loc [ 0 , time_cols ]) data = data . reset_index () data . columns = [ 'Date' , 'KG' ] data px . scatter ( data , x = 'Date' , y = 'KG' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"9499d748-9bea-476f-9026-a591288a6ec7\")) { Plotly.newPlot( '9499d748-9bea-476f-9026-a591288a6ec7', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12570.33516482565, 11569.168746227244, 13616.8122044598, 11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('9499d748-9bea-476f-9026-a591288a6ec7'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . plotting . autocorrelation_plot ( data [ 'KG' ], ax = ax ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb1b29f3610> Normally with time series data, we'd want to try a host of preprocessing techniques and remove the trend (really create two separate analyses, one of the trend and one of the seasonality) but to keep things simple and to showcase the utility of machine learning, we are going to deviate from the stats-like approach and work with our data as is. For more details on the stats-like models you can perform a cursory search on ARIMA , ARMA , SARIMA 5.4 Modeling \u00b6 back to top from tensorflow import keras from tensorflow.keras import layers 5.4.1 Sweeping (Rolling) Window \u00b6 back to top We're going to revist this idea of a sweeping window from our feature engineering disucssion. It turns out, even though we are using a NN, there is still some preprocessing we need to do. In our case, each time delta is represented by a month. So we will choose some number of months to include in our feature set, this will in turn determine what our overall training data will look like. Xy = orders . loc [[ 0 ], time_cols ] . values # separate the non-time series columns X_cat = Xy [:,: - 120 ] # select the columns to apply the sweeping window X = Xy [:, - 120 :] with a window size of 3, our X will have 3 features, the prior 3 months leading up to the month for which we will attempt to forecast. window_size = 3 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X [: 5 ] array([[12570.33516483, 11569.16874623, 13616.81220446], [11569.16874623, 13616.81220446, 11884.37088102], [13616.81220446, 11884.37088102, 13950.33233441], [11884.37088102, 13950.33233441, 12781.15653568], [13950.33233441, 12781.15653568, 14256.21002336]]) With a window size of 1, our X data will have a feature size of 1 window_size = 1 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X [: 5 ] array([[12570.33516483], [11569.16874623], [13616.81220446], [11884.37088102], [13950.33233441]]) and so on. 5.4.2 FFNN \u00b6 back to top I'm going to start with a very simple FFNN model: model = keras . Sequential ([ layers . Dense ( 8 , input_shape = [ window_size ]), # one layer, 8 nodes layers . Dense ( 1 ) # single output for the kg ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , monitor = 'loss' ) history = model . fit ( X , y , batch_size = 10 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 23 4250741.5 24 4245127.5 25 4246370.5 26 4248100.0 27 4242104.0 As we can see from the y vs y_pred the FFNN is just predicting the previous month's value: y_pred = model . predict ( X ) pd . DataFrame ( y_pred , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 11569.168746 12474.020508 13616.812204 11480.559570 11884.370881 13512.444336 13950.332334 11793.336914 12781.156536 13843.397461 ... ... 19384.824042 22157.271484 21449.154890 19236.064453 19554.405590 21284.505859 21873.104938 19404.339844 19572.860127 21705.197266 131 rows \u00d7 1 columns We can try this with a more suitable window size window_size = 3 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) model = keras . Sequential ([ # layers.Dense(8, input_shape=[window_size]), layers . Dense ( 1 , input_shape = [ window_size ]) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X , y , batch_size = 10 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 703 514088.96875 704 513142.15625 705 507798.15625 706 511337.62500 707 513890.40625 A cursory glance looks like our values are closer together results = melt_results ( model , X , y ) px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d52a6cbd-a684-4ccd-bc70-41064f9370ef\")) { Plotly.newPlot( 'd52a6cbd-a684-4ccd-bc70-41064f9370ef', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12355.0859375, 13640.7431640625, 12307.1044921875, 14332.3984375, 13131.0283203125, 14212.7783203125, 13552.763671875, 14700.265625, 12652.4365234375, 14801.03515625, 13574.015625, 14962.173828125, 13970.4462890625, 15578.1806640625, 14305.185546875, 15540.5673828125, 14297.0986328125, 16009.984375, 15129.0732421875, 15213.9912109375, 14734.306640625, 16403.990234375, 14958.9521484375, 16761.474609375, 15755.279296875, 16499.529296875, 15817.58203125, 17504.912109375, 14965.267578125, 15796.3193359375, 15130.4443359375, 16766.73828125, 14686.5009765625, 16811.646484375, 16451.060546875, 18008.30078125, 16300.5703125, 18110.662109375, 15727.3662109375, 17264.041015625, 16023.4013671875, 17446.83203125, 16421.3203125, 17137.51953125, 16727.669921875, 17762.119140625, 16536.51953125, 17676.62109375, 16819.26953125, 18412.998046875, 17204.791015625, 18482.048828125, 17078.1171875, 18459.734375, 17126.396484375, 18339.029296875, 17736.703125, 18938.0390625, 17749.787109375, 18435.4296875, 17923.501953125, 19243.39453125, 16717.30078125, 19337.271484375, 18180.85546875, 19215.458984375, 18766.697265625, 18614.162109375, 16836.208984375, 19861.880859375, 17306.4375, 19527.3203125, 17852.443359375, 19114.693359375, 18452.845703125, 20069.751953125, 18064.07421875, 18975.03515625, 17102.572265625, 20803.421875, 19135.88671875, 19629.9375, 17069.33984375, 19737.794921875, 19195.9765625, 21565.546875, 18428.048828125, 19303.3515625, 18109.71875, 20768.50390625, 19236.8671875, 20509.314453125, 18924.150390625, 19914.69140625, 17708.232421875, 21088.181640625, 18728.63671875, 19888.515625, 18887.47265625, 20122.76171875, 19426.662109375, 20018.087890625, 18674.275390625, 21098.416015625, 19263.26953125, 20539.78125, 19121.091796875, 20701.00390625, 18277.365234375, 21191.203125, 19286.3203125, 21770.748046875, 19003.40234375, 21054.41796875, 19604.93359375, 21716.5625, 18637.990234375, 22055.3515625, 19713.060546875, 20759.974609375, 19754.70703125, 21166.15234375, 18657.068359375, 20912.896484375, 20605.359375, 22080.318359375, 19207.404296875, 21405.28515625, 20073.166015625], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('d52a6cbd-a684-4ccd-bc70-41064f9370ef'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; \ud83c\udfcb\ufe0f Exercise-Discussion 1: Varify that the model is linear \u00b6 back to top We're having to change our way of thinking here with time series analysis. Recall that a model without an activation function can only encapsulate linear relationships. How come we can see non-linear relationships in our time series plot above? make a plot that showcases we are indeed still within a linear world. This is an open ended question and I myself don't have the best answer. Think about how you would attempt to show linearity of the model. (On Monday our model was only in 2D, and it was a binary classification task, so it was easier to view the decision boundaries and verify linearity). # Code cell for Exercise 1 \ud83c\udfcb\ufe0f Exercise 2: Vary model architecture and window size \u00b6 back to top Create these three different models. Train on the whole dataset with a window size of 3. record the training loss for the last 5 epochs of each model models = [ keras.Sequential([ layers.Dense(8, input_shape=[window_size]), layers.Dense(1) ]), keras.Sequential([ layers.Dense(8, activation='relu', input_shape=[window_size]), layers.Dense(1) ]), keras.Sequential([ layers.Dense(4, activation='relu', input_shape=[window_size]), layers.Dense(1) ])] You can create the training sets with: window_size = 3 X, y, labels = process_data(orders.loc[[0], time_cols].values, window=window_size, time_cols=132) Use a batch size of 10 when training. When you are finished training a model use melt_results and plotly to make a graph of your predictions vs actuals df = melt_results(model, X, y) px.line(df, x='Date', y='KG', color='Source') You can use the same early_stopping and fit formula from 6.4.2 # Code cell for exercise 2 px . line ( df , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"15dad771-d18e-4fff-8d11-983538b1753d\")) { Plotly.newPlot( '15dad771-d18e-4fff-8d11-983538b1753d', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11825.6875, 13559.1416015625, 12137.75390625, 13947.8779296875, 12989.833984375, 14231.40625, 13158.73828125, 14926.837890625, 12870.529296875, 14588.9521484375, 13427.3798828125, 14895.6435546875, 13730.8017578125, 15391.9677734375, 14224.8291015625, 15569.8564453125, 14314.1650390625, 15647.681640625, 15102.7060546875, 15677.5302734375, 14454.068359375, 16251.0849609375, 14895.8544921875, 16456.4453125, 15615.7265625, 16722.203125, 15433.7666015625, 17458.36328125, 15580.2919921875, 16309.3955078125, 14863.07421875, 16818.68359375, 14903.3330078125, 16513.052734375, 15724.498046875, 17878.58203125, 16155.3134765625, 18159.625, 16172.05078125, 17382.794921875, 16107.341796875, 17313.5390625, 16483.404296875, 17252.341796875, 16464.197265625, 17786.560546875, 16568.794921875, 17770.6953125, 16597.220703125, 18251.64453125, 17074.072265625, 18531.99609375, 17140.28515625, 18475.177734375, 17254.068359375, 18316.220703125, 17492.2578125, 18841.234375, 17812.646484375, 18745.146484375, 17495.0234375, 19517.84765625, 17125.201171875, 18900.5859375, 18097.98046875, 19135.197265625, 18450.5703125, 19601.57421875, 16863.998046875, 19617.59765625, 17484.56640625, 19359.19140625, 17953.666015625, 19218.322265625, 18016.2109375, 20058.076171875, 18202.548828125, 19723.52734375, 17070.009765625, 19954.546875, 18845.3046875, 20389.919921875, 17590.349609375, 19529.072265625, 18316.64453125, 21206.703125, 19056.572265625, 20005.666015625, 18007.3671875, 20317.125, 19044.81640625, 20686.09765625, 18918.6328125, 20623.279296875, 17837.052734375, 20518.95703125, 19015.080078125, 20039.18359375, 18911.501953125, 20081.958984375, 19192.466796875, 20493.732421875, 18547.373046875, 20818.744140625, 19251.302734375, 20841.6015625, 18938.953125, 21136.314453125, 18364.234375, 21031.203125, 18937.564453125, 21695.7734375, 19253.24609375, 21248.470703125, 19193.845703125, 21972.640625, 18921.76171875, 21569.33203125, 19967.01953125, 21113.927734375, 19555.97265625, 21477.498046875, 19123.1015625, 20727.06640625, 19791.390625, 22219.658203125, 19673.5703125, 21425.2734375, 19877.919921875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('15dad771-d18e-4fff-8d11-983538b1753d'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; 5.4.3 LSTM NN \u00b6 back to top Our data preparation for the LSTM NN includes time steps. The parameter input_dim tells our LSTM block how man time steps we have in the input data. This is a reframing (and a more appropriate reframing) of the same problem. The LSTM model is viewing the input feature w/ multiple time steps as a single feature at different times, rather than separate features. We could, for instance, have a second dimension that includes non-time related information, such as the customer name or truffle types (or other featurse that also vary through time, multiple feed rates or T/P, etc). window_size = 6 batch_size = 10 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X = X . reshape ( - 1 , 1 , window_size ) y = y . reshape ( - 1 , 1 , 1 ) model = keras . Sequential ([ layers . LSTM ( 8 , activation = 'relu' , input_dim = window_size ), layers . Dense ( 8 ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X , y , batch_size = batch_size , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 411 322288.40625 412 361232.25000 413 336341.78125 414 326199.90625 415 320201.21875 results = melt_results ( model , X , y . flatten ()) px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"60577dfc-eefb-4e74-806a-184d8dbf0f50\")) { Plotly.newPlot( '60577dfc-eefb-4e74-806a-184d8dbf0f50', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [13180.5244140625, 11941.052734375, 13897.1728515625, 12522.2275390625, 14309.3193359375, 13096.4521484375, 14802.939453125, 13020.638671875, 14922.9716796875, 13130.0478515625, 14894.341796875, 13569.859375, 15329.9990234375, 14020.2109375, 15622.0634765625, 14315.18359375, 15828.01953125, 14755.5458984375, 15874.12890625, 14772.2138671875, 16116.365234375, 14683.439453125, 16555.751953125, 15248.7373046875, 16811.826171875, 15624.388671875, 17435.9453125, 15604.052734375, 17160.962890625, 15268.173828125, 16771.892578125, 14809.287109375, 16804.845703125, 15244.541015625, 17437.248046875, 16029.177734375, 18312.34765625, 16288.9140625, 17983.26171875, 16235.626953125, 17556.31640625, 16354.9541015625, 17489.228515625, 16529.060546875, 17760.345703125, 16555.396484375, 17977.400390625, 16594.361328125, 18230.009765625, 16900.791015625, 18635.373046875, 17209.576171875, 18730.90234375, 17256.3203125, 18582.009765625, 17442.232421875, 18845.5390625, 17683.0703125, 19098.67578125, 17734.451171875, 19381.06640625, 17376.384765625, 19372.57421875, 17649.228515625, 19386.1328125, 18351.927734375, 19727.396484375, 17688.015625, 19858.5234375, 17253.630859375, 19738.755859375, 17737.013671875, 19527.080078125, 18018.39453125, 20046.962890625, 18187.865234375, 20121.90234375, 17573.572265625, 20144.68359375, 18101.638671875, 20523.671875, 18151.841796875, 20122.90625, 17993.490234375, 20761.791015625, 18826.2734375, 20869.994140625, 18526.685546875, 20408.02734375, 18580.732421875, 20915.5, 19076.302734375, 20948.8515625, 18448.703125, 20821.078125, 18561.212890625, 20518.23046875, 19026.8984375, 20366.970703125, 19106.486328125, 20562.0234375, 18892.654296875, 20958.59375, 18943.490234375, 21216.8046875, 19141.826171875, 21336.763671875, 18609.283203125, 21359.861328125, 18677.748046875, 21713.556640625, 19097.056640625, 21832.57421875, 19257.25390625, 21906.306640625, 19140.806640625, 22075.396484375, 19541.294921875, 21693.501953125, 19897.388671875, 21610.060546875, 19310.5859375, 21347.126953125, 19513.626953125, 21818.173828125, 19804.263671875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('60577dfc-eefb-4e74-806a-184d8dbf0f50'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; \ud83c\udfcb\ufe0f Exercise 3: Compare LSTM with FFNN using Train/Val/Test sets and 3 Month Window \u00b6 back to top ### YOUR OPT WINDOW SIZE FROM EXERCISE 2 ### window_size = 3 batch_size = 10 patience = 50 # training on single order history data = orders . loc [[ 0 ], time_cols ] . values # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [ 0.6 , 0.8 ] X_train , y_train , X_val , y_val , X_test , y_test = train_test_process ( data , train_test_val_ratios ) ### YOUR EARLY STOPPING FORMULA ### ### YOUR MODEL FROM EX 6.3.3.2 ### # compile the model model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) # fit the model history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) print ( pd . DataFrame ( history . history ) . tail ()) train size: 76 val size: 23 test size: 24 loss val_loss 635 272437.62500 815874.4375 636 274113.68750 810266.5000 637 275288.75000 817768.3750 638 274128.71875 800769.0625 639 272784.25000 800416.8125 We'll then record the mse performance of the model to later compare with the LSTM results = [] y_pred = model . predict ( X_test ) mse = mean_squared_error ( y_test , y_pred ) results . append ([ 'Dense' , mse ]) results [['Dense', 415929.951056031]] We'll use the same parameters (window size, batch size, and early stopping to train the LSTM and compare the optimum FFNN architecture we previously used) X_train = X_train . reshape ( - 1 , 1 , window_size ) y_train = y_train . reshape ( - 1 , 1 , 1 ) X_val = X_val . reshape ( - 1 , 1 , window_size ) y_val = y_val . reshape ( - 1 , 1 , 1 ) X_test = X_test . reshape ( - 1 , 1 , window_size ) y_test = y_test . reshape ( - 1 , 1 , 1 ) model = keras . Sequential ([ layers . LSTM ( 8 , activation = 'relu' , input_dim = window_size ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , ) print ( pd . DataFrame ( history . history ) . tail ()) loss val_loss 1286 267132.96875 767315.0625 1287 266946.25000 770766.9375 1288 270205.12500 748175.3750 1289 267703.06250 749915.8125 1290 266674.15625 758844.8125 y_pred = model . predict ( X_test ) mse = mean_squared_error ( y_test . flatten (), y_pred . flatten ()) results . append ([ 'LSTM' , mse ]) Comparison of results: pd . DataFrame ( results , columns = [ 'Model' , 'Test MSE' ]) . set_index ( 'Model' ) . astype ( int ) [['Dense', 415929.951056031], ['LSTM', 393786.0529972827]] As a last visualization in this exercise we'll look at the trian/val/test predictions along the actual data = orders . loc [[ 0 ], time_cols ] . values idx_split1 = int ( data . shape [ 1 ] * train_test_val_ratios [ 0 ]) idx_split2 = int ( data . shape [ 1 ] * train_test_val_ratios [ 1 ]) y_p_train = model . predict ( X_train ) y_p_val = model . predict ( X_val ) y_p_test = model . predict ( X_test ) new = orders . loc [[ 0 ], time_cols ] . T . reset_index () new . columns = [ 'Date' , 'Real' ] new [ 'Train' ] = np . nan new . iloc [ window_size : idx_split1 , 2 ] = y_p_train new [ 'Val' ] = np . nan new . iloc [ idx_split1 + window_size : idx_split2 , 3 ] = y_p_val new [ 'Test' ] = np . nan new . iloc [ idx_split2 + window_size :, 4 ] = y_p_test new = new . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) px . line ( new , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"36fed7ea-f858-4788-9a4e-ee6fe8319e47\")) { Plotly.newPlot( '36fed7ea-f858-4788-9a4e-ee6fe8319e47', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12570.33516482565, 11569.168746227244, 13616.8122044598, 11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Train<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Train\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Train\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, 11530.80859375, 13547.0107421875, 12017.6533203125, 13778.6494140625, 12893.5078125, 14261.6748046875, 12927.7666015625, 15076.15234375, 12940.1923828125, 14507.783203125, 13324.0244140625, 14885.705078125, 13578.6259765625, 15323.1728515625, 14155.8232421875, 15607.4013671875, 14294.25, 15480.1943359375, 15074.0634765625, 15926.88671875, 14279.1435546875, 16202.197265625, 14831.9873046875, 16323.2197265625, 15521.9248046875, 16855.90234375, 15203.2568359375, 17469.328125, 15860.4794921875, 16587.314453125, 14690.3681640625, 16879.89453125, 14974.7041015625, 16384.044921875, 15324.7646484375, 17848.638671875, 16040.9189453125, 18219.875, 16360.8349609375, 17463.84375, 16121.544921875, 17262.85546875, 16494.96875, 17322.7890625, 16306.2822265625, 17820.662109375, 16557.044921875, 17838.72265625, 16453.333984375, 18195.025390625, 16977.841796875, 18581.884765625, 17140.587890625, 18504.537109375, 17291.974609375, 18320.216796875, 17343.390625, 18812.208984375, 17820.373046875, 18920.37109375, 17241.39453125, 19696.1484375, 17286.443359375, 18702.0078125, 18034.021484375, 19108.53515625, 18266.767578125, 20135.396484375, 16819.486328125, 19538.892578125, 17530.896484375, 19302.970703125, 17974.005859375, 19292.216796875, 17759.21484375, 20087.013671875, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Val<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Val\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Val\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 19562.25, 18666.6640625, 20812.396484375, 17802.416015625, 19452.72265625, 17828.015625, 21070.75, 19337.732421875, 20385.990234375, 17909.234375, 20121.26171875, 18914.283203125, 20805.439453125, 18875.658203125, 21021.876953125, 17844.279296875, 20268.6875, 19130.21875, 20132.666015625, 18896.615234375, 20078.298828125, 19047.263671875, 20762.689453125, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Test<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Test\", \"line\": {\"color\": \"#ab63fa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Test\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 21025.208984375, 18800.91015625, 21401.697265625, 18349.783203125, 20994.505859375, 18708.802734375, 21705.333984375, 19330.486328125, 21383.529296875, 18933.072265625, 22155.6640625, 19004.689453125, 21361.583984375, 20062.9765625, 21315.884765625, 19415.896484375, 21672.541015625, 19315.009765625, 20655.81640625, 19342.04296875, 22334.814453125, 19861.693359375, 21464.390625, 19739.919921875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('36fed7ea-f858-4788-9a4e-ee6fe8319e47'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; 5.5 Model Extensibility \u00b6 back to top from ipywidgets import interact \ud83c\udfcb\ufe0f Exercise 4: Apply Model to Other Orders \u00b6 Take the last LSTM model and apply it to other orders in the dataset. What do you notice? back to top def apply_model ( dataset = orders . index , window_size = 3 ): window_size = window_size data = pd . DataFrame ( orders . loc [ dataset , time_cols ]) data = data . reset_index () data . columns = [ 'Date' , 'KG' ] X , y , labels = process_data ( orders . loc [[ dataset ], time_cols ] . values , window = window_size , time_cols = 132 ) y_pred = model . predict ( X . reshape ( - 1 , 1 , window_size )) . flatten () results = pd . DataFrame ( y_pred , y ) results = results . reset_index () results . index = data [ 'Date' ][ window_size :] results = results . reset_index () results . columns = [ 'Date' , 'real' , 'predicted' ] results = results . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) fig = px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) return fig interact ( apply_model ) interactive(children=(Dropdown(description='dataset', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1\u2026 <function __main__.apply_model> \ud83c\udfcb\ufe0f Exercise-Discussion 5.1: How Would You Create a General Forecast Model? \u00b6 back to top After exploring how your model does on other order histories, what do you think is a good strategy for developing company wide order forecasts? Some possible questions: should you create a single model for the whole company? could you embed meta data about the order in this all-inclusive model? should you make models specific to certain customers, products, etc. what kind of analysis could you do before hand to determine how your models should be grouped? melted = orders . melt ( id_vars = [ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' , 'customer' ], var_name = 'month' , value_name = 'kg' ) def my_eda ( color = cat_cols ): fig = px . line ( melted , x = 'month' , y = 'kg' , color = color ) return fig interact ( my_eda ) interactive(children=(Dropdown(description='color', options=('base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.my_eda> \ud83c\udfcb\ufe0f Exercise 5.2: EDA \u00b6 back to top In our quest to create a model that works well for all orders to truffltopia. I tell you that there are some orders with patterned behavior, according to their meta data. Your first task, is to find out which categorical variable best separates the data. You can use any statistical or visual method you like # recall the categorical variables: ['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group', 'customer'] From C1 S6, it may be useful to think of this diagram: \ud83c\udfcb\ufe0f Exercise 5.3: Decide on Model \u00b6 back to top Will you model the whole dataset together? Will you create a number of submodels? Choose based on the groupings you determined statistically significant in the data. As a base comparison I have provided a formula that trains a model on the entire order history: data = orders data = data[time_cols].values batch_size = 256 window_size = 12 print(\"batch size: {}\".format(batch_size)) print(\"window size: {}\".format(window_size), end='\\n\\n') # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [0.8, 0.9] X_train, y_train, X_val, y_val, X_test, y_test = train_test_process(data, train_test_val_ratios, window_size) early_stopping = keras.callbacks.EarlyStopping( patience=50, min_delta=0.001, restore_best_weights=True, monitor='loss' ) model = keras.Sequential([ layers.Dense(8, input_shape=[window_size]), layers.Dense(16), layers.Dense(32), layers.Dense(16), layers.Dense(1) ]) model.compile(loss='mean_squared_error', optimizer='adam') history = model.fit( X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=10000, callbacks=[early_stopping], verbose=0, # hide the output because we have so many epochs ) print(pd.DataFrame(history.history).tail()) data = orders data = data [ time_cols ] . values batch_size = 256 window_size = 12 print ( \"batch size: {} \" . format ( batch_size )) print ( \"window size: {} \" . format ( window_size ), end = ' \\n\\n ' ) # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [ 0.8 , 0.9 ] X_train , y_train , X_val , y_val , X_test , y_test = train_test_process ( data , train_test_val_ratios , window_size ) early_stopping = keras . callbacks . EarlyStopping ( patience = 50 , min_delta = 0.001 , restore_best_weights = True , monitor = 'loss' ) model = keras . Sequential ([ layers . Dense ( 8 , input_shape = [ window_size ]), layers . Dense ( 16 ), layers . Dense ( 32 ), layers . Dense ( 16 ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) print ( pd . DataFrame ( history . history ) . tail ()) batch size: 256 window size: 12 train size: 9300 val size: 100 test size: 200 loss val_loss 326 279111.15625 953265.0625 327 322529.15625 580780.2500 328 285901.56250 476007.4375 329 302237.68750 496192.8125 330 281779.40625 480916.6250 And a history of the loss with the following settings: batch size: 256 window size: 12 train size: 9300 val size: 100 test size: 200 loss val_loss 326 279111.15625 953265.0625 327 322529.15625 580780.2500 328 285901.56250 476007.4375 329 302237.68750 496192.8125 330 281779.40625 480916.6250 interact ( apply_model , window_size = window_size ) interactive(children=(Dropdown(description='dataset', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1\u2026 <function __main__.apply_model>","title":"Time Series Analysis"},{"location":"S5_Time_Series_Analysis/#general-applications-of-neural-networks-session-5-recurrent-neural-networks-and-time-series-analysis","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com In this session, we'll be exploring NN as they apply to sequenced data, specifically time series data.","title":"General Applications of Neural Networks  Session 5: Recurrent Neural Networks and Time Series Analysis"},{"location":"S5_Time_Series_Analysis/#50-preparing-environment-and-importing-data","text":"back to top","title":"5.0 Preparing Environment and Importing Data"},{"location":"S5_Time_Series_Analysis/#501-import-packages","text":"back to top import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px import random from scipy.stats import gamma , norm , expon from ipywidgets import interact from statsmodels.tsa.stattools import pacf , acf from sklearn.metrics import mean_squared_error def melt_results ( model , X , y ): y_pred = model . predict ( X ) results = pd . DataFrame ( y_pred , y ) results = results . reset_index () results . index = data [ 'Date' ][ window_size :] results = results . reset_index () results . columns = [ 'Date' , 'real' , 'predicted' ] results = results . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) return results def process_data ( Xy , window = 3 , time_cols = 12 , remove_null = False ): \"\"\" This function splits your time series data into the proper windows Parameters ---------- Xy: array The input data. If there are non-time series columns, assumes they are on the left and time columns are on the right. time_cols: int The number of time columns, default 12 window: int The time window size, default 3 Returns ------- X_: array The independent variables, includes time and non-time series columns with the new window y_: array The dependent variable, selected from the time columns at the end of the window labels: The time series labels, can be used in subsequent plot \"\"\" # separate the non-time series columns X_cat = Xy [:,: - time_cols ] # select the columns to apply the sweeping window X = Xy [:, - time_cols :] X_ = [] y = [] for i in range ( X . shape [ 1 ] - window ): # after attaching the current window to the non-time series # columns, add it to a growing list X_ . append ( np . concatenate (( X_cat , X [:, i : i + window ]), axis = 1 )) # add the next time delta after the window to the list of y # values y . append ( X [:, i + window ]) # X_ is 3D: [number of replicates from sweeping window, # length of input data, # size of new feature with categories and time] # we want to reshape X_ so that the replicates due to the sweeping window is # a part of the same dimension as the instances of the input data X_ = np . array ( X_ ) . reshape ( X . shape [ 0 ] * np . array ( X_ ) . shape [ 0 ], window + X_cat . shape [ 1 ]) y = np . array ( y ) . reshape ( X . shape [ 0 ] * np . array ( y ) . shape [ 0 ],) if remove_null : # remove training data where the target is 0 (may be unfair advantage) X_ = X_ [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] y = y [ np . where ( ~ np . isnan ( y . astype ( float )))[ 0 ]] # create labels that show the previous month values used to train the model labels = [] for row in X_ : labels . append ( \"X: {} \" . format ( np . array2string ( row [ - window :] . astype ( float ) . round ()))) return X_ , y , labels def train_test_process ( data , train_test_val_ratios = [ 0.6 , 0.8 ], window_size = 3 ): # get the indices at the associated ratios idx_split1 = int ( data . shape [ 1 ] * train_test_val_ratios [ 0 ]) idx_split2 = int ( data . shape [ 1 ] * train_test_val_ratios [ 1 ]) # index the data to build the sets data_train = data [:,: idx_split1 ] data_val = data [:, idx_split1 : idx_split2 ] data_test = data [:, idx_split2 :] # build out the training sets with the sweeping window method X_train , y_train , labels = process_data ( data_train , window = window_size , time_cols = 132 ) X_val , y_val , labels = process_data ( data_val , window = window_size , time_cols = 132 ) X_test , y_test , labels = process_data ( data_test , window = window_size , time_cols = 132 ) print ( \"train size: {} \" . format ( X_train . shape [ 0 ])) print ( \"val size: {} \" . format ( X_val . shape [ 0 ])) print ( \"test size: {} \" . format ( X_test . shape [ 0 ]), end = ' \\n\\n ' ) return X_train , y_train , X_val , y_val , X_test , y_test","title":"5.0.1 Import Packages"},{"location":"S5_Time_Series_Analysis/#502-load-dataset","text":"back to top orders = pd . read_csv ( \"https://raw.githubusercontent.com/wesleybeckner/\" \\ \"truffletopia/main/truffletopia/data/12_year_orders.csv\" ) cat_cols = [ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' , 'customer' ] time_cols = [ i for i in orders . columns if i not in cat_cols ] # note that our data is 'untidy' if we wanted to tidy the data we would need to # unpivot or 'melt' our date columns like so: orders . melt ( id_vars = cat_cols , var_name = 'date' , value_name = 'kg' ) # however the data as it is, is useful for our purposes of timeseries prediction # today .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group customer date kg 0 Cheese Candy Outer Horchata Vanilla Amethyst Perk-a-Cola 1/2010 12570.335165 1 Tiramisu Chocolate Outer Irish Cream Egg Nog Slate Dandy's Candies 1/2010 7922.970436 2 Sponge Chocolate Outer Ginger Ale Apple Slate Dandy's Candies 1/2010 10521.306722 3 Cheese Chocolate Outer Coffee Pear Opal Dandy's Candies 1/2010 4739.122200 4 Chiffon Jelly Filled Butter Toffee Apricot Olive Slugworth 1/2010 2756.891961 ... ... ... ... ... ... ... ... ... 13195 Chiffon Chocolate Outer Acai Berry Tangerine Slate Fickelgruber 12/2020 25714.512372 13196 Butter Jelly Filled Plum Peppermint Olive Fickelgruber 12/2020 15043.303525 13197 Chiffon Chocolate Outer Wild Cherry Cream Peppermint Taupe Perk-a-Cola 12/2020 8769.613116 13198 Cheese Candy Outer Mango Mango Rose Dandy's Candies 12/2020 5065.975534 13199 Sponge Chocolate Outer Ginger Ale Passion Fruit Black Fickelgruber 12/2020 9466.712219 13200 rows \u00d7 8 columns","title":"5.0.2 Load Dataset"},{"location":"S5_Time_Series_Analysis/#51-why-we-think-in-sequences","text":"back to top There are some problems that are best framed as a sequence in either the input or the output. For example, in our image classification we are performing a mapping of many-to-one: sequence input (the pixels) to a single output (classification). Other examples include: One-to-many: sequence output, e.x. word (if treated as a single input) to generate a picture Many-to-many: sequence input and output, e.x. machine translation (like english to mandarin) Synchronized many-to-many: synced sequence input and output, e.x. video classification State of the art handling of sequences has occurred in a class of networks called recurrent neural networks","title":"5.1 Why We Think in Sequences"},{"location":"S5_Time_Series_Analysis/#52-recurrent-neural-networks","text":"back to top Recurrent Neural Networks (RNNs) can be thought of as a FFNN with loops added into the architecture. This allows the network to retain information, create \"memory\" that can be associated with signals later in the sequence. We didn't go into much detail about the actual training algorithm of neural networks: back propagation . But what we will say here, is that this algorithm breaks down with recurrent neural networks because of the looped connections. A trick was created to overcome this, where the looped connections are unrolled, using a copy of the \"unhooked\" neuron to represent where the loop was initally fed back. This algorithm is called back propagation through time . Another problem is introduced when training recurrent neural networks, in that the gradients calculated during back propagation can become very large, exploding gradients , or very small vanishing gradients . This problem is modulated in FNNNs by the ReLU, In RNNs, a more sophisticated gating mechanism is used in an architecture we call Long Short-Term Memory Networks LSTM shown in both typical and unfolded format","title":"5.2 Recurrent Neural Networks"},{"location":"S5_Time_Series_Analysis/#521-long-short-term-memory-networks","text":"back to top Long Short-Term Memory Networks (LSTMs) are a type of RNN that are trained using back propagation through time and overcome the vanishing/exploding gradient problem. Similar to CNNs, their architecture is composed of blocks, this time with memory blocks rather than convolutional blocks. A block is smarter than the classical neuron; it contains gates that manage the block's state and output. The gates are operated by a sigmoid function, determining whether they are open or closed (triggered or not trigerred). There are three types of gates within a memory block: Forget gate: decides what information is discarded Input gate: decides what information updates the memory state Output gate: decides what information to send forward depending on the input and memory state These weights that configure these gates are learned during training, and their coordination allow each memory block to learn sophisticated relationships in and among sequenced data. Big takeaway: memory blocks contain trainable parameters that allow the block to learn relationships between sequenced data","title":"5.2.1 Long Short-Term Memory Networks"},{"location":"S5_Time_Series_Analysis/#53-exploratory-data-analysis-with-plotlypandas","text":"back to top orders . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base_cake truffle_type primary_flavor secondary_flavor color_group customer 1/2010 2/2010 3/2010 4/2010 5/2010 6/2010 7/2010 8/2010 9/2010 10/2010 11/2010 12/2010 1/2011 2/2011 3/2011 4/2011 5/2011 6/2011 7/2011 8/2011 9/2011 10/2011 11/2011 12/2011 1/2012 2/2012 3/2012 4/2012 5/2012 6/2012 7/2012 8/2012 9/2012 10/2012 ... 9/2017 10/2017 11/2017 12/2017 1/2018 2/2018 3/2018 4/2018 5/2018 6/2018 7/2018 8/2018 9/2018 10/2018 11/2018 12/2018 1/2019 2/2019 3/2019 4/2019 5/2019 6/2019 7/2019 8/2019 9/2019 10/2019 11/2019 12/2019 1/2020 2/2020 3/2020 4/2020 5/2020 6/2020 7/2020 8/2020 9/2020 10/2020 11/2020 12/2020 0 Cheese Candy Outer Horchata Vanilla Amethyst Perk-a-Cola 12570.335165 11569.168746 13616.812204 11884.370881 13950.332334 12781.156536 14256.210023 12887.711960 15038.574006 12626.489306 14611.291109 13194.814300 14921.016216 13477.391457 15409.211080 13999.215069 15597.436976 14098.124978 15596.818092 14941.694032 15715.347212 14181.212142 16282.098006 14650.929410 16433.209008 15400.579034 16756.981263 15128.148250 17523.979943 15413.044691 16366.264377 14568.470959 16901.111542 14659.021365 ... 20736.279239 18617.387585 20783.711234 17470.755865 20523.579840 18796.936906 20028.582493 18677.535295 20048.107422 18929.248617 20571.155902 18207.204656 20839.042892 18966.532984 20909.977545 18589.807152 21287.370123 17987.976867 21111.062685 18538.311321 21797.267132 18935.352772 21331.378420 18783.759611 22139.123373 18553.797271 21579.506284 19726.433111 21147.624131 19232.360491 21575.521051 18856.178110 20701.250676 19406.448560 22328.687163 19384.824042 21449.154890 19554.405590 21873.104938 19572.860127 1 Tiramisu Chocolate Outer Irish Cream Egg Nog Slate Dandy's Candies 7922.970436 6464.558625 6616.092291 8244.991928 6602.132649 7032.700478 8437.517865 6919.862786 7003.449554 8516.767749 7541.471510 7145.880001 8821.556334 7325.240199 7618.246523 9385.832260 7705.860411 7709.843383 9471.542415 7791.968645 8214.485181 9393.875864 7911.159820 8181.208339 9750.400651 8084.792749 8234.370603 9777.179731 8134.876166 8449.426300 9911.512891 8686.751188 8359.638696 10147.391908 ... 9709.721305 12367.917469 10109.247976 10597.517266 12723.546352 10535.166596 10043.352958 12434.413543 10594.590384 10473.165191 12323.923036 10232.771159 9973.322947 12450.632426 10247.199668 10310.557749 12527.881699 10288.118368 10792.495418 12538.911064 10564.257282 10672.337286 12442.348062 10975.342816 10504.218214 12700.925307 10853.622645 10917.981718 13005.963533 10610.202654 10145.394106 13132.925131 10821.805709 10829.961838 12995.340352 10504.814195 10617.199735 13377.165673 11065.835571 11135.386324 2 Sponge Chocolate Outer Ginger Ale Apple Slate Dandy's Candies 10521.306722 5543.335645 5294.892374 11010.452413 5267.190367 5546.045669 11394.362620 5712.245098 5798.349463 11781.306993 5918.299339 5892.693500 12298.538324 6260.141878 6244.742736 12336.799353 6201.024217 6638.056331 12661.775736 6545.099808 6536.149679 12757.183923 6717.449248 6473.324997 13467.205626 6806.690857 7052.340323 13488.253357 6613.263036 7017.902839 13676.458530 6925.928151 6931.425252 13418.564207 ... 8486.837094 16402.754127 8037.467789 8184.225799 16627.369839 8287.853805 8307.474776 16869.445360 8090.179106 8258.503638 16961.500038 8261.880924 8221.327715 16468.858014 8243.277819 8381.807681 16778.471019 8673.021737 8552.964237 16902.958241 8346.914162 8431.816880 17088.184916 8928.738286 8511.985934 17273.307554 8242.173578 8755.268429 16961.225344 8686.594959 8516.098910 17498.911792 8369.846849 8334.206937 17519.678690 8595.378915 8909.348040 17234.636475 9002.216839 8794.467252 3 Cheese Chocolate Outer Coffee Pear Opal Dandy's Candies 4739.122200 2733.281035 4984.394797 2750.709519 5274.473185 2737.736109 5236.191952 2807.504142 5581.285441 2500.882597 5635.195267 3035.782263 5492.449149 2987.850135 6021.641513 3141.406171 5884.424448 2898.492508 5925.633348 2990.291270 6055.228068 3587.204540 6138.483396 2997.648037 6370.293979 3442.123255 6187.807429 3140.255766 6243.439310 3568.780726 6393.113315 3246.908338 6540.176631 3383.552285 ... 7756.552767 4233.577541 8119.681843 4046.102552 7738.053080 4401.610595 7675.782975 4248.467389 7835.989038 3784.924538 7778.705078 4023.523503 7996.384938 4236.811930 7780.903630 4324.838890 8706.750222 4592.025934 7864.924599 3831.383360 8121.771691 4428.045665 8216.481440 4193.238269 8316.693448 4591.368108 8184.213024 4112.875085 8342.099081 4074.668047 8093.541144 4301.081977 8235.616589 4151.474242 8213.665500 4008.885583 7912.641813 4275.162782 8031.227879 4628.989194 4 Chiffon Jelly Filled Butter Toffee Apricot Olive Slugworth 2756.891961 1739.900797 1791.975108 1533.023665 1735.868123 1824.082183 2637.470462 1707.745100 1621.994676 1814.318681 1669.867919 1681.547977 2830.947230 1732.084718 1739.759848 1647.585142 1750.293269 1710.317714 2805.902600 1751.767883 2009.631273 1937.539054 1735.097831 1839.490125 2799.159990 1566.971234 1752.097294 1573.071123 1760.829823 1795.254784 2754.540709 1886.366523 1807.204970 1740.528223 ... 1842.142055 1500.057312 1731.064289 1675.758204 2904.893170 1789.992333 1810.125486 1795.379685 1907.771505 1973.079708 2865.869167 1574.850737 1783.459110 1787.164751 1752.689655 1734.478146 2804.497744 1956.641539 1909.752412 1693.251443 1748.211959 1842.843637 2757.454985 1674.184059 1698.962332 1631.735285 1769.115620 1663.851403 2919.917902 1830.186857 1864.015449 1800.566323 1625.130275 1908.316219 2696.631511 1859.017636 1690.042699 1764.410866 1909.608709 1711.780317 5 rows \u00d7 138 columns data = pd . DataFrame ( orders . loc [ 0 , time_cols ]) data = data . reset_index () data . columns = [ 'Date' , 'KG' ] data px . scatter ( data , x = 'Date' , y = 'KG' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"9499d748-9bea-476f-9026-a591288a6ec7\")) { Plotly.newPlot( '9499d748-9bea-476f-9026-a591288a6ec7', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12570.33516482565, 11569.168746227244, 13616.8122044598, 11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('9499d748-9bea-476f-9026-a591288a6ec7'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 )) pd . plotting . autocorrelation_plot ( data [ 'KG' ], ax = ax ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb1b29f3610> Normally with time series data, we'd want to try a host of preprocessing techniques and remove the trend (really create two separate analyses, one of the trend and one of the seasonality) but to keep things simple and to showcase the utility of machine learning, we are going to deviate from the stats-like approach and work with our data as is. For more details on the stats-like models you can perform a cursory search on ARIMA , ARMA , SARIMA","title":"5.3 Exploratory Data Analysis with Plotly/Pandas"},{"location":"S5_Time_Series_Analysis/#54-modeling","text":"back to top from tensorflow import keras from tensorflow.keras import layers","title":"5.4 Modeling"},{"location":"S5_Time_Series_Analysis/#541-sweeping-rolling-window","text":"back to top We're going to revist this idea of a sweeping window from our feature engineering disucssion. It turns out, even though we are using a NN, there is still some preprocessing we need to do. In our case, each time delta is represented by a month. So we will choose some number of months to include in our feature set, this will in turn determine what our overall training data will look like. Xy = orders . loc [[ 0 ], time_cols ] . values # separate the non-time series columns X_cat = Xy [:,: - 120 ] # select the columns to apply the sweeping window X = Xy [:, - 120 :] with a window size of 3, our X will have 3 features, the prior 3 months leading up to the month for which we will attempt to forecast. window_size = 3 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X [: 5 ] array([[12570.33516483, 11569.16874623, 13616.81220446], [11569.16874623, 13616.81220446, 11884.37088102], [13616.81220446, 11884.37088102, 13950.33233441], [11884.37088102, 13950.33233441, 12781.15653568], [13950.33233441, 12781.15653568, 14256.21002336]]) With a window size of 1, our X data will have a feature size of 1 window_size = 1 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X [: 5 ] array([[12570.33516483], [11569.16874623], [13616.81220446], [11884.37088102], [13950.33233441]]) and so on.","title":"5.4.1 Sweeping (Rolling) Window"},{"location":"S5_Time_Series_Analysis/#542-ffnn","text":"back to top I'm going to start with a very simple FFNN model: model = keras . Sequential ([ layers . Dense ( 8 , input_shape = [ window_size ]), # one layer, 8 nodes layers . Dense ( 1 ) # single output for the kg ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) early_stopping = keras . callbacks . EarlyStopping ( patience = 10 , min_delta = 0.001 , restore_best_weights = True , monitor = 'loss' ) history = model . fit ( X , y , batch_size = 10 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 23 4250741.5 24 4245127.5 25 4246370.5 26 4248100.0 27 4242104.0 As we can see from the y vs y_pred the FFNN is just predicting the previous month's value: y_pred = model . predict ( X ) pd . DataFrame ( y_pred , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 11569.168746 12474.020508 13616.812204 11480.559570 11884.370881 13512.444336 13950.332334 11793.336914 12781.156536 13843.397461 ... ... 19384.824042 22157.271484 21449.154890 19236.064453 19554.405590 21284.505859 21873.104938 19404.339844 19572.860127 21705.197266 131 rows \u00d7 1 columns We can try this with a more suitable window size window_size = 3 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) model = keras . Sequential ([ # layers.Dense(8, input_shape=[window_size]), layers . Dense ( 1 , input_shape = [ window_size ]) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X , y , batch_size = 10 , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 703 514088.96875 704 513142.15625 705 507798.15625 706 511337.62500 707 513890.40625 A cursory glance looks like our values are closer together results = melt_results ( model , X , y ) px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d52a6cbd-a684-4ccd-bc70-41064f9370ef\")) { Plotly.newPlot( 'd52a6cbd-a684-4ccd-bc70-41064f9370ef', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12355.0859375, 13640.7431640625, 12307.1044921875, 14332.3984375, 13131.0283203125, 14212.7783203125, 13552.763671875, 14700.265625, 12652.4365234375, 14801.03515625, 13574.015625, 14962.173828125, 13970.4462890625, 15578.1806640625, 14305.185546875, 15540.5673828125, 14297.0986328125, 16009.984375, 15129.0732421875, 15213.9912109375, 14734.306640625, 16403.990234375, 14958.9521484375, 16761.474609375, 15755.279296875, 16499.529296875, 15817.58203125, 17504.912109375, 14965.267578125, 15796.3193359375, 15130.4443359375, 16766.73828125, 14686.5009765625, 16811.646484375, 16451.060546875, 18008.30078125, 16300.5703125, 18110.662109375, 15727.3662109375, 17264.041015625, 16023.4013671875, 17446.83203125, 16421.3203125, 17137.51953125, 16727.669921875, 17762.119140625, 16536.51953125, 17676.62109375, 16819.26953125, 18412.998046875, 17204.791015625, 18482.048828125, 17078.1171875, 18459.734375, 17126.396484375, 18339.029296875, 17736.703125, 18938.0390625, 17749.787109375, 18435.4296875, 17923.501953125, 19243.39453125, 16717.30078125, 19337.271484375, 18180.85546875, 19215.458984375, 18766.697265625, 18614.162109375, 16836.208984375, 19861.880859375, 17306.4375, 19527.3203125, 17852.443359375, 19114.693359375, 18452.845703125, 20069.751953125, 18064.07421875, 18975.03515625, 17102.572265625, 20803.421875, 19135.88671875, 19629.9375, 17069.33984375, 19737.794921875, 19195.9765625, 21565.546875, 18428.048828125, 19303.3515625, 18109.71875, 20768.50390625, 19236.8671875, 20509.314453125, 18924.150390625, 19914.69140625, 17708.232421875, 21088.181640625, 18728.63671875, 19888.515625, 18887.47265625, 20122.76171875, 19426.662109375, 20018.087890625, 18674.275390625, 21098.416015625, 19263.26953125, 20539.78125, 19121.091796875, 20701.00390625, 18277.365234375, 21191.203125, 19286.3203125, 21770.748046875, 19003.40234375, 21054.41796875, 19604.93359375, 21716.5625, 18637.990234375, 22055.3515625, 19713.060546875, 20759.974609375, 19754.70703125, 21166.15234375, 18657.068359375, 20912.896484375, 20605.359375, 22080.318359375, 19207.404296875, 21405.28515625, 20073.166015625], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('d52a6cbd-a684-4ccd-bc70-41064f9370ef'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };","title":"5.4.2 FFNN"},{"location":"S5_Time_Series_Analysis/#exercise-discussion-1-varify-that-the-model-is-linear","text":"back to top We're having to change our way of thinking here with time series analysis. Recall that a model without an activation function can only encapsulate linear relationships. How come we can see non-linear relationships in our time series plot above? make a plot that showcases we are indeed still within a linear world. This is an open ended question and I myself don't have the best answer. Think about how you would attempt to show linearity of the model. (On Monday our model was only in 2D, and it was a binary classification task, so it was easier to view the decision boundaries and verify linearity). # Code cell for Exercise 1","title":"\ud83c\udfcb\ufe0f Exercise-Discussion 1: Varify that the model is linear"},{"location":"S5_Time_Series_Analysis/#exercise-2-vary-model-architecture-and-window-size","text":"back to top Create these three different models. Train on the whole dataset with a window size of 3. record the training loss for the last 5 epochs of each model models = [ keras.Sequential([ layers.Dense(8, input_shape=[window_size]), layers.Dense(1) ]), keras.Sequential([ layers.Dense(8, activation='relu', input_shape=[window_size]), layers.Dense(1) ]), keras.Sequential([ layers.Dense(4, activation='relu', input_shape=[window_size]), layers.Dense(1) ])] You can create the training sets with: window_size = 3 X, y, labels = process_data(orders.loc[[0], time_cols].values, window=window_size, time_cols=132) Use a batch size of 10 when training. When you are finished training a model use melt_results and plotly to make a graph of your predictions vs actuals df = melt_results(model, X, y) px.line(df, x='Date', y='KG', color='Source') You can use the same early_stopping and fit formula from 6.4.2 # Code cell for exercise 2 px . line ( df , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"15dad771-d18e-4fff-8d11-983538b1753d\")) { Plotly.newPlot( '15dad771-d18e-4fff-8d11-983538b1753d', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [11825.6875, 13559.1416015625, 12137.75390625, 13947.8779296875, 12989.833984375, 14231.40625, 13158.73828125, 14926.837890625, 12870.529296875, 14588.9521484375, 13427.3798828125, 14895.6435546875, 13730.8017578125, 15391.9677734375, 14224.8291015625, 15569.8564453125, 14314.1650390625, 15647.681640625, 15102.7060546875, 15677.5302734375, 14454.068359375, 16251.0849609375, 14895.8544921875, 16456.4453125, 15615.7265625, 16722.203125, 15433.7666015625, 17458.36328125, 15580.2919921875, 16309.3955078125, 14863.07421875, 16818.68359375, 14903.3330078125, 16513.052734375, 15724.498046875, 17878.58203125, 16155.3134765625, 18159.625, 16172.05078125, 17382.794921875, 16107.341796875, 17313.5390625, 16483.404296875, 17252.341796875, 16464.197265625, 17786.560546875, 16568.794921875, 17770.6953125, 16597.220703125, 18251.64453125, 17074.072265625, 18531.99609375, 17140.28515625, 18475.177734375, 17254.068359375, 18316.220703125, 17492.2578125, 18841.234375, 17812.646484375, 18745.146484375, 17495.0234375, 19517.84765625, 17125.201171875, 18900.5859375, 18097.98046875, 19135.197265625, 18450.5703125, 19601.57421875, 16863.998046875, 19617.59765625, 17484.56640625, 19359.19140625, 17953.666015625, 19218.322265625, 18016.2109375, 20058.076171875, 18202.548828125, 19723.52734375, 17070.009765625, 19954.546875, 18845.3046875, 20389.919921875, 17590.349609375, 19529.072265625, 18316.64453125, 21206.703125, 19056.572265625, 20005.666015625, 18007.3671875, 20317.125, 19044.81640625, 20686.09765625, 18918.6328125, 20623.279296875, 17837.052734375, 20518.95703125, 19015.080078125, 20039.18359375, 18911.501953125, 20081.958984375, 19192.466796875, 20493.732421875, 18547.373046875, 20818.744140625, 19251.302734375, 20841.6015625, 18938.953125, 21136.314453125, 18364.234375, 21031.203125, 18937.564453125, 21695.7734375, 19253.24609375, 21248.470703125, 19193.845703125, 21972.640625, 18921.76171875, 21569.33203125, 19967.01953125, 21113.927734375, 19555.97265625, 21477.498046875, 19123.1015625, 20727.06640625, 19791.390625, 22219.658203125, 19673.5703125, 21425.2734375, 19877.919921875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('15dad771-d18e-4fff-8d11-983538b1753d'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };","title":"\ud83c\udfcb\ufe0f Exercise 2: Vary model architecture and window size"},{"location":"S5_Time_Series_Analysis/#543-lstm-nn","text":"back to top Our data preparation for the LSTM NN includes time steps. The parameter input_dim tells our LSTM block how man time steps we have in the input data. This is a reframing (and a more appropriate reframing) of the same problem. The LSTM model is viewing the input feature w/ multiple time steps as a single feature at different times, rather than separate features. We could, for instance, have a second dimension that includes non-time related information, such as the customer name or truffle types (or other featurse that also vary through time, multiple feed rates or T/P, etc). window_size = 6 batch_size = 10 X , y , labels = process_data ( orders . loc [[ 0 ], time_cols ] . values , window = window_size , time_cols = 132 ) X = X . reshape ( - 1 , 1 , window_size ) y = y . reshape ( - 1 , 1 , 1 ) model = keras . Sequential ([ layers . LSTM ( 8 , activation = 'relu' , input_dim = window_size ), layers . Dense ( 8 ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X , y , batch_size = batch_size , epochs = 1000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) history_df = pd . DataFrame ( history . history ) history_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss 411 322288.40625 412 361232.25000 413 336341.78125 414 326199.90625 415 320201.21875 results = melt_results ( model , X , y . flatten ()) px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"60577dfc-eefb-4e74-806a-184d8dbf0f50\")) { Plotly.newPlot( '60577dfc-eefb-4e74-806a-184d8dbf0f50', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=predicted<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=predicted\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=predicted\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [13180.5244140625, 11941.052734375, 13897.1728515625, 12522.2275390625, 14309.3193359375, 13096.4521484375, 14802.939453125, 13020.638671875, 14922.9716796875, 13130.0478515625, 14894.341796875, 13569.859375, 15329.9990234375, 14020.2109375, 15622.0634765625, 14315.18359375, 15828.01953125, 14755.5458984375, 15874.12890625, 14772.2138671875, 16116.365234375, 14683.439453125, 16555.751953125, 15248.7373046875, 16811.826171875, 15624.388671875, 17435.9453125, 15604.052734375, 17160.962890625, 15268.173828125, 16771.892578125, 14809.287109375, 16804.845703125, 15244.541015625, 17437.248046875, 16029.177734375, 18312.34765625, 16288.9140625, 17983.26171875, 16235.626953125, 17556.31640625, 16354.9541015625, 17489.228515625, 16529.060546875, 17760.345703125, 16555.396484375, 17977.400390625, 16594.361328125, 18230.009765625, 16900.791015625, 18635.373046875, 17209.576171875, 18730.90234375, 17256.3203125, 18582.009765625, 17442.232421875, 18845.5390625, 17683.0703125, 19098.67578125, 17734.451171875, 19381.06640625, 17376.384765625, 19372.57421875, 17649.228515625, 19386.1328125, 18351.927734375, 19727.396484375, 17688.015625, 19858.5234375, 17253.630859375, 19738.755859375, 17737.013671875, 19527.080078125, 18018.39453125, 20046.962890625, 18187.865234375, 20121.90234375, 17573.572265625, 20144.68359375, 18101.638671875, 20523.671875, 18151.841796875, 20122.90625, 17993.490234375, 20761.791015625, 18826.2734375, 20869.994140625, 18526.685546875, 20408.02734375, 18580.732421875, 20915.5, 19076.302734375, 20948.8515625, 18448.703125, 20821.078125, 18561.212890625, 20518.23046875, 19026.8984375, 20366.970703125, 19106.486328125, 20562.0234375, 18892.654296875, 20958.59375, 18943.490234375, 21216.8046875, 19141.826171875, 21336.763671875, 18609.283203125, 21359.861328125, 18677.748046875, 21713.556640625, 19097.056640625, 21832.57421875, 19257.25390625, 21906.306640625, 19140.806640625, 22075.396484375, 19541.294921875, 21693.501953125, 19897.388671875, 21610.060546875, 19310.5859375, 21347.126953125, 19513.626953125, 21818.173828125, 19804.263671875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('60577dfc-eefb-4e74-806a-184d8dbf0f50'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };","title":"5.4.3 LSTM NN"},{"location":"S5_Time_Series_Analysis/#exercise-3-compare-lstm-with-ffnn-using-trainvaltest-sets-and-3-month-window","text":"back to top ### YOUR OPT WINDOW SIZE FROM EXERCISE 2 ### window_size = 3 batch_size = 10 patience = 50 # training on single order history data = orders . loc [[ 0 ], time_cols ] . values # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [ 0.6 , 0.8 ] X_train , y_train , X_val , y_val , X_test , y_test = train_test_process ( data , train_test_val_ratios ) ### YOUR EARLY STOPPING FORMULA ### ### YOUR MODEL FROM EX 6.3.3.2 ### # compile the model model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) # fit the model history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) print ( pd . DataFrame ( history . history ) . tail ()) train size: 76 val size: 23 test size: 24 loss val_loss 635 272437.62500 815874.4375 636 274113.68750 810266.5000 637 275288.75000 817768.3750 638 274128.71875 800769.0625 639 272784.25000 800416.8125 We'll then record the mse performance of the model to later compare with the LSTM results = [] y_pred = model . predict ( X_test ) mse = mean_squared_error ( y_test , y_pred ) results . append ([ 'Dense' , mse ]) results [['Dense', 415929.951056031]] We'll use the same parameters (window size, batch size, and early stopping to train the LSTM and compare the optimum FFNN architecture we previously used) X_train = X_train . reshape ( - 1 , 1 , window_size ) y_train = y_train . reshape ( - 1 , 1 , 1 ) X_val = X_val . reshape ( - 1 , 1 , window_size ) y_val = y_val . reshape ( - 1 , 1 , 1 ) X_test = X_test . reshape ( - 1 , 1 , window_size ) y_test = y_test . reshape ( - 1 , 1 , 1 ) model = keras . Sequential ([ layers . LSTM ( 8 , activation = 'relu' , input_dim = window_size ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , ) print ( pd . DataFrame ( history . history ) . tail ()) loss val_loss 1286 267132.96875 767315.0625 1287 266946.25000 770766.9375 1288 270205.12500 748175.3750 1289 267703.06250 749915.8125 1290 266674.15625 758844.8125 y_pred = model . predict ( X_test ) mse = mean_squared_error ( y_test . flatten (), y_pred . flatten ()) results . append ([ 'LSTM' , mse ]) Comparison of results: pd . DataFrame ( results , columns = [ 'Model' , 'Test MSE' ]) . set_index ( 'Model' ) . astype ( int ) [['Dense', 415929.951056031], ['LSTM', 393786.0529972827]] As a last visualization in this exercise we'll look at the trian/val/test predictions along the actual data = orders . loc [[ 0 ], time_cols ] . values idx_split1 = int ( data . shape [ 1 ] * train_test_val_ratios [ 0 ]) idx_split2 = int ( data . shape [ 1 ] * train_test_val_ratios [ 1 ]) y_p_train = model . predict ( X_train ) y_p_val = model . predict ( X_val ) y_p_test = model . predict ( X_test ) new = orders . loc [[ 0 ], time_cols ] . T . reset_index () new . columns = [ 'Date' , 'Real' ] new [ 'Train' ] = np . nan new . iloc [ window_size : idx_split1 , 2 ] = y_p_train new [ 'Val' ] = np . nan new . iloc [ idx_split1 + window_size : idx_split2 , 3 ] = y_p_val new [ 'Test' ] = np . nan new . iloc [ idx_split2 + window_size :, 4 ] = y_p_test new = new . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) px . line ( new , x = 'Date' , y = 'KG' , color = 'Source' ) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"36fed7ea-f858-4788-9a4e-ee6fe8319e47\")) { Plotly.newPlot( '36fed7ea-f858-4788-9a4e-ee6fe8319e47', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Real<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Real\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Real\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [12570.33516482565, 11569.168746227244, 13616.8122044598, 11884.3708810225, 13950.332334409884, 12781.156535682429, 14256.210023357236, 12887.711959877463, 15038.574005789536, 12626.48930557771, 14611.291109090684, 13194.81429999148, 14921.016215576235, 13477.391456909943, 15409.211079596587, 13999.215068692507, 15597.436975845374, 14098.12497823274, 15596.818092478728, 14941.69403166363, 15715.347212025836, 14181.212141927937, 16282.0980055455, 14650.929410064904, 16433.20900828632, 15400.579033515967, 16756.981262857273, 15128.148250492244, 17523.979943307248, 15413.0446914734, 16366.264377017458, 14568.470958551738, 16901.11154186154, 14659.021365286097, 16494.903960781197, 15398.721298130027, 17938.090871773184, 15850.35787113158, 18236.778754419982, 15956.750789202086, 17401.696472111977, 15890.103219350918, 17283.79073343649, 16302.509223010222, 17229.645014787257, 16223.309276278227, 17796.223621100053, 16344.001270241426, 17782.006164552513, 16326.588260101846, 18253.569321985724, 16818.123129181142, 18554.33980878632, 16900.704327264033, 18479.00603218699, 17042.963875823145, 18287.35559715585, 17244.887842050513, 18822.494484753846, 17603.725932131478, 18766.104076650663, 17170.126490680243, 19632.147600450644, 16856.921979192426, 18854.690380403008, 17880.884218985302, 19087.480847049384, 18196.112254637806, 19770.963054596545, 16488.739325030063, 19699.01989730995, 17194.707087425755, 19372.657901571318, 17715.24432224015, 19227.53144133251, 17691.136252909622, 20114.534506297117, 17926.25260490304, 19880.02532889845, 16690.02893115867, 19928.02694695529, 18553.766165315024, 20547.154033981024, 17301.11715078875, 19538.97650435099, 17902.44835514176, 21269.577926886348, 18842.69654955895, 20095.445399491346, 17670.300576591326, 20310.884287446843, 18754.84178182952, 20736.279238797022, 18617.387584546323, 20783.71123390676, 17470.755864944782, 20523.579839792714, 18796.93690580505, 20028.582492587037, 18677.535295190337, 20048.1074217522, 18929.24861718753, 20571.15590247796, 18207.20465623173, 20839.04289237627, 18966.53298378622, 20909.977545252816, 18589.807151786372, 21287.370122673103, 17987.976866769444, 21111.062684974826, 18538.311320658097, 21797.267132392342, 18935.35277235507, 21331.37841983855, 18783.75961074272, 22139.12337340894, 18553.79727063604, 21579.50628438568, 19726.43311123112, 21147.624131226225, 19232.360491469408, 21575.52105110441, 18856.1781102771, 20701.25067582265, 19406.448559709923, 22328.68716294986, 19384.824041986754, 21449.154889830093, 19554.40558950196, 21873.104938389297, 19572.860127015803], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Train<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Train\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Train\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, 11530.80859375, 13547.0107421875, 12017.6533203125, 13778.6494140625, 12893.5078125, 14261.6748046875, 12927.7666015625, 15076.15234375, 12940.1923828125, 14507.783203125, 13324.0244140625, 14885.705078125, 13578.6259765625, 15323.1728515625, 14155.8232421875, 15607.4013671875, 14294.25, 15480.1943359375, 15074.0634765625, 15926.88671875, 14279.1435546875, 16202.197265625, 14831.9873046875, 16323.2197265625, 15521.9248046875, 16855.90234375, 15203.2568359375, 17469.328125, 15860.4794921875, 16587.314453125, 14690.3681640625, 16879.89453125, 14974.7041015625, 16384.044921875, 15324.7646484375, 17848.638671875, 16040.9189453125, 18219.875, 16360.8349609375, 17463.84375, 16121.544921875, 17262.85546875, 16494.96875, 17322.7890625, 16306.2822265625, 17820.662109375, 16557.044921875, 17838.72265625, 16453.333984375, 18195.025390625, 16977.841796875, 18581.884765625, 17140.587890625, 18504.537109375, 17291.974609375, 18320.216796875, 17343.390625, 18812.208984375, 17820.373046875, 18920.37109375, 17241.39453125, 19696.1484375, 17286.443359375, 18702.0078125, 18034.021484375, 19108.53515625, 18266.767578125, 20135.396484375, 16819.486328125, 19538.892578125, 17530.896484375, 19302.970703125, 17974.005859375, 19292.216796875, 17759.21484375, 20087.013671875, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Val<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Val\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Val\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 19562.25, 18666.6640625, 20812.396484375, 17802.416015625, 19452.72265625, 17828.015625, 21070.75, 19337.732421875, 20385.990234375, 17909.234375, 20121.26171875, 18914.283203125, 20805.439453125, 18875.658203125, 21021.876953125, 17844.279296875, 20268.6875, 19130.21875, 20132.666015625, 18896.615234375, 20078.298828125, 19047.263671875, 20762.689453125, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Source=Test<br>Date=%{x}<br>KG=%{y}\", \"legendgroup\": \"Source=Test\", \"line\": {\"color\": \"#ab63fa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Source=Test\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [\"1/2010\", \"2/2010\", \"3/2010\", \"4/2010\", \"5/2010\", \"6/2010\", \"7/2010\", \"8/2010\", \"9/2010\", \"10/2010\", \"11/2010\", \"12/2010\", \"1/2011\", \"2/2011\", \"3/2011\", \"4/2011\", \"5/2011\", \"6/2011\", \"7/2011\", \"8/2011\", \"9/2011\", \"10/2011\", \"11/2011\", \"12/2011\", \"1/2012\", \"2/2012\", \"3/2012\", \"4/2012\", \"5/2012\", \"6/2012\", \"7/2012\", \"8/2012\", \"9/2012\", \"10/2012\", \"11/2012\", \"12/2012\", \"1/2013\", \"2/2013\", \"3/2013\", \"4/2013\", \"5/2013\", \"6/2013\", \"7/2013\", \"8/2013\", \"9/2013\", \"10/2013\", \"11/2013\", \"12/2013\", \"1/2014\", \"2/2014\", \"3/2014\", \"4/2014\", \"5/2014\", \"6/2014\", \"7/2014\", \"8/2014\", \"9/2014\", \"10/2014\", \"11/2014\", \"12/2014\", \"1/2015\", \"2/2015\", \"3/2015\", \"4/2015\", \"5/2015\", \"6/2015\", \"7/2015\", \"8/2015\", \"9/2015\", \"10/2015\", \"11/2015\", \"12/2015\", \"1/2016\", \"2/2016\", \"3/2016\", \"4/2016\", \"5/2016\", \"6/2016\", \"7/2016\", \"8/2016\", \"9/2016\", \"10/2016\", \"11/2016\", \"12/2016\", \"1/2017\", \"2/2017\", \"3/2017\", \"4/2017\", \"5/2017\", \"6/2017\", \"7/2017\", \"8/2017\", \"9/2017\", \"10/2017\", \"11/2017\", \"12/2017\", \"1/2018\", \"2/2018\", \"3/2018\", \"4/2018\", \"5/2018\", \"6/2018\", \"7/2018\", \"8/2018\", \"9/2018\", \"10/2018\", \"11/2018\", \"12/2018\", \"1/2019\", \"2/2019\", \"3/2019\", \"4/2019\", \"5/2019\", \"6/2019\", \"7/2019\", \"8/2019\", \"9/2019\", \"10/2019\", \"11/2019\", \"12/2019\", \"1/2020\", \"2/2020\", \"3/2020\", \"4/2020\", \"5/2020\", \"6/2020\", \"7/2020\", \"8/2020\", \"9/2020\", \"10/2020\", \"11/2020\", \"12/2020\"], \"xaxis\": \"x\", \"y\": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 21025.208984375, 18800.91015625, 21401.697265625, 18349.783203125, 20994.505859375, 18708.802734375, 21705.333984375, 19330.486328125, 21383.529296875, 18933.072265625, 22155.6640625, 19004.689453125, 21361.583984375, 20062.9765625, 21315.884765625, 19415.896484375, 21672.541015625, 19315.009765625, 20655.81640625, 19342.04296875, 22334.814453125, 19861.693359375, 21464.390625, 19739.919921875], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Date\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"KG\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('36fed7ea-f858-4788-9a4e-ee6fe8319e47'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };","title":"\ud83c\udfcb\ufe0f Exercise 3: Compare LSTM with FFNN using Train/Val/Test sets and 3 Month Window"},{"location":"S5_Time_Series_Analysis/#55-model-extensibility","text":"back to top from ipywidgets import interact","title":"5.5 Model Extensibility"},{"location":"S5_Time_Series_Analysis/#exercise-4-apply-model-to-other-orders","text":"Take the last LSTM model and apply it to other orders in the dataset. What do you notice? back to top def apply_model ( dataset = orders . index , window_size = 3 ): window_size = window_size data = pd . DataFrame ( orders . loc [ dataset , time_cols ]) data = data . reset_index () data . columns = [ 'Date' , 'KG' ] X , y , labels = process_data ( orders . loc [[ dataset ], time_cols ] . values , window = window_size , time_cols = 132 ) y_pred = model . predict ( X . reshape ( - 1 , 1 , window_size )) . flatten () results = pd . DataFrame ( y_pred , y ) results = results . reset_index () results . index = data [ 'Date' ][ window_size :] results = results . reset_index () results . columns = [ 'Date' , 'real' , 'predicted' ] results = results . melt ( id_vars = 'Date' , var_name = 'Source' , value_name = 'KG' ) fig = px . line ( results , x = 'Date' , y = 'KG' , color = 'Source' ) return fig interact ( apply_model ) interactive(children=(Dropdown(description='dataset', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1\u2026 <function __main__.apply_model>","title":"\ud83c\udfcb\ufe0f Exercise 4: Apply Model to Other Orders"},{"location":"S5_Time_Series_Analysis/#exercise-discussion-51-how-would-you-create-a-general-forecast-model","text":"back to top After exploring how your model does on other order histories, what do you think is a good strategy for developing company wide order forecasts? Some possible questions: should you create a single model for the whole company? could you embed meta data about the order in this all-inclusive model? should you make models specific to certain customers, products, etc. what kind of analysis could you do before hand to determine how your models should be grouped? melted = orders . melt ( id_vars = [ 'base_cake' , 'truffle_type' , 'primary_flavor' , 'secondary_flavor' , 'color_group' , 'customer' ], var_name = 'month' , value_name = 'kg' ) def my_eda ( color = cat_cols ): fig = px . line ( melted , x = 'month' , y = 'kg' , color = color ) return fig interact ( my_eda ) interactive(children=(Dropdown(description='color', options=('base_cake', 'truffle_type', 'primary_flavor', 's\u2026 <function __main__.my_eda>","title":"\ud83c\udfcb\ufe0f Exercise-Discussion 5.1: How Would You Create a General Forecast Model?"},{"location":"S5_Time_Series_Analysis/#exercise-52-eda","text":"back to top In our quest to create a model that works well for all orders to truffltopia. I tell you that there are some orders with patterned behavior, according to their meta data. Your first task, is to find out which categorical variable best separates the data. You can use any statistical or visual method you like # recall the categorical variables: ['base_cake', 'truffle_type', 'primary_flavor', 'secondary_flavor', 'color_group', 'customer'] From C1 S6, it may be useful to think of this diagram:","title":"\ud83c\udfcb\ufe0f Exercise 5.2: EDA"},{"location":"S5_Time_Series_Analysis/#exercise-53-decide-on-model","text":"back to top Will you model the whole dataset together? Will you create a number of submodels? Choose based on the groupings you determined statistically significant in the data. As a base comparison I have provided a formula that trains a model on the entire order history: data = orders data = data[time_cols].values batch_size = 256 window_size = 12 print(\"batch size: {}\".format(batch_size)) print(\"window size: {}\".format(window_size), end='\\n\\n') # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [0.8, 0.9] X_train, y_train, X_val, y_val, X_test, y_test = train_test_process(data, train_test_val_ratios, window_size) early_stopping = keras.callbacks.EarlyStopping( patience=50, min_delta=0.001, restore_best_weights=True, monitor='loss' ) model = keras.Sequential([ layers.Dense(8, input_shape=[window_size]), layers.Dense(16), layers.Dense(32), layers.Dense(16), layers.Dense(1) ]) model.compile(loss='mean_squared_error', optimizer='adam') history = model.fit( X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=10000, callbacks=[early_stopping], verbose=0, # hide the output because we have so many epochs ) print(pd.DataFrame(history.history).tail()) data = orders data = data [ time_cols ] . values batch_size = 256 window_size = 12 print ( \"batch size: {} \" . format ( batch_size )) print ( \"window size: {} \" . format ( window_size ), end = ' \\n\\n ' ) # describes the split train 0-.6/val .6-.8/test .8-1 train_test_val_ratios = [ 0.8 , 0.9 ] X_train , y_train , X_val , y_val , X_test , y_test = train_test_process ( data , train_test_val_ratios , window_size ) early_stopping = keras . callbacks . EarlyStopping ( patience = 50 , min_delta = 0.001 , restore_best_weights = True , monitor = 'loss' ) model = keras . Sequential ([ layers . Dense ( 8 , input_shape = [ window_size ]), layers . Dense ( 16 ), layers . Dense ( 32 ), layers . Dense ( 16 ), layers . Dense ( 1 ) ]) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = batch_size , epochs = 10000 , callbacks = [ early_stopping ], verbose = 0 , # hide the output because we have so many epochs ) print ( pd . DataFrame ( history . history ) . tail ()) batch size: 256 window size: 12 train size: 9300 val size: 100 test size: 200 loss val_loss 326 279111.15625 953265.0625 327 322529.15625 580780.2500 328 285901.56250 476007.4375 329 302237.68750 496192.8125 330 281779.40625 480916.6250 And a history of the loss with the following settings: batch size: 256 window size: 12 train size: 9300 val size: 100 test size: 200 loss val_loss 326 279111.15625 953265.0625 327 322529.15625 580780.2500 328 285901.56250 476007.4375 329 302237.68750 496192.8125 330 281779.40625 480916.6250 interact ( apply_model , window_size = window_size ) interactive(children=(Dropdown(description='dataset', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1\u2026 <function __main__.apply_model>","title":"\ud83c\udfcb\ufe0f Exercise 5.3: Decide on Model"},{"location":"about/","text":"General Applications of Neural Networks \u00b6 This course teaches you to deploy the workhorse of modern machine learning frameworks: neural networks. We start with the simplest building block, the perceptron, and demonstrate how these are organized into feed forward neural networks. We then explore predictive capabilities in computer vision, natural language processing, and time series analysis. Different architectures are employed for specific application areas. The breakdown for this course is as follows: Data Topics Neural networks: the perceptron, feed forward neural networks; computer vision: convolutional neural networks, importing and manipulating images, generating images; time series analysis: long short term memory networks, autocorrelation Sessions S1: Multilayer Perceptron S2: Feed Forward Neural Networks S3: Computer Vision I S4: Computer Vision II S5: Time Series Analysis S6: Natural Language Processing I S7: Natural Language Processing II Exercises E1: Neural Network Linearity E2: Wine Quality Prediction with Neural Networks E3: Customer Forecasting with Neural Networks E4: TBD E5: TBD Project P1: Object Detection I P2: Object Detection II P3: Defect Detection","title":"About"},{"location":"about/#general-applications-of-neural-networks","text":"This course teaches you to deploy the workhorse of modern machine learning frameworks: neural networks. We start with the simplest building block, the perceptron, and demonstrate how these are organized into feed forward neural networks. We then explore predictive capabilities in computer vision, natural language processing, and time series analysis. Different architectures are employed for specific application areas. The breakdown for this course is as follows: Data Topics Neural networks: the perceptron, feed forward neural networks; computer vision: convolutional neural networks, importing and manipulating images, generating images; time series analysis: long short term memory networks, autocorrelation Sessions S1: Multilayer Perceptron S2: Feed Forward Neural Networks S3: Computer Vision I S4: Computer Vision II S5: Time Series Analysis S6: Natural Language Processing I S7: Natural Language Processing II Exercises E1: Neural Network Linearity E2: Wine Quality Prediction with Neural Networks E3: Customer Forecasting with Neural Networks E4: TBD E5: TBD Project P1: Object Detection I P2: Object Detection II P3: Defect Detection","title":"General Applications of Neural Networks"},{"location":"exercises/E1_Neural_Network_Linearity/","text":"Technology Fundamentals Course 4, Lab 1: Practice with FFNNs \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu , harshav@uw.edu In this lab we will compare the FFNN to the classification algorithms we created in Course 2. Data and Helper Functions \u00b6 import plotly.express as px from sklearn.datasets import make_blobs , make_moons import pandas as pd import numpy as np import matplotlib.pyplot as plt X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 3 ) X , y = make_moons ( random_state = 42 , noise = .05 , n_samples = 1000 ) px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y . astype ( str )) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"c40b16c5-9ac5-473f-b958-547b4bf59671\")) { Plotly.newPlot( 'c40b16c5-9ac5-473f-b958-547b4bf59671', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=1<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=1\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=1\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 0.05236582472018032, 1.8560150272613007, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, -0.14441356025963623, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 0.02372728551128933, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 0.06076367955538808, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 0.07116809834154524, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.5222017799968333, 0.02569784702500033, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.5146634657066751, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.5233938680708092, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, 0.5003388639021644, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, 0.5088865294391419, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=0<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=0\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=0\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, 1.049364864300265, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.9747088538643435, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.9217096483090739, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.9213927020182104, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, 1.0262181373626456, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, 1.027708672633818, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 1.0467834766218154, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 0.9502141462766489, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 0.9356591486239149, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.9324771987893767, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.9445673085715209, 0.880199864297024, 0.27234218972315183, 0.987043634180512, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9921053817619112, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.04375671133118682, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.029465956171557988, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.01785388719359586, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.028879374506168877, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, -0.03674552072388997, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.0677453838688456, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.06362642722102937, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.06709165073838032, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.007092961807579082, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, -0.05498257762065896, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, -0.07676642286508525, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.06072464380436819, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('c40b16c5-9ac5-473f-b958-547b4bf59671'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) L1 Q1: \u00b6 Build and train a linear classification model using keras tf. Verify that the model is linear by either showing the weights or plotting the decision boundary (hint: you can use plot_boundaries above). # Code Cell for L1 Q1 from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 500 , verbose = 0 ) model . summary () results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) Model: \"sequential_14\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_51 (Dense) (None, 2) 6 _________________________________________________________________ dense_52 (Dense) (None, 2) 6 _________________________________________________________________ dense_53 (Dense) (None, 2) 6 _________________________________________________________________ dense_54 (Dense) (None, 1) 3 ================================================================= Total params: 21 Trainable params: 21 Non-trainable params: 0 _________________________________________________________________ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 495 0.225831 0.888 496 0.225785 0.887 497 0.226102 0.888 498 0.225775 0.886 499 0.225990 0.888 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"b1aa98d7-5474-46b9-b93d-6bfed5d06810\")) { Plotly.newPlot( 'b1aa98d7-5474-46b9-b93d-6bfed5d06810', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, -0.841925571991321, -0.026378361734335026, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.003467472728882294, 0.4831018198648976, -0.6441953693353871, -0.009956890749796943, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.1393412145128936, -0.16009343904193618, -0.06916239179473493, 0.19334863345288741, 0.9287172152733659, 0.03195991474457149, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, -0.4162270875140181, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, 0.028529406322205582, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.09060345442824758, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.039627331512575664, 0.5896368379627389, 0.04048431440928878, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.053504864135638805, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, -0.06746998748693264, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8454796924228637, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, -0.004780511062108857, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 0.037662937381448866, 0.0866983907273326, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.01345055834815248, -0.5775693779815646, 0.045238045924702014, -0.5894412664957717, -1.003483362657891, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.3371084971589208, 0.050525847113584425, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.025859919370369393, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.0037866371787284034, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, 0.05236582472018032, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.49581083660996056, 0.041614857041363154, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, 0.016099275663376437, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.013483712865137141, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, -0.4242503002479633, -0.0333870459512904, 0.03594467094355608, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.02378383532079511, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, -0.14441356025963623, 0.688271037238145, 0.360575625318509, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, -0.935842328272884, 0.028498130131812523, -0.0005931332760947608, -0.6267995490316154, 0.05938183068183091, -0.11006562690830578, -0.03404569998465, -0.025610378067933835, -1.0064524676654591, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.05507085892061413, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.02849832116612446, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, -0.8186016008031817, -0.020658309769524194, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.059660086073984084, -0.5003137640460915, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.04112200360905427, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, -0.6572881746353788, -0.10295591122486067, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, -0.823259632802671, -0.021306689117003466, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 0.026933766169225222, 0.5280164903121144, 0.49694546350805135, -0.007849683379450141, -0.03450175489395475, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.02372728551128933, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, 0.1088360353798975, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, 0.08004379141252124, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, -0.11317939378933384, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.011583851976794662, 0.07361863450396877, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.05483770690547791, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.006139402232989595, 0.7386135390848821, -0.7628793315212464, -0.01796529840604455, 0.3699024379670562, 0.6408239551321332, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.05643656243980857, 0.9740132594201618, 0.5341113402855393, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.880199864297024, 0.27234218972315183, 0.06076367955538808, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.07116809834154524, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, -0.03253814552150909, -0.9750360307004108, -0.008952862777465757, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.4061860760728654, 0.5305869534842842, 0.44766267100521034, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.1763581693865052, 0.9515075766286676, 0.7255288065374311, 0.351022501469148, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 1.0176596674044673, 0.9020976159451496, 0.17661598882501486, 0.9778536919017272, 0.3879432914489443, 0.2340754360790136, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.8943725943657549, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.2696835681307252, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 0.44046007450720215, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.17866537307866523, 0.8688608803652028, 0.3512595859263131, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.28467928313356083, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.23050518847115897, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.47206214221453446, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.34577178535893566, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.2699238672297299, 0.1791693072694373, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.20426594971541467, 0.7664914852608554, 0.3093597670558822, 0.8826265104252563, 0.1789454932929465, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.9537140769604843, 0.4556311320681723, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.37632306770073937, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.4099959536645816, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.5222017799968333, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.7981273131858493, 0.20972429653054808, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.26473023395633355, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.48229393261157355, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, 0.9363076866507517, 0.22178526030182227, 0.2997910331159463, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.16855957491065562, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.5146634657066751, 0.6989704640420543, 0.915544418207169, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.2382146970526554, 0.24820871257833463, 0.3053079331926927, 0.9517795979001498, 0.4127985592774463, 0.3771905515137585, 0.9397949553902923, 0.3953381776673755, 0.32469793364884003, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.23375974123782337, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2385485980623932, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.559661595179487, 0.2869686430492015, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.2117040879093407, 0.8629537052218356, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.41493145306385265, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.7303787147472682, 0.9909734747559312, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.5798832810346423, 0.27666404974095093, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.40034297008824193, 0.7247797916248883, 0.8183366714816989, 0.4311452112892778, 0.39598707601074584, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.5233938680708092, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.17353941166679487, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.37696350991865785, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.42171188266107507, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.2681596821679075, 0.21994047525303553, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.25059410971915247, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, 0.18454012286879, 0.6777489151718835, 0.5323517097321465, 0.3138372844046242, 0.9333919037100045, 0.7949502064180195, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.41458517754244617, 0.4418917406497507, 0.8655180578975504, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, 0.5024329383266082, 0.9702952661304376, 0.5003388639021644, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.5088865294391419, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.43728846428088664, 0.1379387515057112, 1.008121644708765, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 0.9443139169305158, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.9386747622287412, 0.9422608694499245, 0.2592820495979804, 0.9641405035778899, 1.4236794415177, 1.012193132759459, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.9371456802671978, 0.06942815391313314, 0.6363666829222565, 0.9527236026865668, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 1.9774500535344222, 1.7917769304081708, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, 0.8909551327802967, 0.9535901481763159, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 1.0086540617608457, 0.6989590699758024, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 0.9438415932280083, 1.8067823256210032, 1.049364864300265, 1.9170066261220016, 2.005231249144342, 0.12948978784396156, 0.9600191987137255, 0.9294360806160966, 2.122675629230518, 0.9747088538643435, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 1.9069142312438205, 1.9401659307880803, 0.9217096483090739, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.8933491957200252, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 1.8560150272613007, 0.9213927020182104, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 1.0262181373626456, 0.790773566950995, 1.7699943333823245, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 0.9309728496845366, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 1.027708672633818, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, 2.0254007743767777, 1.1053097702500083, 0.9386873812049207, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 1.0489967672524148, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.8892406769100474, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, 0.9980180411993752, 0.7730503979626255, 0.05012452522302984, 0.9412848237200061, 0.3151842349303073, 1.0344375696524877, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, 0.8589247883431155, 0.9875820054800791, 0.974735950859411, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, 1.0306645007550932, 1.097718904549744, 1.998930446459027, 0.9500022266038769, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 0.9263216268973042, 1.6185029791547985, 1.86328352562573, 1.0157932159614604, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 0.8960483177245648, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, 0.18682261802422412, 0.9785598832082368, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 1.0467834766218154, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 2.069498120967828, 1.0746796458168144, 1.8384436606065246, 1.5854004542490547, 0.9502141462766489, 1.0089088191871194, 2.008509873407435, 1.950669255439995, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 1.0252337252138446, 2.0061978338583937, 1.9192704169033825, 1.0490609947203429, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 0.9401730117100286, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 0.9356591486239149, 1.3572078064349313, 1.0224961959128591, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.9493236420152802, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 0.9324771987893767, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, 0.9769910506371062, 0.9522384439744107, 1.1663479471564113, 1.534681762803947, 1.037439503990736, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 1.0504913158044216, 0.9416598253275545, 0.9653269741882563, 1.2226875287424543, 0.9438735111314883, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 0.9445673085715209, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 0.987043634180512, 1.2347826949424812, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 1.012479575428554, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, 0.973726870455468, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 0.9565549092838592, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.9921053817619112, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [-0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.30597998244822927, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, 0.17013082886121492, 0.1795255739860949, -0.16386479200305562, 0.07038051427651006, -0.3953522895945679, 0.20049308034949212, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.19620781805876192, 0.05677354690499763, -0.49075947312125456, 0.33301568358301975, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.234079692103942, -0.013644617927113946, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.09637557897209917, 0.23220564206501929, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.18011861732039008, -0.405582649094571, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, 0.2670293326736063, -0.1570953567668757, 0.04375671133118682, -0.04517604084317692, 0.5160335878313548, 0.1542936172964292, 0.2164966819737342, 0.1674674984914969, 0.2704560677068569, 0.029465956171557988, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, -0.06137931074981458, -0.0014130783217369686, 0.01785388719359586, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.20351931931804346, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.02569784702500033, 0.028879374506168877, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.03674552072388997, -0.3815626054906763, -0.30131756948222327, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, 0.13088286445288597, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, 0.0677453838688456, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.3792433919842689, -0.467268047386986, 0.23472335199424224, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.13469184867095194, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.31359150938324615, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.19918801224063537, -0.4720015970691104, 0.14978588115915387, 0.33927881802500365, -0.21958607670366795, 0.1051489326854959, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.10924503343914722, 0.19915064453773526, 0.2901899106002602, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.09913384146619357, -0.5445274689896823, 0.41683314533623594, 0.1223014150437775, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, 0.293046936035356, -0.24519104116011547, -0.008690954824159736, 0.3571915030054305, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, 0.12562704192371885, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, -0.2994973461644936, 0.2919937556919383, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, 0.06362642722102937, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.4362608360142391, 0.2848873018568047, -0.04600110928594187, -0.4060725545016431, 0.06709165073838032, 0.26874213756713305, 0.4611731921189639, 0.1280078066973535, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.23952636533842916, 0.19334620117519105, 0.2195221158776203, 0.20629588358405002, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.3389677681314225, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, 0.007092961807579082, -0.4104902712188111, 0.13347628461694513, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.22980460025990193, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.05498257762065896, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.2253268053322805, 0.14879379507579316, -0.46060854249014255, -0.3099236494641088, 0.2061980698699697, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, 0.1342348865555803, 0.2761789058580983, -0.5001340667227457, -0.39384046284985436, 0.344303422286615, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, -0.07676642286508525, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, 0.1717890736536283, -0.424752896655102, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, 0.09123679877099287, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.35626734597179077, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.14832545170674544, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, 0.06072464380436819, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('b1aa98d7-5474-46b9-b93d-6bfed5d06810'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model ) L1 Q2: \u00b6 Now add an activation function to your previous model. Does the model become non-linear? # Code Cell for L1 Q2 model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 500 , verbose = 0 ) results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 495 0.093515 0.981 496 0.092895 0.981 497 0.091976 0.982 498 0.091197 0.982 499 0.090521 0.982 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"ee1ca0d8-cccf-4afd-8a53-a241c6fe1016\")) { Plotly.newPlot( 'ee1ca0d8-cccf-4afd-8a53-a241c6fe1016', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 1.049364864300265, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.9747088538643435, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.9217096483090739, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 1.8560150272613007, 0.9213927020182104, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 1.0262181373626456, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 1.027708672633818, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 1.0467834766218154, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, 0.9502141462766489, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 0.9356591486239149, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 0.9324771987893767, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 0.9445673085715209, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.9921053817619112, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.04375671133118682, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.029465956171557988, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, 0.01785388719359586, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.02569784702500033, 0.028879374506168877, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.03674552072388997, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, 0.0677453838688456, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, 0.06362642722102937, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.06709165073838032, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, 0.007092961807579082, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.05498257762065896, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, -0.07676642286508525, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, 0.06072464380436819, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, 0.05236582472018032, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, -0.14441356025963623, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.02372728551128933, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.880199864297024, 0.27234218972315183, 0.987043634180512, 0.06076367955538808, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.07116809834154524, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.5222017799968333, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.5146634657066751, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.5233938680708092, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.5003388639021644, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.5088865294391419, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('ee1ca0d8-cccf-4afd-8a53-a241c6fe1016'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model ) L1 Q3: \u00b6 Continue to add complexity to your Q3 model until you get an accuracy above 99% # Code Cell for L1 Q3 model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 100 , verbose = 0 ) results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 95 0.021280 1.0 96 0.019798 1.0 97 0.018461 1.0 98 0.017353 1.0 99 0.016389 1.0 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"fa337ce2-7fd1-4407-92ed-9078ecb1cdc2\")) { Plotly.newPlot( 'fa337ce2-7fd1-4407-92ed-9078ecb1cdc2', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 0.05236582472018032, 1.8560150272613007, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, -0.14441356025963623, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 0.02372728551128933, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 0.06076367955538808, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 0.07116809834154524, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.5222017799968333, 0.02569784702500033, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.5146634657066751, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.5233938680708092, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, 0.5003388639021644, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, 0.5088865294391419, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, 1.049364864300265, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.9747088538643435, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.9217096483090739, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.9213927020182104, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, 1.0262181373626456, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, 1.027708672633818, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 1.0467834766218154, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 0.9502141462766489, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 0.9356591486239149, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.9324771987893767, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.9445673085715209, 0.880199864297024, 0.27234218972315183, 0.987043634180512, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9921053817619112, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.04375671133118682, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.029465956171557988, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.01785388719359586, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.028879374506168877, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, -0.03674552072388997, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.0677453838688456, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.06362642722102937, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.06709165073838032, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.007092961807579082, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, -0.05498257762065896, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, -0.07676642286508525, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.06072464380436819, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('fa337ce2-7fd1-4407-92ed-9078ecb1cdc2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model )","title":"Neural Network Linearity"},{"location":"exercises/E1_Neural_Network_Linearity/#technology-fundamentals-course-4-lab-1-practice-with-ffnns","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu , harshav@uw.edu In this lab we will compare the FFNN to the classification algorithms we created in Course 2.","title":"Technology Fundamentals Course 4, Lab 1: Practice with FFNNs"},{"location":"exercises/E1_Neural_Network_Linearity/#data-and-helper-functions","text":"import plotly.express as px from sklearn.datasets import make_blobs , make_moons import pandas as pd import numpy as np import matplotlib.pyplot as plt X , y = make_blobs ( random_state = 42 , centers = 2 , cluster_std = 3 ) X , y = make_moons ( random_state = 42 , noise = .05 , n_samples = 1000 ) px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y . astype ( str )) if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"c40b16c5-9ac5-473f-b958-547b4bf59671\")) { Plotly.newPlot( 'c40b16c5-9ac5-473f-b958-547b4bf59671', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=1<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=1\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=1\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 0.05236582472018032, 1.8560150272613007, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, -0.14441356025963623, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 0.02372728551128933, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 0.06076367955538808, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 0.07116809834154524, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.5222017799968333, 0.02569784702500033, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.5146634657066751, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.5233938680708092, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, 0.5003388639021644, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, 0.5088865294391419, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=0<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=0\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=0\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, 1.049364864300265, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.9747088538643435, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.9217096483090739, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.9213927020182104, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, 1.0262181373626456, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, 1.027708672633818, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 1.0467834766218154, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 0.9502141462766489, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 0.9356591486239149, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.9324771987893767, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.9445673085715209, 0.880199864297024, 0.27234218972315183, 0.987043634180512, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9921053817619112, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.04375671133118682, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.029465956171557988, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.01785388719359586, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.028879374506168877, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, -0.03674552072388997, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.0677453838688456, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.06362642722102937, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.06709165073838032, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.007092961807579082, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, -0.05498257762065896, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, -0.07676642286508525, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.06072464380436819, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('c40b16c5-9ac5-473f-b958-547b4bf59671'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; def plot_boundaries ( X , clf , ax = False ): plot_step = 0.02 x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , plot_step ), np . arange ( y_min , y_max , plot_step )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) if ax : cs = ax . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 ) return ax else : cs = plt . contourf ( xx , yy , Z , cmap = 'viridis' , alpha = 0.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = 'viridis' , edgecolor = 'grey' , alpha = 0.9 )","title":"Data and Helper Functions"},{"location":"exercises/E1_Neural_Network_Linearity/#l1-q1","text":"Build and train a linear classification model using keras tf. Verify that the model is linear by either showing the weights or plotting the decision boundary (hint: you can use plot_boundaries above). # Code Cell for L1 Q1 from tensorflow import keras from tensorflow.keras import layers model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 500 , verbose = 0 ) model . summary () results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) Model: \"sequential_14\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_51 (Dense) (None, 2) 6 _________________________________________________________________ dense_52 (Dense) (None, 2) 6 _________________________________________________________________ dense_53 (Dense) (None, 2) 6 _________________________________________________________________ dense_54 (Dense) (None, 1) 3 ================================================================= Total params: 21 Trainable params: 21 Non-trainable params: 0 _________________________________________________________________ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 495 0.225831 0.888 496 0.225785 0.887 497 0.226102 0.888 498 0.225775 0.886 499 0.225990 0.888 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"b1aa98d7-5474-46b9-b93d-6bfed5d06810\")) { Plotly.newPlot( 'b1aa98d7-5474-46b9-b93d-6bfed5d06810', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, -0.841925571991321, -0.026378361734335026, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.003467472728882294, 0.4831018198648976, -0.6441953693353871, -0.009956890749796943, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.1393412145128936, -0.16009343904193618, -0.06916239179473493, 0.19334863345288741, 0.9287172152733659, 0.03195991474457149, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, -0.4162270875140181, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, 0.028529406322205582, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.09060345442824758, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.039627331512575664, 0.5896368379627389, 0.04048431440928878, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.053504864135638805, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, -0.06746998748693264, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8454796924228637, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, -0.004780511062108857, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 0.037662937381448866, 0.0866983907273326, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.01345055834815248, -0.5775693779815646, 0.045238045924702014, -0.5894412664957717, -1.003483362657891, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.3371084971589208, 0.050525847113584425, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.025859919370369393, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.0037866371787284034, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, 0.05236582472018032, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.49581083660996056, 0.041614857041363154, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, 0.016099275663376437, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.013483712865137141, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, -0.4242503002479633, -0.0333870459512904, 0.03594467094355608, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.02378383532079511, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, -0.14441356025963623, 0.688271037238145, 0.360575625318509, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, -0.935842328272884, 0.028498130131812523, -0.0005931332760947608, -0.6267995490316154, 0.05938183068183091, -0.11006562690830578, -0.03404569998465, -0.025610378067933835, -1.0064524676654591, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.05507085892061413, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.02849832116612446, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, -0.8186016008031817, -0.020658309769524194, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.059660086073984084, -0.5003137640460915, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.04112200360905427, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, -0.6572881746353788, -0.10295591122486067, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, -0.823259632802671, -0.021306689117003466, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 0.026933766169225222, 0.5280164903121144, 0.49694546350805135, -0.007849683379450141, -0.03450175489395475, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.02372728551128933, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, 0.1088360353798975, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, 0.08004379141252124, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, -0.11317939378933384, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.011583851976794662, 0.07361863450396877, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.05483770690547791, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.006139402232989595, 0.7386135390848821, -0.7628793315212464, -0.01796529840604455, 0.3699024379670562, 0.6408239551321332, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.05643656243980857, 0.9740132594201618, 0.5341113402855393, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.880199864297024, 0.27234218972315183, 0.06076367955538808, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.07116809834154524, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, -0.03253814552150909, -0.9750360307004108, -0.008952862777465757, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.4061860760728654, 0.5305869534842842, 0.44766267100521034, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.1763581693865052, 0.9515075766286676, 0.7255288065374311, 0.351022501469148, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 1.0176596674044673, 0.9020976159451496, 0.17661598882501486, 0.9778536919017272, 0.3879432914489443, 0.2340754360790136, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.8943725943657549, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.2696835681307252, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 0.44046007450720215, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.17866537307866523, 0.8688608803652028, 0.3512595859263131, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.28467928313356083, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.23050518847115897, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.47206214221453446, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.34577178535893566, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.2699238672297299, 0.1791693072694373, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.20426594971541467, 0.7664914852608554, 0.3093597670558822, 0.8826265104252563, 0.1789454932929465, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.9537140769604843, 0.4556311320681723, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.37632306770073937, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.4099959536645816, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.5222017799968333, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.7981273131858493, 0.20972429653054808, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.26473023395633355, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.48229393261157355, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, 0.9363076866507517, 0.22178526030182227, 0.2997910331159463, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.16855957491065562, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.5146634657066751, 0.6989704640420543, 0.915544418207169, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.2382146970526554, 0.24820871257833463, 0.3053079331926927, 0.9517795979001498, 0.4127985592774463, 0.3771905515137585, 0.9397949553902923, 0.3953381776673755, 0.32469793364884003, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.23375974123782337, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2385485980623932, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.559661595179487, 0.2869686430492015, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.2117040879093407, 0.8629537052218356, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.41493145306385265, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.7303787147472682, 0.9909734747559312, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.5798832810346423, 0.27666404974095093, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.40034297008824193, 0.7247797916248883, 0.8183366714816989, 0.4311452112892778, 0.39598707601074584, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.5233938680708092, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.17353941166679487, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.37696350991865785, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.42171188266107507, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.2681596821679075, 0.21994047525303553, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.25059410971915247, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, 0.18454012286879, 0.6777489151718835, 0.5323517097321465, 0.3138372844046242, 0.9333919037100045, 0.7949502064180195, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.41458517754244617, 0.4418917406497507, 0.8655180578975504, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, 0.5024329383266082, 0.9702952661304376, 0.5003388639021644, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.5088865294391419, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.43728846428088664, 0.1379387515057112, 1.008121644708765, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 0.9443139169305158, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.9386747622287412, 0.9422608694499245, 0.2592820495979804, 0.9641405035778899, 1.4236794415177, 1.012193132759459, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.9371456802671978, 0.06942815391313314, 0.6363666829222565, 0.9527236026865668, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 1.9774500535344222, 1.7917769304081708, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, 0.8909551327802967, 0.9535901481763159, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 1.0086540617608457, 0.6989590699758024, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 0.9438415932280083, 1.8067823256210032, 1.049364864300265, 1.9170066261220016, 2.005231249144342, 0.12948978784396156, 0.9600191987137255, 0.9294360806160966, 2.122675629230518, 0.9747088538643435, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 1.9069142312438205, 1.9401659307880803, 0.9217096483090739, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.8933491957200252, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 1.8560150272613007, 0.9213927020182104, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 1.0262181373626456, 0.790773566950995, 1.7699943333823245, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 0.9309728496845366, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 1.027708672633818, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, 2.0254007743767777, 1.1053097702500083, 0.9386873812049207, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 1.0489967672524148, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.8892406769100474, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, 0.9980180411993752, 0.7730503979626255, 0.05012452522302984, 0.9412848237200061, 0.3151842349303073, 1.0344375696524877, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, 0.8589247883431155, 0.9875820054800791, 0.974735950859411, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, 1.0306645007550932, 1.097718904549744, 1.998930446459027, 0.9500022266038769, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 0.9263216268973042, 1.6185029791547985, 1.86328352562573, 1.0157932159614604, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 0.8960483177245648, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, 0.18682261802422412, 0.9785598832082368, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 1.0467834766218154, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 2.069498120967828, 1.0746796458168144, 1.8384436606065246, 1.5854004542490547, 0.9502141462766489, 1.0089088191871194, 2.008509873407435, 1.950669255439995, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 1.0252337252138446, 2.0061978338583937, 1.9192704169033825, 1.0490609947203429, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 0.9401730117100286, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 0.9356591486239149, 1.3572078064349313, 1.0224961959128591, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.9493236420152802, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 0.9324771987893767, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, 0.9769910506371062, 0.9522384439744107, 1.1663479471564113, 1.534681762803947, 1.037439503990736, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 1.0504913158044216, 0.9416598253275545, 0.9653269741882563, 1.2226875287424543, 0.9438735111314883, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 0.9445673085715209, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 0.987043634180512, 1.2347826949424812, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 1.012479575428554, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, 0.973726870455468, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 0.9565549092838592, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.9921053817619112, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [-0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.30597998244822927, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, 0.17013082886121492, 0.1795255739860949, -0.16386479200305562, 0.07038051427651006, -0.3953522895945679, 0.20049308034949212, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.19620781805876192, 0.05677354690499763, -0.49075947312125456, 0.33301568358301975, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.234079692103942, -0.013644617927113946, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.09637557897209917, 0.23220564206501929, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.18011861732039008, -0.405582649094571, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, 0.2670293326736063, -0.1570953567668757, 0.04375671133118682, -0.04517604084317692, 0.5160335878313548, 0.1542936172964292, 0.2164966819737342, 0.1674674984914969, 0.2704560677068569, 0.029465956171557988, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, -0.06137931074981458, -0.0014130783217369686, 0.01785388719359586, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.20351931931804346, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.02569784702500033, 0.028879374506168877, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.03674552072388997, -0.3815626054906763, -0.30131756948222327, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, 0.13088286445288597, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, 0.0677453838688456, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.3792433919842689, -0.467268047386986, 0.23472335199424224, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.13469184867095194, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.31359150938324615, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.19918801224063537, -0.4720015970691104, 0.14978588115915387, 0.33927881802500365, -0.21958607670366795, 0.1051489326854959, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.10924503343914722, 0.19915064453773526, 0.2901899106002602, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.09913384146619357, -0.5445274689896823, 0.41683314533623594, 0.1223014150437775, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, 0.293046936035356, -0.24519104116011547, -0.008690954824159736, 0.3571915030054305, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, 0.12562704192371885, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, -0.2994973461644936, 0.2919937556919383, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, 0.06362642722102937, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.4362608360142391, 0.2848873018568047, -0.04600110928594187, -0.4060725545016431, 0.06709165073838032, 0.26874213756713305, 0.4611731921189639, 0.1280078066973535, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.23952636533842916, 0.19334620117519105, 0.2195221158776203, 0.20629588358405002, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.3389677681314225, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, 0.007092961807579082, -0.4104902712188111, 0.13347628461694513, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.22980460025990193, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.05498257762065896, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.2253268053322805, 0.14879379507579316, -0.46060854249014255, -0.3099236494641088, 0.2061980698699697, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, 0.1342348865555803, 0.2761789058580983, -0.5001340667227457, -0.39384046284985436, 0.344303422286615, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, -0.07676642286508525, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, 0.1717890736536283, -0.424752896655102, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, 0.09123679877099287, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.35626734597179077, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.14832545170674544, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, 0.06072464380436819, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('b1aa98d7-5474-46b9-b93d-6bfed5d06810'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model )","title":"L1 Q1:"},{"location":"exercises/E1_Neural_Network_Linearity/#l1-q2","text":"Now add an activation function to your previous model. Does the model become non-linear? # Code Cell for L1 Q2 model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 500 , verbose = 0 ) results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 495 0.093515 0.981 496 0.092895 0.981 497 0.091976 0.982 498 0.091197 0.982 499 0.090521 0.982 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"ee1ca0d8-cccf-4afd-8a53-a241c6fe1016\")) { Plotly.newPlot( 'ee1ca0d8-cccf-4afd-8a53-a241c6fe1016', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 1.049364864300265, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.9747088538643435, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.9217096483090739, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 1.8560150272613007, 0.9213927020182104, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 1.0262181373626456, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 1.027708672633818, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 1.0467834766218154, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, 0.9502141462766489, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 0.9356591486239149, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 0.9324771987893767, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 0.9445673085715209, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.9921053817619112, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.04375671133118682, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.029465956171557988, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, 0.01785388719359586, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.02569784702500033, 0.028879374506168877, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.03674552072388997, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, 0.0677453838688456, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, 0.06362642722102937, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.06709165073838032, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, 0.007092961807579082, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.05498257762065896, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, -0.07676642286508525, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, 0.06072464380436819, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, 0.05236582472018032, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, -0.14441356025963623, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.02372728551128933, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.880199864297024, 0.27234218972315183, 0.987043634180512, 0.06076367955538808, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.07116809834154524, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.5222017799968333, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.5146634657066751, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.5233938680708092, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.5003388639021644, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.5088865294391419, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('ee1ca0d8-cccf-4afd-8a53-a241c6fe1016'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model )","title":"L1 Q2:"},{"location":"exercises/E1_Neural_Network_Linearity/#l1-q3","text":"Continue to add complexity to your Q3 model until you get an accuracy above 99% # Code Cell for L1 Q3 model = keras . Sequential ([ #### YOUR CODE HERE ### ]) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'binary_accuracy' ], ) history = model . fit ( X , y , batch_size = 100 , epochs = 100 , verbose = 0 ) results = pd . DataFrame ( history . history ) display ( results . tail ()) y_pred = model . predict ( X ) > 0.5 px . scatter ( x = X [:, 0 ], y = X [:, 1 ], color = y_pred . astype ( str )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss binary_accuracy 95 0.021280 1.0 96 0.019798 1.0 97 0.018461 1.0 98 0.017353 1.0 99 0.016389 1.0 if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"fa337ce2-7fd1-4407-92ed-9078ecb1cdc2\")) { Plotly.newPlot( 'fa337ce2-7fd1-4407-92ed-9078ecb1cdc2', [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=True<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=True\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=True\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.02137124176534741, 0.9767004512590101, 0.9040588181889767, 0.37736315593139597, 0.5893033674343496, 0.29248724488050315, -0.026378361734335026, 1.6201408002696527, 1.2671828519438877, 0.33220069503933425, 1.9528119960894716, 1.9797372994482496, 1.7292636434383168, 1.789588100653137, 1.0280973990524327, 0.003467472728882294, -0.009956890749796943, 2.0610013072906024, 1.7978819432299944, 1.9950804766368369, 0.22347835896743645, 0.6064473078062914, 0.22571792435646254, 1.8264689725355827, 2.114573602922549, 1.9331897179587911, 0.1839527505802008, 1.4482994021578994, 0.03927763416731231, 1.7276007815375978, 0.17219449019081365, 0.2592820495979804, 1.4236794415177, -0.06916239179473493, 0.22129849296716522, 0.6383540756953149, 0.5182235408416075, 1.6513479104495974, 0.03195991474457149, 0.1813037127643914, 0.9289773585494512, 0.5063106204922861, 0.06942815391313314, 0.6363666829222565, 0.028529406322205582, 2.107742158583974, 0.7472132777121037, 0.9576110668652604, 0.09060345442824758, 1.2259994001145722, 1.0737613683967255, 1.5302872382727954, 0.8193499688889234, 1.927132618544801, 0.3396973495753294, 0.2990892258697176, 0.039627331512575664, 1.9774500535344222, 1.7917769304081708, 0.04048431440928878, 1.3920122159014456, 1.9079747606592947, 1.3405775237186934, 1.9685855923937061, -0.053504864135638805, 1.8038830076655947, 0.748948740808155, 0.05514691703582684, 0.15407894482893966, 1.7174943175246813, 1.2451341863071925, 0.2237028071897016, 1.6474147351820063, 2.0456742978149194, -0.06746998748693264, 0.8798867459445642, 1.7595863893485137, 1.8879655570584535, 1.346952559188547, 1.480554109948365, 0.3433614582910149, 1.9484177139008794, 1.96124335725528, 0.7196622785080242, 1.7838907365498862, -0.0004480613532620026, 1.1702347337962746, 0.47484883409038314, 0.3317739989643634, 1.6767534541171911, 0.5557643618513285, 0.6533698201044806, 1.9867331318361088, 1.337871790654681, 0.5861423330338507, 0.5289005103096923, -0.004780511062108857, 1.4751730271216446, 1.972904240930428, 0.9118024391714524, 1.8874002057435182, 1.5123085979208808, 0.5008496411497253, 0.24627662146488044, 0.037662937381448866, 0.6989590699758024, 0.0866983907273326, 1.470844471256458, 0.23961380079492778, 1.8915139434900679, 1.4149844263627622, 1.8795783019431593, 1.4601402660267848, 0.5631458703213645, 0.505938936618251, 1.8237284273484695, 0.8617890757089063, 0.8587543302771186, 1.9751469952452954, 1.8067823256210032, 0.01345055834815248, 1.9170066261220016, 0.045238045924702014, 2.005231249144342, 0.12948978784396156, 2.122675629230518, 0.050525847113584425, 1.2134043847723197, 2.0149218966869675, 1.1636033071860212, 1.7815074696421953, 0.025859919370369393, 1.9069142312438205, 1.9401659307880803, 0.7790334103991241, 1.739468418562985, 1.3187551851741415, 0.3049130067182995, 1.7825289955102306, 0.0037866371787284034, 0.15606479311853597, 1.2630027463416758, 0.1736533194704183, 1.2930675494841775, 0.7429789732853964, 1.9698221925388755, 1.973064864650542, 1.2043505112854298, 0.05236582472018032, 1.8560150272613007, 0.041614857041363154, 1.155756186016954, 1.8930245663669192, 0.22873470686652986, 1.9514279260266112, 0.07501920323939616, 0.1606189694102546, 0.016099275663376437, 0.08370632261382979, 0.7841558501859509, 0.8165937829227553, 0.3311065969289635, 1.2325778713164164, -0.013483712865137141, 0.7462129108559318, 0.751555299880831, 0.8397036715524079, 0.4231989432627436, 0.790773566950995, -0.0333870459512904, 1.7699943333823245, 0.03594467094355608, 0.02378383532079511, 1.7848942652190163, 1.9810512506649693, 1.8279435152801018, 0.1279384010309648, 1.1121972017368138, 1.7092389835433595, 0.7360538785534525, 0.9463166409279263, 1.8496096782631468, 1.3779600594125094, 1.1381032547306398, 1.7975757981798375, -0.14441356025963623, 2.0254007743767777, 1.1053097702500083, 0.6511475042110576, 0.15037229947894018, 1.0407754082730785, 1.9745653327281836, 0.028498130131812523, -0.0005931332760947608, 0.30664611939361913, 0.6630934871644625, 1.846528370379491, 0.05938183068183091, -0.11006562690830578, 1.8351011264909998, 1.2112640644720454, 1.332332014428568, 0.7494549993616355, 0.6582395614993388, 1.5101879214682936, -0.025610378067933835, 0.7730503979626255, 0.05012452522302984, 0.3151842349303073, 1.7067700791871292, 0.1639608272398762, 0.07680084701622314, 1.9939024983067954, 0.36212092076740127, 0.05507085892061413, 0.1722035888759144, 0.39124411287504435, 0.44080787330120264, 0.5530920912545451, 0.1350846962801227, 1.7421546804334165, 0.2402888633712877, 1.8438768678642203, 0.5310418626479455, 0.19740722097108435, 2.007510326425419, -0.02849832116612446, 1.9654465339224418, 2.0039253826299612, 1.9748942831212735, 1.9964524832853443, 0.1794165330999754, 0.16621598347955552, 0.29020049664857867, -0.020658309769524194, 1.7747864019616464, 0.11808024999143729, 0.9703699037886488, 1.6323651875872884, 1.751037225027912, 0.4327084392492476, 1.8701775093337638, 1.2881298002277455, 1.921188582608904, 0.6169662392098072, 1.9483874840764341, 0.9831396233126057, -0.059660086073984084, 1.097718904549744, 1.998930446459027, 0.5298317556988078, 1.6966052004933032, 0.04697393609253137, 1.4384993065953982, 0.4051401818686814, 0.13579291901212504, 0.2493331504861996, 1.618564793465073, 0.8744883378184299, 1.997897894004819, 0.04112200360905427, 1.3966771834821312, 0.2088046765137585, 0.6666417997815854, 1.6185029791547985, 1.86328352562573, 2.089646129566544, 1.4669834439461524, 0.9286580312028455, 0.5397040023316766, 2.005786138282938, 1.265958305102635, 1.9009061182212441, 0.26854727881522983, 1.417333239042822, 1.2884055810780777, 1.2994282078900423, -0.021306689117003466, 0.18682261802422412, 1.4632507510889974, 0.4818781502363078, 1.0381259577624737, 1.6756272435967492, 0.15366639724410308, -0.0013791069746475343, 0.8787700210264979, 0.45416019741913094, 1.9391311317991364, 0.2876965990001003, 0.12403530794244598, 1.1650343477488616, 0.8271855530078582, 0.7832138810684961, 0.556716137641891, 1.9792537105434254, 0.026933766169225222, 2.069498120967828, 1.8384436606065246, 1.5854004542490547, -0.007849683379450141, 2.008509873407435, 1.950669255439995, -0.03450175489395475, 1.9122286307695038, 0.7830248375464267, 1.650357567663155, 2.0061978338583937, 1.9192704169033825, 2.0027734756022153, 1.990110706121436, 1.6104218342284438, 0.7203613581635865, 1.459872164146514, 1.5134938761044576, 1.7028088273169695, 0.02372728551128933, 2.0742272547332874, 1.4381982100995288, 0.10786937518049508, 2.007418311346916, 1.7485411150245282, 1.269242075565549, 1.98662387681975, 0.9863996407585056, 0.6148815245005763, 1.0008493828357523, 0.14330554698428066, 1.1857369071786714, 0.47630071613682673, 1.9692119161270707, 0.5339136986500325, 1.5442852173359636, 1.0254459036538643, 1.3572078064349313, 0.4085914134658922, 0.9555708401625238, 0.09674490945399504, 0.20891960230694748, 0.9033409850043334, 1.7595198387596154, 0.22413948364200742, 1.2720102945585732, 0.3759588366643407, 1.9054269551641303, 0.1088360353798975, 1.7781686156616714, 0.9131936579047576, 1.9573488562918777, 0.9134444537387055, 0.5178329096679996, 1.635694786421806, 0.1840392824700099, 1.8848565006149718, 0.23170521815608025, 1.8353068602390123, 1.1013745411540978, 1.7507859416822633, 1.469933985687836, 1.7619274539984036, 0.317029756648171, 1.2576690900707146, 1.140534502005988, 0.8510567032049889, 1.8200289370913776, 0.3855678284342961, 1.832080754959263, 1.0634936112769395, 1.3053348150185526, 1.8621721833533527, 0.325996513904797, 1.7573640359274352, 1.0580777929774057, 1.9823097592146892, 0.08004379141252124, 1.4333027745425153, 0.6603635483887582, 1.611278836084678, 0.14822076605677742, 0.5874122948186635, 1.9750561801758517, 1.5363449378274021, 1.1845436903499498, 0.6173283311001421, 0.48098980209674824, 1.7606291290200764, 0.6605825424517209, 1.0043291321131098, 0.2991117272202126, 0.35279701969295335, 2.007399214271862, 1.2869309464006964, -0.11317939378933384, 0.3392984267693231, 1.9599907938610681, 1.985322021629067, 1.7951840131930346, 0.29461885686695893, 1.60614653328497, 1.2973847427178125, 0.40547734263154167, 1.590514881405918, 0.07779476062119, 1.9003372147960562, 0.05671920919605883, 1.3144820850804113, 0.6269646163400832, 0.011583851976794662, 0.07361863450396877, 0.406144019076425, 1.763596683339985, 1.6486915549879573, 1.5784073139407897, 1.9194571347552354, 0.8604744183514398, 0.1050688352018364, 0.9485547267590134, 0.05850233758424467, 0.7037244996391261, 0.05483770690547791, 1.796519025042513, 1.2999899944042876, 1.1027061334692163, 1.5413356967250154, 1.1285843092956866, 0.3787275371250402, 1.9640493582511989, 0.25879463399588387, 0.9965972911648352, 0.006139402232989595, 0.770050828617131, 0.6041502752345098, 1.3638882469117608, 0.8185628281649207, 0.14273001233810748, -0.01796529840604455, 1.1663479471564113, 1.534681762803947, 1.6846267444491305, 1.1756935498597783, 0.5297805518522823, 0.20628531048316168, 0.42256340726764485, 1.9101111437351328, 0.05643656243980857, 1.5304580449002139, 1.8575318832645034, 1.4242140233798932, 0.9653269741882563, 1.2226875287424543, 1.762308943971179, 1.0815176342417825, 0.7972594934522245, 0.8523282860094877, 0.6435205295945912, 0.8201188281664391, 1.949698043093755, 1.4632739185814487, 1.711411973570764, 0.0377421527141316, 0.8983474237124535, 0.6461149510574886, 1.4203855929449098, 1.1095924557853807, 0.1582506264818656, 1.8833546686102136, 1.9069052925816163, 0.12026467007841032, 1.2347826949424812, 0.06076367955538808, 1.8029873997383545, 0.06372793373743997, 1.1467960086968432, 0.27559690624013683, 1.5573957022189715, 1.9640988136292095, 0.4787810817445109, 1.4987652585129752, 1.9228331053919576, 0.07116809834154524, 1.4986023046344203, 0.09278557158171516, 1.6245480211656356, 0.11573681035972688, 0.6661308525351213, 1.2447234938305243, 0.6066243125576949, 0.5580310295644941, -0.03253814552150909, 0.15796310516079087, 1.5864160940282395, 1.0113964275939702, 1.9284727682287581, 1.3955926313105051, 1.1029894137565603, 0.42181790965367505, 0.48754233720761514, 1.6784404873697873, 0.30098821838516837, 0.744140443395479, 1.6625846200665195], \"xaxis\": \"x\", \"y\": [0.4061860760728654, -0.4583230648958083, -0.3765195183209672, -0.39703716744111384, -0.32137599301771974, -0.20696308753957884, 0.44766267100521034, -0.28758855823009816, -0.40793471050810065, -0.1903220237466552, 0.2255233265092496, 0.08315130002556324, -0.08800079322720622, -0.16231967104251357, -0.48303309784936055, 0.1763581693865052, 0.351022501469148, 0.3960143671698958, -0.10106546073450899, 0.2271142085715099, -0.21586279245949078, -0.4212323135166668, 0.024977047589629604, -0.05647377016362439, 0.36945226280097176, -0.07086860261133202, -0.062473487613485354, -0.36510073669959753, 0.06597151810090561, -0.2516050380479384, -0.06381550157535147, -0.16386479200305562, -0.3953522895945679, 0.17661598882501486, -0.12171058712942494, -0.408799626771914, -0.4183909292604944, -0.24634340470786834, 0.2340754360790136, -0.08944621278233666, -0.510029264631326, -0.3677976488360454, 0.05677354690499763, -0.49075947312125456, 0.2696835681307252, 0.5273171289504526, -0.4960348041311498, -0.4947163632719785, 0.44046007450720215, -0.43192992061372937, -0.4859081555214971, -0.4304313563314821, -0.5655198060066096, 0.12263604229336884, -0.2841725640704342, -0.2380513220034473, 0.17866537307866523, 0.234079692103942, -0.013644617927113946, 0.3512595859263131, -0.34835877115116176, 0.4691799647680357, -0.42494305591392667, 0.39982060303077027, 0.28467928313356083, -0.13450494592689866, -0.4293870444629516, 0.14378153218491985, -0.09557523761053704, -0.22448294846397726, -0.4206072593789681, -0.08135224116333942, -0.34429730965075306, 0.49051265245336784, 0.23050518847115897, -0.4028902160777428, -0.14885474512953262, -0.04110588559652935, -0.43673822048439637, -0.34519903966654514, -0.31869193161881093, 0.3701702341160022, 0.19708634921110102, -0.49340521395819037, -0.21543869376133298, -0.10658348938494083, -0.4767799985778196, -0.3630095581477992, -0.28699829251186965, -0.23178610771605618, -0.35013986680104026, -0.3854601791172613, 0.437254551456034, -0.46594001225453546, -0.39522788407776976, -0.3569102788487348, 0.34577178535893566, -0.27638029140841375, 0.20279704092807, -0.5203196892381192, 0.17551704890955575, -0.3279704513047752, -0.2661609658321474, -0.16438222715020687, 0.2699238672297299, -0.405582649094571, 0.1791693072694373, -0.3391802287119178, -0.043693783369562436, -0.026010502210310968, -0.39271918981335385, 0.04951466728441879, -0.41771656670951957, -0.3529693202363816, -0.33464354412947306, -0.06752866669723487, -0.4671916367297798, -0.4933593942306419, 0.34516872759181816, -0.1570953567668757, 0.20426594971541467, -0.04517604084317692, 0.3093597670558822, 0.5160335878313548, 0.1542936172964292, 0.2704560677068569, 0.4556311320681723, -0.49691173784567494, 0.4350805779252967, -0.47175850915187234, -0.060831116504472336, 0.37632306770073937, -0.06137931074981458, -0.0014130783217369686, -0.4805161212850643, -0.2388520573994403, -0.38880631602459426, -0.26192333261031603, -0.08323851419298126, 0.4099959536645816, -0.07175030992060519, -0.48603916409127124, -0.06661085196672883, -0.3554466153293382, -0.4457193448524265, 0.26813589050438524, 0.190258884605349, -0.5041389513121618, 0.5222017799968333, 0.02569784702500033, 0.20972429653054808, -0.4849424137294191, 0.11374451033285203, 0.07582801055837357, 0.1308745386572107, 0.11018531951623901, -0.030204660697929007, 0.26473023395633355, -0.03735414513002047, -0.49528840167789856, -0.46330105698977236, -0.2144652925475458, -0.46585115597183846, 0.48229393261157355, -0.43319891443616887, -0.48547524624962674, -0.524150615444562, -0.28766255568915755, -0.3815626054906763, 0.22178526030182227, -0.30131756948222327, 0.2997910331159463, 0.16855957491065562, -0.0665910731709224, 0.13660024640650817, -0.011642381835068535, 0.07871634845323806, -0.5010192741549158, -0.35548172079177337, -0.4610881619351797, -0.5114164410334371, -0.022935938056117114, -0.41126698511928245, -0.461218231709397, 0.007243964529664387, 0.5146634657066751, 0.3792433919842689, -0.467268047386986, -0.4791468813357371, -0.06255384159572161, -0.4494169500181439, 0.42885668629275014, 0.24820871257833463, 0.3053079331926927, -0.21624987078715377, -0.49550450791345874, 0.05487082868594822, 0.4127985592774463, 0.3771905515137585, -0.09093132806216964, -0.47004486273214596, -0.43138934329191997, -0.423169886416473, -0.3503292324442232, -0.3281413991388843, 0.3953381776673755, -0.4720015970691104, 0.14978588115915387, -0.21958607670366795, -0.24049274540801777, -0.2597915296356863, 0.04549620472209095, 0.3102069849204364, -0.27540337094061534, 0.23375974123782337, -0.034307817720931555, -0.2498660561593295, -0.3366527360088319, -0.38472688401355126, -0.014257023261254526, -0.1837540348606524, -0.24314235378398685, -0.12649031652831352, -0.329285898806121, -0.1486050014430451, 0.3353613214434557, 0.2385485980623932, 0.05047610190433598, 0.39640492916913445, 0.12276049274392374, 0.14415511656727206, -0.07255868720010722, 0.012998220974296412, -0.17578660292800455, 0.2869686430492015, -0.2961415116236608, -0.05600095485044723, -0.5578863156519999, -0.22835371624305612, -0.1907002936430473, -0.41383499313481653, -0.054789148604113574, -0.45665718303554276, 0.21230160586222674, -0.4272278422646017, 0.310981494088566, -0.5006563858372098, 0.2117040879093407, -0.5445274689896823, 0.41683314533623594, -0.38745300329638643, -0.13040658859699517, 0.1429532190010081, -0.3678733987567342, -0.2818068282163254, 0.1457565233343805, -0.16463514325194664, -0.3665942587017212, -0.5292664350026407, 0.18153241711314042, 0.41493145306385265, -0.38325516299803597, -0.009266433896129293, -0.3993537411926558, -0.24519104116011547, -0.008690954824159736, 0.5111977639044867, -0.33197563927650525, -0.4811806238347502, -0.32670624237749574, 0.2517367430052659, -0.4108599172569497, 0.06884624921625394, -0.16934228945630578, -0.49649196599262907, -0.4342955629128192, -0.391228050261687, 0.27666404974095093, -0.2994973461644936, -0.3592554626920059, -0.33058701912230276, -0.483749733290317, -0.30196268140090565, -0.09434073568509392, 0.06431261997843951, -0.46020108759798356, -0.35852250778355554, 0.28322985858303007, -0.03272397505213495, -0.1533052186434246, -0.5538646435942736, -0.3847805177457887, -0.4469712669556637, -0.41716918739053505, 0.25427922835640343, 0.40034297008824193, 0.4362608360142391, -0.04600110928594187, -0.4060725545016431, 0.4311452112892778, 0.4611731921189639, 0.1280078066973535, 0.39598707601074584, 0.22183077041778737, -0.5130443944643716, -0.2876586058051412, 0.19334620117519105, 0.2195221158776203, 0.2791591168023465, 0.33784215299305254, -0.2745889347500731, -0.49648995875084545, -0.37089188599707423, -0.25464025729806505, -0.18686790327375957, 0.5233938680708092, 0.27497396850831407, -0.4290266587295294, 0.09455846835868911, 0.33050218726665287, 0.01647781207852024, -0.373794389437505, 0.40151135775759583, -0.5209083849835582, -0.4616613245553046, -0.5573282811293513, -0.03915308816129039, -0.4611575896292627, -0.40139914916613406, 0.06459128794776822, -0.32609079417826137, -0.27030211320979936, -0.5405754540652656, -0.4104902712188111, -0.19709239955080726, -0.5635488387131781, 0.03935232959008142, -0.18722714593536427, -0.40548091611549897, -0.11474333251102056, -0.1971201739041523, -0.5070311626880198, -0.39262296954736897, -0.15197652003938208, 0.17353941166679487, -0.07831611469642595, -0.4948732745220247, 0.13990310302916548, -0.5351697205917099, -0.3686040986997715, -0.27262385990772786, -0.1181585480107689, 0.16310217004119348, -0.14037098421020477, 0.08395525905410062, -0.49408546973413037, 0.007533422208704815, -0.3418352960629417, -0.0979450791986222, -0.2500306027966267, -0.4485312851520229, -0.42319743192671533, -0.49257266389918386, 0.02912799165352744, -0.3229995165168446, -0.0037677342948546307, -0.55151959896679, -0.38111958979848415, -0.02151310653751875, -0.2281699208647904, -0.24140487034603894, -0.4505672333571912, 0.4299160638551837, 0.37696350991865785, -0.4023712162668439, -0.4391884728666082, -0.24056081510259955, -0.19466620888258224, -0.3963442851524476, 0.4921087226499833, -0.3169110668448662, -0.49602582762261843, -0.3543297813157656, -0.3407501953440836, -0.1645072764585691, -0.46171768169208605, -0.6198949615593068, -0.21630106121441156, -0.24071859885323263, 0.17731412254586246, -0.40589858333167295, 0.42171188266107507, -0.2765971195231466, 0.3846723857950752, 0.06235966220807006, -0.05991679836558082, -0.17847000223050563, -0.3199082654434057, -0.47369946279909625, -0.34098172995460835, -0.2504036441729891, 0.1362159595166606, 0.3175533331049689, 0.07641896671175771, -0.47108917856272214, -0.4556865878347496, 0.2681596821679075, 0.21994047525303553, -0.3287827196362138, -0.19523967174519058, -0.2610286666010861, -0.30085098745780675, 0.07974242024047626, -0.4867565609411477, 0.08462690019342022, -0.5610516183525553, 0.09232673581761183, -0.3718903682603237, 0.25059410971915247, -0.1536972001584023, -0.4253054616573801, -0.5189382845453103, -0.29685089429169703, -0.48570454398856633, -0.2501967865427847, 0.043810024896126336, -0.22711133075944995, -0.5596217228214833, 0.18454012286879, -0.4864987917356861, -0.42310147851257024, -0.309998550926577, -0.4420895878541432, 0.04188902338886154, 0.3138372844046242, -0.46060854249014255, -0.3099236494641088, -0.15991856779098618, -0.6179021184258471, -0.33246005027412895, -0.01838203013244639, -0.327378254766558, 0.3953493792458464, 0.41458517754244617, -0.3965605112268295, 0.277009924192454, -0.42728863419164936, -0.5001340667227457, -0.39384046284985436, -0.240288927536174, -0.4979585890635959, -0.4681693056022481, -0.41780283505235516, -0.5043479855116628, -0.45047049731282135, 0.42602413245146686, -0.4919269546479593, -0.30290727895118685, 0.15244452738171008, -0.4993570847729972, -0.429568570409702, -0.4267845476641277, -0.5742754996974517, 0.07143306465657524, 0.0863596104615732, 0.3867739263941996, 0.06103080945209591, -0.424752896655102, 0.5003388639021644, -0.1117260298126734, 0.026707120903246673, -0.4650921552723531, -0.11342576787592833, -0.3136291646838624, 0.4124179709225864, -0.31014682084986656, -0.3585847844214688, 0.39100404554896934, 0.5088865294391419, -0.28562725320580185, 0.13465646755547123, -0.1895166388286573, 0.10601675313644868, -0.3890865981012992, -0.5300059382894151, -0.5516589745627907, -0.49711468889367605, 0.43728846428088664, 0.08026063038867517, -0.30812883217216425, -0.5518191270267814, 0.2767918770828352, -0.5465862304424174, -0.4129282672912732, -0.32201273977909656, -0.2681105581944324, -0.200177757167414, -0.10490681680684293, -0.4638932538587638, -0.3079192974488752], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"color=False<br>x=%{x}<br>y=%{y}\", \"legendgroup\": \"color=False\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"color=False\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-0.841925571991321, 0.682467332555129, 0.8741860776206716, -0.20280226248192992, -0.41546770859685006, 0.4831018198648976, -0.6441953693353871, 0.19155834582323616, 0.1666505883345491, 0.787176934158832, -0.501556254167852, 0.5592992302226678, -0.9914702898209569, -0.7324780053165639, 0.3276079199427982, 0.9443139169305158, -0.5414160541687476, 0.8320404551747068, -0.3879217644130111, -0.33602530250350193, -0.11872037817262393, -0.865991827636781, -0.032116240642788764, 0.9386747622287412, 0.9422608694499245, 0.9641405035778899, 0.1393412145128936, -0.16009343904193618, 1.012193132759459, 0.19334863345288741, 0.9287172152733659, 0.746327515916764, -0.9943493334391437, -0.22369599100458393, -0.23439526743015504, 0.9371456802671978, -0.4162270875140181, 0.9527236026865668, -0.09751882951540852, 0.8246730415480225, -0.369425283058781, -0.5502716592604471, -0.6404742039978804, -0.9781422260432127, 0.14683167526526725, -0.9031858805065766, 0.7786937878528977, 0.8461658161110637, -0.4923444527611955, -0.010532785476474734, 0.5896368379627389, 0.910197514429224, -0.871511586474877, 0.4650983141814987, -0.23558716675493915, -0.8369697720562213, -0.13631551718687845, 0.026491760271055168, -0.9752932613852875, -0.122922615199171, 0.8970144239590865, 0.7129154298647282, 0.16519466430868734, 0.8909551327802967, 0.8454796924228637, 0.9535901481763159, 0.13122493007517538, -0.27551758672630255, 0.015949890070261825, -0.41791790705782667, 0.662595851647462, -0.9225781667003112, -1.0155188927618566, -0.45332645775587854, 0.8874526650631741, -0.28034152440456417, -0.344582921827727, -0.4042105777500545, -0.664246307468959, -1.0265601231065522, 0.7905422542877861, -0.95576492430432, -0.6540610699855107, 0.009338007384638743, 0.014982459587409312, -0.9390554836282453, -0.7285612260371523, 0.6265179559034115, -0.5980777994869654, -0.6713042375828886, 0.30148672595569054, 0.22204923505624388, -0.901686985713673, -0.4514663111856433, 0.2227686418623045, 0.9293536410235851, 1.0086540617608457, 0.4557161601585934, -1.0675964821437036, 0.8378590398759589, 0.5994638541545335, -0.43970939415791677, 0.001939034376703431, -0.3161043554127958, 0.9438415932280083, 1.049364864300265, -0.5775693779815646, -0.5894412664957717, -1.003483362657891, 0.9600191987137255, 0.9294360806160966, 0.14714304085927188, 0.3074913555842822, -0.5582076929017485, 0.9747088538643435, 0.3371084971589208, -0.42634765510706046, 0.7683645452457621, -0.9887667184006907, 0.9248048203622942, 0.8900332512110143, -0.15893857542769574, 0.889755821566578, -0.9162745929631899, -0.5041796650698196, 0.9217096483090739, -0.9901397931797628, 0.8516985508778182, -0.14561296270184196, -0.5839445171011158, -0.7474011078828955, -0.734089636000622, -1.0049409374311087, 0.7270091725557934, 0.003819246449453647, 0.8933491957200252, 0.28693459678931693, 0.8865935042028288, 0.1142057975243623, -0.27467109309712384, 0.24989922747741583, -0.6905443807169022, -0.9894431937403603, 0.1914388385046656, -0.939747216799151, -0.5860367228694352, -0.9749898950201394, -0.9460060520199547, 0.9213927020182104, 0.49581083660996056, -0.8773184770510103, -1.1067327332191668, -0.8818228021603742, -0.4349828008195228, -1.0444988327098021, -0.8826306636855094, 0.10478005830047077, -0.6081652305508954, 0.32623579965960603, 0.886865955844748, 0.6662809207524713, -0.35349092607964283, -0.525014070807405, -0.9386471206896609, 0.7068007202242268, -0.28660952719264865, 1.0262181373626456, -0.4242503002479633, -0.9696961271110569, -0.8981885613259298, 0.5176112076347361, 0.7211272266102963, -0.8704962842266091, -0.93388110963664, 0.8126292481813207, 0.4411477343364732, -0.21226578438982383, 0.8940941762766556, -0.25349055244319174, 0.6292083348658327, -0.35835531826760353, -0.811404142238073, -0.3975211685555937, 0.5897374577605289, -0.8840464057558804, 0.622576864851381, -0.9791656577645753, 0.5378544352470284, 0.9309728496845366, 0.35261663224760875, -0.8079995409866508, 0.5272069315204925, 0.6168226317048133, 0.44129228197492437, 1.027708672633818, -1.0506684482526314, 0.8444048862137858, 0.19558207794121654, 0.5789427356734955, 0.07884409115952734, 0.21004740763789947, 0.688271037238145, 0.360575625318509, 0.9386873812049207, 0.7927304779194625, 0.8617756759502471, -0.8049598469457511, -0.9884967025636874, 1.0489967672524148, -0.935842328272884, -0.6267995490316154, 0.8892406769100474, -0.03404569998465, -1.0064524676654591, 0.9980180411993752, -0.9830827066454677, -0.9639518963702124, -0.6400927973513629, 0.8439790921232345, 0.9412848237200061, -0.11082515480158367, -0.2842734477511065, -0.6268511560806208, -0.5793790155512624, -0.9927469250871123, -0.10568746527056146, 1.0344375696524877, 0.36960201201667353, -0.6509019122589677, 0.19503730371646702, 0.4908312066810775, -0.7839391293806417, 0.5427850808377916, 0.12446402919555219, -0.7415566357055887, -0.8662308793820274, 0.9348459697177338, 0.8218328129152243, -0.7993662189785357, 0.8070927734783936, 0.6808152881569263, -0.8923840242075799, -0.9357907116784944, -0.9286626231968224, 0.9209491382256922, 0.8561387129939999, -0.4073503339604967, 0.8773274863423824, 0.8589247883431155, -0.8186016008031817, 0.9875820054800791, 0.974735950859411, -0.38678649200135107, 0.47888012878477054, -0.9425502609252339, 0.7818873970631965, -0.9910319240969651, 0.4867470338922282, -0.959087466807264, -0.8761551276393382, 0.4857676340516148, -0.7754546961327481, -0.7869816311952328, -0.04211856793139306, -0.5003137640460915, 1.0306645007550932, -0.45750351133460765, 0.11675022566808599, 0.36276377695916867, 0.9500022266038769, -0.6427158607366137, -0.5542136117666949, -0.8907767352581123, 0.2661707700737591, 0.7978938713967487, 0.8773888062169855, -0.2509521445460971, -0.9897356738787813, 0.5528277500221775, 0.9064509902438325, -0.9406773617062165, 0.32891584245760774, -0.08302679877482544, 0.15036434055699982, -1.0886859295192106, 0.7588814470312026, 0.7432105761091852, 0.7316902177614554, -1.0139580168391111, 0.9263216268973042, -0.6572881746353788, -0.10295591122486067, 1.0157932159614604, -0.6457067517714352, 0.338062931663714, 0.005503112200495708, -0.9536736834631697, -0.34098516306448784, -0.5696451530054487, 0.5630806672790234, 0.6773020373629023, 0.8749393304212794, -0.6116207856857013, 0.6449201999494786, -0.9236452640138701, -0.8283795775013268, -0.3606283338326535, 0.8960483177245648, -0.823259632802671, 0.9785598832082368, 0.4646728874744018, 0.018950500066500398, 0.5986400014800275, -0.21692875042714008, 1.0467834766218154, 0.7184353013368235, -0.39563532358090203, 0.0020953802889846704, 0.38327562731590625, 0.7624104613725036, 0.4974302302272833, -0.2742912209724403, 1.0746796458168144, 0.5280164903121144, 0.49694546350805135, 0.9502141462766489, 1.0089088191871194, -0.9876768764765262, -0.21027627935946355, 0.19520308489992108, 1.0252337252138446, -0.9175370571842107, -0.7016505210387458, 0.08429801967580965, 1.0490609947203429, -0.6446727019669775, -0.9234076427291916, 0.4762542880279132, 0.4324722566061934, 0.6552915662859992, 0.7680735320439248, 0.07677816623782226, 0.8525269294703098, -0.9632730124381795, -0.7812515608718273, 0.6420466136971893, 0.9401730117100286, 0.5931230589778632, -0.8998296135431146, 0.2735541970160142, 0.9356591486239149, 1.0224961959128591, -0.9651327972731387, -0.00744034775312594, -0.2324567213831401, -0.11189027471692276, -0.2952855223460142, -0.8474330468474298, 0.5138533690503994, -0.9393126305283686, 0.5946891091133355, 0.7231318458301057, -0.6258617275934728, -1.0012057785657524, -0.4318807803842828, 0.41028637633491494, 0.7497094761575074, 0.580365938420061, 0.31499037882258313, -0.1831499847618166, -0.29361910271335373, -0.7392161111824123, 0.7102820825877286, 0.39847669952508774, 0.2447104394037704, -0.883511688076127, 0.014287205761313208, -0.8773098151108902, 0.7973102796627205, -0.8745200062301077, 0.09917627095510018, -0.7687467327838147, -0.08384114583623242, -0.9868377529704373, -0.989656492495596, 0.7396237486538573, -0.03167264864017717, -0.7213533171423128, 0.1522406547147911, -0.3585300419674959, -0.8374682385763518, -0.7074600604308738, -0.06439035896652032, -0.10229260311405808, -0.6621737133999107, -0.7431148446980856, 0.1479688101074062, -0.9361536810858393, -0.5424982467778454, 0.8470477760290743, 0.06370403235649998, 0.5413296455098155, 0.9493236420152802, 0.5892592749123341, 0.8345448406575776, 0.06110828054398841, -0.9583995486859708, 0.30188691657237304, -0.8163345899821505, -0.8084897129625784, 0.16528839017978667, 0.9324771987893767, 0.7386135390848821, -0.7628793315212464, 0.9769910506371062, 0.9522384439744107, 0.3699024379670562, 0.6408239551321332, 1.037439503990736, 0.12793302111043478, -1.0266624186914628, -0.7601780410850274, -0.29151766151408376, -0.8118963680855009, -0.8307228499597747, -1.0229933517414864, 0.7187167327909912, 0.8948591436499106, -0.8943943261747443, 0.7209154809970969, 0.9740132594201618, 1.0504913158044216, 0.5341113402855393, 0.9416598253275545, -1.0639848876240656, -0.6750231019219641, 0.41509095823899256, 0.2320575374306522, 0.9438735111314883, -0.5137772819037197, -0.33237577650904043, 0.009385175833752665, -0.9159395290252508, -0.9583889583915941, -0.1567198008010405, 0.2654396765832578, 0.6194588462160627, -0.7462630501338061, -0.9135174088905357, 0.7229282419800694, -0.8017674916959139, 0.9445673085715209, 0.880199864297024, 0.27234218972315183, 0.987043634180512, -0.977757569878373, -0.9273553782354008, -0.6801879767687514, 0.9235398515738621, -0.5773529710154244, -0.5711594473614869, 0.0102690115109004, 1.012479575428554, -0.48105813799722064, -0.760738862660445, -0.06899959920039071, 0.542192881867624, -0.9751503331787739, 0.03165909256000281, 0.4788201674304247, -0.9616901007095184, 0.973726870455468, -0.9750360307004108, -0.008952862777465757, 0.9565549092838592, -0.43960808071688207, -0.9634774947719471, 0.4268845495239975, -0.11361667794354532, 0.30275579158701, -0.717926295245547, -0.927002780083817, -0.977848205382481, -0.5465021298811779, -0.41518387526048056, 0.6315020606957786, 0.7729843582975352, -0.6917321792409309, 0.7828262807231933, -0.9741741736528217, -0.5222400543915524, 0.22727153288324353, -1.0831006410360984, 0.7906874090373106, 0.9921053817619112, 0.9291918192609102, 0.798054909425106, -0.14549196420816424, -0.9435587275506016, 0.7941940613267031], \"xaxis\": \"x\", \"y\": [0.5305869534842842, 0.8071206917588001, 0.5078104062937839, 1.0227757577963752, 0.9342365814558391, 0.9515075766286676, 0.7255288065374311, 0.9283872946027532, 1.0159377976686577, 0.6353374225167823, 0.8714835294467724, 0.7816641355572947, 0.04177826991749532, 0.6095355254121818, 1.080074093288222, 0.30597998244822927, 0.97468109171144, 0.5732687838495222, 0.9626317486450024, 0.9451616576493191, 0.9559097035285317, 0.3279031765427942, 0.9895194662423529, 0.17013082886121492, 0.1795255739860949, 0.07038051427651006, 1.0176596674044673, 0.9020976159451496, 0.20049308034949212, 0.9778536919017272, 0.3879432914489443, 0.5874255696312145, 0.031248420905410683, 0.9173953823265485, 0.9818751419426923, 0.19620781805876192, 0.8943725943657549, 0.33301568358301975, 0.8786468540068699, 0.382172795220859, 0.9068915596643669, 0.910901764969981, 0.7076571700403769, 0.3239655128741004, 1.0059143753229096, 0.26782972705985697, 0.5551683580203007, 0.34639384516322635, 0.7874521362554806, 0.9232037901643552, 0.8688608803652028, 0.5213481084190841, 0.44035200495430116, 0.9692826800458622, 0.9677030193556847, 0.44519509171090715, 1.022867483802495, 1.0608013138227834, 0.3856379436662887, 0.871011826934748, 0.3790723293531127, 0.6454084399548248, 0.9239581410603653, 0.09637557897209917, 0.47206214221453446, 0.23220564206501929, 0.966681291860104, 0.984097704047472, 0.9407220428533601, 0.8815550652203666, 0.8855053296429825, 0.4557666673704325, 0.23607952100952503, 0.867787508630084, 0.368412983667411, 0.9894935919118412, 0.9952617060793372, 0.952360581073669, 0.7218877567390231, 0.15033085077668845, 0.658511194923437, 0.007284101935356932, 0.7598724528111099, 0.9320862835818461, 1.0088934434625134, -0.040601026684593984, 0.6745257125585686, 0.7658753623569184, 0.7611171626442352, 0.6453018117036439, 0.8925345009852251, 1.005376290206583, 0.625091325059969, 0.8577880033925253, 0.9326054282019067, 0.4544214481454153, 0.18011861732039008, 0.8593094307720844, 0.009375552298312488, 0.42613394784431835, 0.8254125216022237, 0.8165426656934837, 0.9714448094213274, 1.0286541910247868, 0.2670293326736063, 0.04375671133118682, 0.7664914852608554, 0.8826265104252563, 0.1789454932929465, 0.2164966819737342, 0.1674674984914969, 1.0496910377743496, 0.9728541528373591, 0.8931880707864593, 0.029465956171557988, 0.9537140769604843, 1.037485833477008, 0.7226740159027809, 0.39223558661869046, 0.4749728652333638, 0.441846271673797, 1.0125637198792687, 0.4593450358638575, 0.605640479730254, 0.9107940636281776, 0.01785388719359586, 0.27410981756835184, 0.5038333552077577, 1.0206425110383304, 0.8407084149111261, 0.7000376304130725, 0.6420506891671954, 0.004883933733048386, 0.6988473764149679, 1.0226217899943728, 0.20351931931804346, 0.9639785446425321, 0.45560484949814084, 1.0131968543991803, 0.9710435287885352, 0.9932291355359535, 0.8531919997908746, 0.3244471130378394, 0.9773646970897695, 0.25034315760119613, 0.7464152312408232, 0.1424986937123538, 0.5098904719734667, 0.028879374506168877, 0.7981273131858493, 0.4396666098985431, 0.07267437177451369, 0.5223149863127989, 0.7705942625598919, 0.3354378683085143, 0.3242285609034321, 0.9115123363221448, 0.7909478758543164, 0.9833737188823278, 0.4302512501908781, 0.7989226098393425, 0.9490554440705049, 0.7703485414808247, 0.07298159439471733, 0.7078223296189133, 0.8845639326784839, -0.03674552072388997, 0.9363076866507517, 0.0994494559895311, 0.4674170060188141, 0.9385110390286734, 0.6815114340126481, 0.6421184651234796, 0.44154772139454257, 0.571851929992048, 1.01433226967749, 0.9264541655421089, 0.5072156999570544, 0.9320171181555849, 0.765846606131384, 0.9539462745602485, 0.439492714268857, 0.8255152924019483, 0.717050230699137, 0.4994334683965428, 0.7625012604658197, 0.32170701622630016, 0.887706950325268, 0.13088286445288597, 0.9728034344433351, 0.5632723591391211, 0.8758606756143166, 0.7948343213229538, 0.9445576042501078, 0.0677453838688456, 0.11417567582262482, 0.5092356472357061, 0.9823731466909509, 0.8177573819797911, 0.9456267425872341, 0.985501974923304, 0.6989704640420543, 0.915544418207169, 0.23472335199424224, 0.5973529776217953, 0.3909265265004409, 0.6722924279598369, 0.34075838472143666, 0.13469184867095194, 0.2382146970526554, 0.9517795979001498, 0.31359150938324615, 0.9397949553902923, 0.32469793364884003, 0.19918801224063537, 0.07782114468558568, 0.3372709972657454, 0.7965253008349386, 0.4772883334123352, 0.33927881802500365, 1.0555416435788973, 0.9500045280851553, 0.7213348770226666, 0.8525040796958357, 0.2693533426586962, 1.0739092227180786, 0.1051489326854959, 0.9290136591624826, 0.7238574189589927, 0.9934997453960699, 0.9315553481927829, 0.544311717977997, 0.8915231978162554, 1.0537270620795822, 0.7276645708345509, 0.5197716874164034, 0.40288022527174744, 0.6317714989594121, 0.578121385928969, 0.6817100559603938, 0.602352350311978, 0.4526083586914133, 0.2782880324340885, 0.6096430647478994, 0.509818914109573, 0.4896726930570929, 0.8405416554908718, 0.5919600470221493, 0.10924503343914722, 0.559661595179487, 0.19915064453773526, 0.2901899106002602, 0.82350138780635, 0.8811995104489155, -0.0026199730601336266, 0.6273754137551937, 0.45209713956439024, 0.8906354373500319, 0.35947799638202405, 0.5686159968568669, 0.9365310458982259, 0.5540764057933456, 0.6209591838506415, 0.9423341023576534, 0.8629537052218356, 0.09913384146619357, 0.861804124497599, 1.0134025094685868, 1.0238436707072753, 0.1223014150437775, 0.7564191790890565, 0.8496420275164502, 0.5057299257128748, 1.0579533031401507, 0.5563107155071795, 0.41221059458806253, 0.9509819363304264, 0.402648582285326, 0.8466241570008279, 0.4532358091802571, 0.3715828859500071, 0.9257037629236954, 1.0161241267627343, 0.9679831533207046, 0.08551006422360004, 0.7401986887743578, 0.6912617761978903, 0.6912719677427132, 0.13717432461280588, 0.293046936035356, 0.7303787147472682, 0.9909734747559312, 0.3571915030054305, 0.7973631230391343, 0.9283639175083588, 1.0139295286001617, 0.24754094272197763, 0.8297071581830998, 0.88824784151029, 0.7858833180860436, 0.6022763776161989, 0.3718083436833463, 0.8563632726474141, 0.7400800529100179, 0.12549582962539463, 0.6150166980415612, 0.9537314208270358, 0.12562704192371885, 0.5798832810346423, 0.2919937556919383, 0.8888324350466479, 0.952470434606834, 0.8053072035989765, 0.9399710154775853, 0.06362642722102937, 0.7620088082285006, 0.9083481282905073, 0.9669339876079467, 0.9888299620492754, 0.6944938807177057, 0.8881890939896832, 0.9593959903064364, 0.2848873018568047, 0.7247797916248883, 0.8183366714816989, 0.06709165073838032, 0.26874213756713305, 0.011223732396861794, 1.0316484357431759, 0.9790536541271544, 0.23952636533842916, 0.3497625223408625, 0.6407765418269316, 0.9763388133908357, 0.20629588358405002, 0.7660736707702512, 0.1010531804177125, 0.9152903776989848, 0.8174046370931856, 0.8564215169827514, 0.6352561134416187, 1.0497410572208679, 0.5447934084827437, 0.5351946118904838, 0.4595313536126999, 0.6633019568604016, 0.3389677681314225, 0.8589967160940021, 0.49395016022302285, 0.905401424075449, 0.007092961807579082, 0.13347628461694513, 0.23619407435686118, 1.0447304829420934, 1.0001769556335662, 1.0405634438955484, 0.9751431235666935, 0.5622571965501586, 0.7679885906434996, 0.05897545247341297, 0.8490873977683159, 0.8047207889589721, 0.7796026034672046, 0.26754334219326603, 0.924924136104976, 0.8914539231095208, 0.7583960871301029, 0.7823095355684584, 0.9184490407965745, 1.0132329261188513, 0.9517602430233854, 0.5838573081607782, 0.6988846386900802, 0.8781521803158315, 0.9174454116809432, 0.34317618912624265, 1.034794048641326, 0.5362047491056425, 0.66125328425766, 0.6125423024649467, 0.9802163157294758, 0.6907147631176219, 0.9531165518236112, 0.41001836066068875, 0.004652055868909304, 0.6551194038774679, 0.9947812869407875, 0.7367757629916575, 1.0747219512919652, 0.8426179619616073, 0.5240496559285501, 0.718934130013812, 0.8666280370406515, 0.9899555869779699, 0.6631002553563234, 0.7397758587210745, 0.9967524765686744, 0.10159623822021915, 0.8512914999877094, 0.4181767970733139, 1.0298817385520407, 0.8712548832350971, 0.22980460025990193, 0.7826122428257578, 0.5651412799070576, 1.026816561161334, 0.1829189720690934, 0.8992626732748483, 0.5696317575774928, 0.5552874891305405, 0.9008222562513402, -0.05498257762065896, 0.6777489151718835, 0.5323517097321465, 0.2253268053322805, 0.14879379507579316, 0.9333919037100045, 0.7949502064180195, 0.2061980698699697, 1.045099959603982, -0.020414328454609262, 0.6390572981450053, 0.9333161402422115, 0.5291682973157141, 0.5688352326721351, -0.05047730941467041, 0.6775605785555954, 0.39192999197460315, 0.19452518014587553, 0.7266915290237453, 0.4418917406497507, 0.1342348865555803, 0.8655180578975504, 0.2761789058580983, -0.01705441664861696, 0.7325257918980794, 0.9279765263168213, 0.9511044577331956, 0.344303422286615, 0.9180462031788715, 0.945257094106218, 1.0404483010061818, 0.33369917527554266, 0.24149665784368612, 0.9848957397860001, 1.0003422289697628, 0.8158800591916902, 0.6359107992573945, 0.11955831614046483, 0.6178248779350897, 0.723566646982552, -0.07676642286508525, 0.5024329383266082, 0.9702952661304376, 0.1717890736536283, 0.48423409928227584, 0.20270900200568004, 0.7567035860284822, 0.5783206702822696, 0.9301751369119973, 0.8495053023268582, 0.9915220663397655, 0.09123679877099287, 0.8942218899566153, 0.6437927678617089, 1.0535936428713195, 0.8163120857164957, 0.11692291853621228, 0.9681850055625798, 0.8908023308240709, 0.15973674583121883, 0.35626734597179077, 0.1379387515057112, 1.008121644708765, 0.14832545170674544, 0.8652922240987655, 0.16618991500334923, 0.8720473790292391, 0.9627746232001045, 0.8647710523126371, 0.6581196686990107, 0.5160844279669388, 0.09269477750950172, 0.8399209148346867, 0.950890691176492, 0.7672789717451685, 0.5693084514974619, 0.6406910264205575, 0.5507458266987093, 0.22562358315680073, 0.8393728919656983, 0.9973794456364957, 0.276910915007692, 0.573684335533575, 0.06072464380436819, 0.5432224447970733, 0.5730642907156376, 0.952210602813155, 0.32789359847521676, 0.6077717126326211], \"yaxis\": \"y\"}], {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('fa337ce2-7fd1-4407-92ed-9078ecb1cdc2'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) }; plot_boundaries ( X , model )","title":"L1 Q3:"},{"location":"exercises/E4_Testing_and_Serving_Code/","text":"Technology Fundamentals Course 4, Lab 2: DevOps: Testing and Serving Code.ipynb \u00b6 Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu , harshav@uw.edu In this lab we will practice writing unit tests (part 1) as well as serving our python code in a web framework (part 2). There is an optional part 3 where we move our unit tests into a local directory and run them with pytest . Part 1: Writing Tests \u00b6 ! pip install fastapi Collecting fastapi \u001b[?25l Downloading https://files.pythonhosted.org/packages/4e/b9/a91a699f5c201413b3f61405dbccc29ebe5ad25945230e9cec98fdb2434c/fastapi-0.65.1-py3-none-any.whl (50kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 6.8MB/s \u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.1MB 35.9MB/s \u001b[?25hCollecting starlette==0.14.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 9.2MB/s \u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (3.7.4.3) Installing collected packages: pydantic, starlette, fastapi Successfully installed fastapi-0.65.1 pydantic-1.8.2 starlette-0.14.2 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout Types of Tests \u00b6 There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) In this lab we will focus on unit tests . Unit Tests \u00b6 Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ). L2 Q1 Write a Unit Test \u00b6 Remember our Pokeball discussion in C2? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a **_side effect_**, in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute _contains_. class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the _release_ method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something> test_release () Pikachu has been released ## L2 Q2 Write a Unit Test for the Catch Rate First, we will check that the succcessful catch is operating correctly. Remember that we depend on `random.random` and condition our success on whether that random value is less than the `catch_rate` of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the `catch_rate` of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Sabrina's fave pokemon ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Sabrina's fave pokemon ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured! ## L2 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: 1. seed the random number generator 2. for 100 iterations: * create a pokeball * try to catch something * log whether it was successful 3. check that for the 100 attempts the success was approximately 50/50 _note:_ you can use my `suppress stdout()` function to suppress the print statements from `ball.catch` ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") > quick segway: what is the actual behavior of `random.seed()`? Does it produce the same number every time we call `random.random()` now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to `random.random`. However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is _seeding_ the random number generator such that it will produce the same sequence of random numbers every time, from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. _End Segway_ def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate () ## Test Runners When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called **_test runners_**. We won't dedicate time to any single one here but the three most common are: * unittest * nose2 * pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility. # maybe have a demo of writing a file from jupyterlab cell # and then running that test file with pytest # conversely, could go to the actual command line since it looks like # everyone has a local environment, have them clone a few files from truffletopia # and demo pytest that way. ! pytest test_release # Part 2: Serving Python Our next objective is to serve our code to the wide, wide world (ahem, the world, wide, web) in as simple a manner as possible. As _s i m p l e_ as possible. from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) async def root (): return { \"message\" : \"Hello World\" } Copy the above code into a local file `main.py` install fastapi and uvicorn with: pip install fastapi[all] then from the terminal run: uvicorn main:app --reload `uvicorn` is the server we will use to run our fastapi application. `main` refers to the name of the file to run and `app` the object within it. `--reload` will cause the server to reboot the app anytime changes are made to the file `main.py` You should see on the command line now something like: INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) This is telling us where our python app is running ## Interactive API docs Now go to http://127.0.0.1:8000/docs ## Recap, step by step 1. we imported fastapi `from fastapi import FastAPI` 2. created a `FastAPI` instance `app = FastAPI()` 3. created a _path_ _operation_ **_path_** here refers to the last part of the URL starting from the first `/`. So in a URL like: `truffletopia.io/basecake/chiffon` ...the path would be: `/basecake/chiffon` > a path is commonly referred to as an \"endpoint\" as in \"API endpoint\" or a \"route\" **_operation_** refers to one of the HTTP \"methods\" One of: * `POST`: create data * `GET`: read data * `PUT`: update data * `DELETE`: delete data ...and more exotic ones We can think of these HTTP methods as synonymous with _operation_. Taking it together: `@app.get(\"/\")` tells FastAPI that the function right below is in charge of handling requests that go to: * the path `/` * using a `get` operation > the `@` in python is called a decorator and lets the python executor know it is going to be modifying a function in some way, in this case FastAPI's handling of the `get` requests to `/` 4. define the **_path operation function_** * path is `/` * operation is `GET` * function is the funtion below the decorator If you're curious about the `async` infront of our path operation function you can read about it [here](https://fastapi.tiangolo.com/async/#in-a-hurry).","title":"Testing and Serving Code"},{"location":"exercises/E4_Testing_and_Serving_Code/#technology-fundamentals-course-4-lab-2-devops-testing-and-serving-codeipynb","text":"Instructor : Wesley Beckner Contact : wesleybeckner@gmail.com Teaching Assitants : Varsha Bang, Harsha Vardhan Contact : vbang@uw.edu , harshav@uw.edu In this lab we will practice writing unit tests (part 1) as well as serving our python code in a web framework (part 2). There is an optional part 3 where we move our unit tests into a local directory and run them with pytest .","title":"Technology Fundamentals Course 4, Lab 2: DevOps: Testing and Serving Code.ipynb"},{"location":"exercises/E4_Testing_and_Serving_Code/#part-1-writing-tests","text":"! pip install fastapi Collecting fastapi \u001b[?25l Downloading https://files.pythonhosted.org/packages/4e/b9/a91a699f5c201413b3f61405dbccc29ebe5ad25945230e9cec98fdb2434c/fastapi-0.65.1-py3-none-any.whl (50kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 6.8MB/s \u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.1MB 35.9MB/s \u001b[?25hCollecting starlette==0.14.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 9.2MB/s \u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (3.7.4.3) Installing collected packages: pydantic, starlette, fastapi Successfully installed fastapi-0.65.1 pydantic-1.8.2 starlette-0.14.2 import random import numpy as np from contextlib import contextmanager import sys , os @contextmanager def suppress_stdout (): with open ( os . devnull , \"w\" ) as devnull : old_stdout = sys . stdout sys . stdout = devnull try : yield finally : sys . stdout = old_stdout","title":"Part 1: Writing Tests"},{"location":"exercises/E4_Testing_and_Serving_Code/#types-of-tests","text":"There are two main types of tests we want to distinguish: * Unit test : an automatic test to test the internal workings of a class or function. It should be a stand-alone test which is not related to other resources. * Integration test : an automatic test that is done on an environment, it tests the coordination of different classes and functions as well as with the running environment. This usually precedes sending code to a QA team. To this I will add: Acid test : extremely rigorous tests that push beyond the intended use cases for your classes/functions. Written when you, like me, cannot afford QA employees to actually test your code. (word origin: gold acid tests in the 1850s , acid tests in the 70's ) In this lab we will focus on unit tests .","title":"Types of Tests"},{"location":"exercises/E4_Testing_and_Serving_Code/#unit-tests","text":"Each unit test should test the smallest portion of your code possible, i.e. a single method or function. Any random number generators should be seeded so that they run the exact same way every time. Unit tests should not rely on any local files or the local environment. Why bother with Unit Tests when we have Integration tests? A major challenge with integration testing is when an integration test fails. It\u2019s very hard to diagnose a system issue without being able to isolate which part of the system is failing. Here comes the unit test to the rescue. Let's take a simple example. If I wanted to test that the sume of two numbers is correct assert sum ([ 2 , 5 ]) == 7 , \"should be 7\" Nothing is sent to the print out because the condition is satisfied. If we run, however: assert sum([2, 4]) == 7, \"should be 7\" we get an error message: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-d5724b127818> in <module>() ----> 1 assert sum([2, 4]) == 7, \"should be 7\" AssertionError: should be 7 To make this a Unit Test, you will want to wrap it in a function def test_sum (): assert sum ([ 1 , 2 , 3 ]) == 6 , \"Should be 6\" test_sum () print ( \"Everything passed\" ) Everything passed And if we include a test that does not pass: def test_sum(): assert sum([3, 3]) == 6, \"Should be 6\" def test_my_broken_func(): assert sum([1, 2]) == 5, \"Should be 5\" test_sum() test_my_broken_func() print(\"Everything passed\") Here our test fails, because the sum of 1 and 2 is 3 and not 5. We get a traceback that tells us the source of the error: --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-13-8a552fbf52bd> in <module>() 6 7 test_sum() ----> 8 test_my_broken_func() 9 print(\"Everything passed\") <ipython-input-13-8a552fbf52bd> in test_my_broken_func() 3 4 def test_my_broken_func(): ----> 5 assert sum([1, 2]) == 5, \"Should be 5\" 6 7 test_sum() AssertionError: Should be 5 Before sending us on our merry way to practice writing unit tests, we will want to ask, what do I want to write a test about? Here, we've been testing sum(). There are many behaviors in sum() we could check, such as: Does it sum a list of whole numbers (integers)? Can it sum a tuple or set? Can it sum a list of floats? What happens if one of the numbers is negative? etc.. In the end, what you test is up to you, and depends on your intended use cases. As a general rule of thumb, your unit test should test what is relevant. The only caveat to that, is that many continuous integration services (like TravisCI ) will benchmark you based on the percentage of lines of code you have that are covered by your unit tests (ex: 85% coverage ).","title":"Unit Tests"},{"location":"exercises/E4_Testing_and_Serving_Code/#l2-q1-write-a-unit-test","text":"Remember our Pokeball discussion in C2? We'll return to that here. This time writing unit tests for our classes. Sometimes when writing unit tests, it can be more complicated than checking the return value of a function. Think back on our pokemon example: class Pokeball: def __init__(self, contains=None, type_name=\"poke ball\"): self.contains = contains self.type_name = type_name self.catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch(self, pokemon): if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon print(f\"{pokemon} captured!\") else: print(f\"{pokemon} escaped!\") pass else: print(\"pokeball is not empty!\") def release(self): if self.contains == None: print(\"Pokeball is already empty\") else: print(self.contains, \"has been released\") self.contains = None If I wanted to write a unit test for the release method, I couldn't directly check for the output of a function. I'll have to check for a **_side effect_**, in this case, the change of an attribute belonging to a pokeball object; that is the change to the attribute _contains_. class Pokeball : def __init__ ( self , contains = None , type_name = \"poke ball\" ): self . contains = contains self . type_name = type_name self . catch_rate = 0.50 # note this attribute is not accessible upon init # the method catch, will update self.contains, if a catch is successful # it will also use self.catch_rate to set the performance of the catch def catch ( self , pokemon ): if self . contains == None : if random . random () < self . catch_rate : self . contains = pokemon print ( f \" { pokemon } captured!\" ) else : print ( f \" { pokemon } escaped!\" ) pass else : print ( \"pokeball is not empty!\" ) def release ( self ): if self . contains == None : print ( \"Pokeball is already empty\" ) else : print ( self . contains , \"has been released\" ) self . contains = None In the following cell, finish the code to test the functionality of the _release_ method: def test_release (): ball = Pokeball () ball . contains = 'Pikachu' ball . release () # turn the pseudo code below into an assert statement ### YOUR CODE HERE ### # assert <object.attribute> == <something> test_release () Pikachu has been released ## L2 Q2 Write a Unit Test for the Catch Rate First, we will check that the succcessful catch is operating correctly. Remember that we depend on `random.random` and condition our success on whether that random value is less than the `catch_rate` of the pokeball: if self.contains == None: if random.random() < self.catch_rate: self.contains = pokemon so to test whether the successful catch is working we will seed our random number generator with a value that returns less than the `catch_rate` of the pokeball and then write our assert statement: def test_successful_catch (): # choose a random seed such that # we know the catch call should succeed ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Sabrina's fave pokemon ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not catch as expected\" NICE. Now we will do the same thing again, this time testing for an unsuccessful catch. SO in order to do this, we need to choose a random seed that will cause our catch to fail: def test_unsuccessful_catch (): # choose a random seed such that # we know the catch call should FAIL ### YOUR CODE BELOW ### # random.seed(<your number here>) ball = Pokeball () ball . catch ( 'Psyduck' ) # Sabrina's fave pokemon ### YOUR CODE BELOW ### # <object.attribute> == <something>, \"ball did not fail as expected\" When you are finished test your functions below test_unsuccessful_catch () Psyduck captured! test_successful_catch () Psyduck captured! ## L2 Q3 Write a Unit Test that Checks Whether the Overall Catch Rate is 50/50 For this one, we're going to take those same ideas around seeding the random number generator. However, here we'd like to run the catch function multiple times to check whether it is truly creating a 50/50 catch rate situation. Here's a pseudo code outline: 1. seed the random number generator 2. for 100 iterations: * create a pokeball * try to catch something * log whether it was successful 3. check that for the 100 attempts the success was approximately 50/50 _note:_ you can use my `suppress stdout()` function to suppress the print statements from `ball.catch` ex: with suppress_stdout(): print(\"HELLO OUT THERE!\") > quick segway: what is the actual behavior of `random.seed()`? Does it produce the same number every time we call `random.random()` now? Check for yourself: random . seed ( 42 ) [ random . random () for i in range ( 5 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124] We see that it still produces random numbers with each call to `random.random`. However, those numbers are the same with every execution of the cell. What happens when we do this: [ random . random () for i in range ( 5 )] [0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] The numbers are different. BUT: random . seed ( 42 ) [ random . random () for i in range ( 10 )] [0.6394267984578837, 0.025010755222666936, 0.27502931836911926, 0.22321073814882275, 0.7364712141640124, 0.6766994874229113, 0.8921795677048454, 0.08693883262941615, 0.4219218196852704, 0.029797219438070344] We see them here in the bottom half of the list again. So, random.seed() is _seeding_ the random number generator such that it will produce the same sequence of random numbers every time, from the given seed. This will reset whenever random.seed() is set again. This behavior is useful because it allows us to continue using random number generation in our code, (for testing, creating examples and demos, etc.) but it will be reproducable each time. _End Segway_ def test_catch_rate (): ### YOUR CODE HERE ### ### END YOUR CODE ### assert np . abs ( np . mean ( results ) - 0.5 ) < 0.1 , \"catch rate not 50/50\" test_catch_rate () ## Test Runners When we start to create many tests like this, it can be cumbersome to run them all at once and log which ones fail. To handle our unit tests we use what are called **_test runners_**. We won't dedicate time to any single one here but the three most common are: * unittest * nose2 * pytest unittest is built into python. I don't like it because you have to follow a strict class/method structure when writing the tests. nose2 is popular with many useful features and is generally good for high volumes of tests. My favorite is pytest, it's flexible and has an ecosystem of plugins for extensibility. # maybe have a demo of writing a file from jupyterlab cell # and then running that test file with pytest # conversely, could go to the actual command line since it looks like # everyone has a local environment, have them clone a few files from truffletopia # and demo pytest that way. ! pytest test_release # Part 2: Serving Python Our next objective is to serve our code to the wide, wide world (ahem, the world, wide, web) in as simple a manner as possible. As _s i m p l e_ as possible. from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) async def root (): return { \"message\" : \"Hello World\" } Copy the above code into a local file `main.py` install fastapi and uvicorn with: pip install fastapi[all] then from the terminal run: uvicorn main:app --reload `uvicorn` is the server we will use to run our fastapi application. `main` refers to the name of the file to run and `app` the object within it. `--reload` will cause the server to reboot the app anytime changes are made to the file `main.py` You should see on the command line now something like: INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) This is telling us where our python app is running ## Interactive API docs Now go to http://127.0.0.1:8000/docs ## Recap, step by step 1. we imported fastapi `from fastapi import FastAPI` 2. created a `FastAPI` instance `app = FastAPI()` 3. created a _path_ _operation_ **_path_** here refers to the last part of the URL starting from the first `/`. So in a URL like: `truffletopia.io/basecake/chiffon` ...the path would be: `/basecake/chiffon` > a path is commonly referred to as an \"endpoint\" as in \"API endpoint\" or a \"route\" **_operation_** refers to one of the HTTP \"methods\" One of: * `POST`: create data * `GET`: read data * `PUT`: update data * `DELETE`: delete data ...and more exotic ones We can think of these HTTP methods as synonymous with _operation_. Taking it together: `@app.get(\"/\")` tells FastAPI that the function right below is in charge of handling requests that go to: * the path `/` * using a `get` operation > the `@` in python is called a decorator and lets the python executor know it is going to be modifying a function in some way, in this case FastAPI's handling of the `get` requests to `/` 4. define the **_path operation function_** * path is `/` * operation is `GET` * function is the funtion below the decorator If you're curious about the `async` infront of our path operation function you can read about it [here](https://fastapi.tiangolo.com/async/#in-a-hurry).","title":"L2 Q1 Write a Unit Test"}]}